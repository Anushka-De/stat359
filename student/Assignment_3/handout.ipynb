{
  "cells": [
    {
      "cell_type": "raw",
      "id": "3f9655a7",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        },
        "id": "3f9655a7"
      },
      "source": [
        "---\n",
        "title: \"Assignment 3: Neural Models for Sentiment Classification\"\n",
        "format:\n",
        "  html:\n",
        "    toc: true\n",
        "    toc-title: Contents\n",
        "    toc-depth: 4\n",
        "    self-contained: true\n",
        "    number-sections: false\n",
        "jupyter: python3\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6e43e1df",
      "metadata": {
        "id": "6e43e1df"
      },
      "source": [
        "### <font color='blue'> Due 11:59pm, Monday Feb 12th 2026</font>\n",
        "\n",
        "**Purpose / learning goals:**\n",
        "- Practice training neural models in PyTorch with emphasis on optimizers, regularization, and learning-rate scheduling to meet a performance threshold.\n",
        "- Use sentiment classification as a downstream task to compare classical neural baselines with fine-tuned pretrained LLMs (BERT/GPT).\n",
        "\n",
        "**Runtime / setup notes:**\n",
        "- This assignment does not require a GPU to train the models. Using a GPU (or Apple MPS) will usually speed up training for the transformer models.\n",
        "\n",
        "In this assignment, you will:\n",
        "- Implement MLP and LSTM classifiers (your code)\n",
        "- Run provided scripts for RNN, GRU, BERT, and GPT (for comparison)\n",
        "\n",
        "**Implementation format:** Task 1 and Task 2 must be implemented as Python scripts (not notebooks). The open-ended questions are answered in a notebook.\n",
        "\n",
        "To motivate the transformer architecture, scripts are provided for pretrained state-of-the-art models such as **GPT** (decoder-only) and **BERT** (encoder-only). You should run these scripts yourself to obtain results for comparison and reflection.\n",
        "\n",
        "*Please read the `README.md` file before proceeding.*\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1e243326",
      "metadata": {
        "id": "1e243326"
      },
      "source": [
        "##  Sentiment Classification: Classical Nets vs. LLMs\n",
        "\n",
        "Sentiment classification is a common **downstream task** for evaluating how well pretrained LLMs adapt to a domain via fine-tuning, compared against classical neural baselines.\n",
        "\n",
        "In this assignment, you'll explore how different neural architectures perform on sentiment classification:\n",
        "\n",
        "- **Classical approaches:** MLP, RNN, LSTM, GRU (using static FastText embeddings)\n",
        "- **Pretrained LLMs:** BERT and GPT (fine-tuned using Hugging Face Transformers)\n",
        "\n",
        "You will implement MLP and LSTM yourself; scripts are provided for the remaining models.\n",
        "\n",
        "Detailed requirements for your implementations are listed in **Your Tasks** below.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9d347c96",
      "metadata": {
        "id": "9d347c96"
      },
      "source": [
        "##  Dataset: Financial PhraseBank\n",
        "\n",
        "This assignment uses the **Financial PhraseBank** dataset, developed by  \n",
        "Mika V. M√§ntyl√§, Graziella Linders, Tanja Suominen, and Miikka Kuutila.\n",
        "\n",
        "- üìÇ Dataset homepage: [Hugging Face ‚Äì Financial_PhraseBank](https://huggingface.co/datasets/takala/financial_phrasebank)  \n",
        "- üìÑ Original paper:  \n",
        "  P. Malo, A. Sinha, et al. (2014). [*‚ÄúGood Debt or Bad Debt: Detecting Semantic Orientations in Economic Texts‚Äù*](https://arxiv.org/pdf/1307.5336)\n",
        "\n",
        "You can load and preview the dataset using the following code:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/Anushka-De/stat359.git"
      ],
      "metadata": {
        "id": "kPP2CDEVJrZK",
        "outputId": "ac672581-a48d-477d-b552-e69cd7b8bddf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "kPP2CDEVJrZK",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'stat359' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd stat359/student/Assignment_3\n",
        "!ls"
      ],
      "metadata": {
        "id": "JmH8P93LJtga",
        "outputId": "c9a6c40a-7825-484e-a3e0-a755442655de",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "JmH8P93LJtga",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/stat359/student/Assignment_3\n",
            "acc_vs_epoch.png\t   outputs\n",
            "best_mlp_fasttext.pt\t   outputs.zip\n",
            "confusion_matrix_test.png  README.md\n",
            "handout.html\t\t   train_sentiment_bert_classifier.py\n",
            "handout.ipynb\t\t   train_sentiment_gpt_classifier.py\n",
            "loss_vs_epoch.png\t   train_sentiment_gru_classifier.py\n",
            "macro_f1_vs_epoch.png\t   train_sentiment_mlp_classifier.py\n",
            "open_questions.ipynb\t   train_sentiment_rnn_classifier.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip -q install \"datasets<4.0.0\""
      ],
      "metadata": {
        "id": "lOb0jBZy35Cz"
      },
      "id": "lOb0jBZy35Cz",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q numpy pandas gensim torch scikit-learn matplotlib ipywidgets nltk tqdm"
      ],
      "metadata": {
        "id": "-fR_RM9R5T7L"
      },
      "id": "-fR_RM9R5T7L",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(\"\\n========== Loading Dataset ==========\")\n",
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset('financial_phrasebank', 'sentences_50agree', trust_remote_code=True)\n",
        "print(\"Dataset loaded. Example:\", dataset['train'][:5])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V-4FZj7Q760p",
        "outputId": "aa277058-1961-401a-c845-67e1e98ca3d6"
      },
      "id": "V-4FZj7Q760p",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "========== Loading Dataset ==========\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n",
            "WARNING:huggingface_hub.utils._http:Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset loaded. Example: {'sentence': ['According to Gran , the company has no plans to move all production to Russia , although that is where the company is growing .', 'Technopolis plans to develop in stages an area of no less than 100,000 square meters in order to host companies working in computer technologies and telecommunications , the statement said .', 'The international electronic industry company Elcoteq has laid off tens of employees from its Tallinn facility ; contrary to earlier layoffs the company contracted the ranks of its office workers , the daily Postimees reported .', 'With the new production plant the company would increase its capacity to meet the expected increase in demand and would improve the use of raw materials and therefore increase the production profitability .', \"According to the company 's updated strategy for the years 2009-2012 , Basware targets a long-term net sales growth in the range of 20 % -40 % with an operating profit margin of 10 % -20 % of net sales .\"], 'label': [1, 1, 0, 2, 2]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9d51c574",
      "metadata": {
        "id": "9d51c574"
      },
      "source": [
        "###  Dataset Description\n",
        "\n",
        "The dataset consists of **4,840 English sentences** extracted from financial news articles.  \n",
        "Each sentence is labeled as **positive**, **neutral**, or **negative**, with annotations provided by 5 to 8 human annotators to ensure labeling consistency.  \n",
        "\n",
        "This assignment uses the `'sentences_50agree'` subset, where at least 50% of annotators agreed on the sentiment."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "da05e0ad",
      "metadata": {
        "id": "da05e0ad"
      },
      "source": [
        "###  Class Imbalance\n",
        "\n",
        "The dataset has an **imbalanced class distribution**:\n",
        "\n",
        "| Sentiment | Count |\n",
        "|-----------|-------|\n",
        "| Negative  | 604   |\n",
        "| Neutral   | 2879  |\n",
        "| Positive  | 1363  |\n",
        "\n",
        "For dealing with imbalanced dataset:\n",
        "\n",
        "- **Accuracy** can be misleading in this setting.\n",
        "- You must use `class_weight` in your loss function (e.g., `nn.CrossEntropyLoss(weight=...)`) to mitigate the imbalance.\n",
        "- The primary evaluation metric will be the **macro-averaged F1 score**, which treats all classes equally regardless of frequency."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ccf5ce4c",
      "metadata": {
        "id": "ccf5ce4c"
      },
      "source": [
        "### Train/Validation/Test Splits\n",
        "\n",
        "The dataset does **not** come with predefined splits.\n",
        "\n",
        "You must split it yourself using **stratified sampling** to preserve class proportions in each subset.\n",
        "\n",
        "For a fair comparison and to stay consistent with the other model scripts, use the following split procedure:\n",
        "\n",
        "- First, create a **test set (15%)** and a **train+validation set (85%)** using stratified sampling on the original labels.\n",
        "- Then, split the **train+validation set** into **training (85%)** and **validation (15%)** using stratified sampling on the train+validation labels.\n",
        "- Use a fixed random seed (e.g., 42) so results are reproducible.\n",
        "\n",
        "This ensures consistent and representative evaluation, especially in the presence of class imbalance."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "random.seed(42)"
      ],
      "metadata": {
        "id": "eSh7jKh7REOV"
      },
      "id": "eSh7jKh7REOV",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "05b696bf",
      "metadata": {
        "id": "05b696bf"
      },
      "source": [
        "## Your Tasks\n",
        "\n",
        "Before you begin, please follow these best practices in your implementation:\n",
        "\n",
        "- Set **random seeds** to ensure reproducibility  \n",
        "- Use `torch.save()` to save your **best-performing model**  \n",
        "- Modularize your code into **reusable functions or classes**\n",
        "\n",
        "You are encouraged to experiment with different **neural network architectures**, **hyperparameters**, **optimizers**, **regularization** (e.g., dropout, weight decay), and **learning-rate scheduling**, as long as your final model meets the required **macro F1 score threshold** for each task.\n",
        "\n",
        "**Implementation format:** Task 1 and Task 2 must be implemented as Python scripts (not notebooks). Name them as specified below.\n",
        "\n",
        "### Task 1: MLP with Mean-Pooled FastText Sentence Embedding **(25 points)**\n",
        "\n",
        "Create a script named `train_sentiment_mlp_classifier.py` and complete the following:\n",
        "\n",
        "- Load **pretrained FastText embeddings** using Gensim.\n",
        "- Tokenize each sentence and compute the **mean of its word vectors** to obtain a fixed-size (300-dimensional) sentence embedding.\n",
        "- Use a **Multi-Layer Perceptron (MLP)** to classify the sentence embedding.\n",
        "- Handle **class imbalance** using `nn.CrossEntropyLoss(weight=...)`.\n",
        "- Track and report the following metrics:\n",
        "  - **Loss**\n",
        "  - **Accuracy**\n",
        "  - **Macro F1 Score**\n",
        "\n",
        "#### Performance Requirement:\n",
        "Your model must achieve a **Test Macro F1 Score >= 0.65**\n",
        "\n",
        "### Task 2: LSTM with Padded FastText Word Vectors **(25 points)**\n",
        "\n",
        "Create a script named `train_sentiment_lstm_classifier.py` and complete the following:\n",
        "\n",
        "- Tokenize each sentence into word tokens and retrieve the corresponding **FastText word vectors**.\n",
        "- **Pad or truncate** each sentence to exactly **32 tokens**.\n",
        "- Construct a tensor of shape **(32, 300)** for each sentence (300 = embedding dimension).\n",
        "- **Do not use** `nn.Embedding`; instead, **precompute and batch** the word vectors directly.\n",
        "- Pass the sequences into an **LSTM model** and classify using the **final hidden state**.\n",
        "- Use `nn.CrossEntropyLoss(weight=...)` and evaluate using **macro-averaged F1 score**.\n",
        "\n",
        "#### Performance Requirement:\n",
        "Your model must achieve a **Test Macro F1 Score >= 0.70**\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python train_sentiment_mlp_classifier.py\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WDBEPzk26FXY",
        "outputId": "d5a96dc1-2c96-474f-80e8-4de13db75b1d"
      },
      "id": "WDBEPzk26FXY",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "========== Loading Dataset ==========\n",
            "Using device: cuda\n",
            "Dataset loaded. Example: {'sentence': 'According to Gran , the company has no plans to move all production to Russia , although that is where the company is growing .', 'label': 1}\n",
            "\n",
            "========== Splitting Data ==========\n",
            "Train: 3501, Val: 618, Test: 727\n",
            "\n",
            "========== Loading FastText Model ==========\n",
            "FastText loaded.\n",
            "\n",
            "========== Creating DataLoaders ==========\n",
            "\n",
            "========== Setting Up Training ==========\n",
            "Training setup complete.\n",
            "\n",
            "========== Starting Training Loop ==========\n",
            "\n",
            "--- Epoch 1/60 ---\n",
            "Train Loss: 1.0597, Train F1: 0.3999, Train Acc: 0.4616\n",
            "Val Loss: 1.0476, Val F1: 0.3647, Val Acc: 0.5210\n",
            ">>> Saved new best model (Val F1: 0.3647) at epoch 1\n",
            "\n",
            "--- Epoch 2/60 ---\n",
            "Train Loss: 0.9865, Train F1: 0.4415, Train Acc: 0.5573\n",
            "Val Loss: 0.9540, Val F1: 0.4142, Val Acc: 0.5372\n",
            ">>> Saved new best model (Val F1: 0.4142) at epoch 2\n",
            "\n",
            "--- Epoch 3/60 ---\n",
            "Train Loss: 0.9099, Train F1: 0.4862, Train Acc: 0.5807\n",
            "Val Loss: 0.9127, Val F1: 0.5029, Val Acc: 0.5890\n",
            ">>> Saved new best model (Val F1: 0.5029) at epoch 3\n",
            "\n",
            "--- Epoch 4/60 ---\n",
            "Train Loss: 0.8592, Train F1: 0.5442, Train Acc: 0.6073\n",
            "Val Loss: 0.8639, Val F1: 0.5065, Val Acc: 0.5324\n",
            ">>> Saved new best model (Val F1: 0.5065) at epoch 4\n",
            "\n",
            "--- Epoch 5/60 ---\n",
            "Train Loss: 0.8131, Train F1: 0.5655, Train Acc: 0.6281\n",
            "Val Loss: 0.8135, Val F1: 0.5519, Val Acc: 0.6392\n",
            ">>> Saved new best model (Val F1: 0.5519) at epoch 5\n",
            "\n",
            "--- Epoch 6/60 ---\n",
            "Train Loss: 0.7709, Train F1: 0.5995, Train Acc: 0.6524\n",
            "Val Loss: 0.7762, Val F1: 0.5924, Val Acc: 0.6634\n",
            ">>> Saved new best model (Val F1: 0.5924) at epoch 6\n",
            "\n",
            "--- Epoch 7/60 ---\n",
            "Train Loss: 0.7380, Train F1: 0.6209, Train Acc: 0.6672\n",
            "Val Loss: 0.7562, Val F1: 0.6297, Val Acc: 0.6893\n",
            ">>> Saved new best model (Val F1: 0.6297) at epoch 7\n",
            "\n",
            "--- Epoch 8/60 ---\n",
            "Train Loss: 0.7128, Train F1: 0.6265, Train Acc: 0.6695\n",
            "Val Loss: 0.7337, Val F1: 0.6268, Val Acc: 0.6909\n",
            "\n",
            "--- Epoch 9/60 ---\n",
            "Train Loss: 0.6723, Train F1: 0.6458, Train Acc: 0.6858\n",
            "Val Loss: 0.7341, Val F1: 0.5855, Val Acc: 0.6327\n",
            "\n",
            "--- Epoch 10/60 ---\n",
            "Train Loss: 0.6695, Train F1: 0.6549, Train Acc: 0.6935\n",
            "Val Loss: 0.7193, Val F1: 0.6060, Val Acc: 0.6343\n",
            "\n",
            "--- Epoch 11/60 ---\n",
            "Train Loss: 0.6447, Train F1: 0.6704, Train Acc: 0.7067\n",
            "Val Loss: 0.6926, Val F1: 0.6228, Val Acc: 0.6634\n",
            "\n",
            "--- Epoch 12/60 ---\n",
            "Train Loss: 0.6233, Train F1: 0.6730, Train Acc: 0.7072\n",
            "Val Loss: 0.7088, Val F1: 0.6670, Val Acc: 0.7120\n",
            ">>> Saved new best model (Val F1: 0.6670) at epoch 12\n",
            "\n",
            "--- Epoch 13/60 ---\n",
            "Train Loss: 0.6077, Train F1: 0.6889, Train Acc: 0.7232\n",
            "Val Loss: 0.6887, Val F1: 0.6242, Val Acc: 0.6553\n",
            "\n",
            "--- Epoch 14/60 ---\n",
            "Train Loss: 0.6038, Train F1: 0.6982, Train Acc: 0.7295\n",
            "Val Loss: 0.7242, Val F1: 0.6643, Val Acc: 0.7233\n",
            "\n",
            "--- Epoch 15/60 ---\n",
            "Train Loss: 0.5929, Train F1: 0.6995, Train Acc: 0.7315\n",
            "Val Loss: 0.6895, Val F1: 0.6468, Val Acc: 0.6926\n",
            "\n",
            "--- Epoch 16/60 ---\n",
            "Train Loss: 0.5873, Train F1: 0.6870, Train Acc: 0.7207\n",
            "Val Loss: 0.7002, Val F1: 0.6558, Val Acc: 0.7087\n",
            "\n",
            "--- Epoch 17/60 ---\n",
            "Train Loss: 0.5648, Train F1: 0.7214, Train Acc: 0.7481\n",
            "Val Loss: 0.6981, Val F1: 0.6644, Val Acc: 0.7168\n",
            "\n",
            "--- Epoch 18/60 ---\n",
            "Train Loss: 0.5624, Train F1: 0.7070, Train Acc: 0.7378\n",
            "Val Loss: 0.6889, Val F1: 0.6479, Val Acc: 0.6861\n",
            "\n",
            "--- Epoch 19/60 ---\n",
            "Train Loss: 0.5600, Train F1: 0.7088, Train Acc: 0.7372\n",
            "Val Loss: 0.6929, Val F1: 0.6621, Val Acc: 0.7120\n",
            "\n",
            "--- Epoch 20/60 ---\n",
            "Train Loss: 0.5514, Train F1: 0.7221, Train Acc: 0.7478\n",
            "Val Loss: 0.6872, Val F1: 0.6578, Val Acc: 0.6990\n",
            "\n",
            "--- Epoch 21/60 ---\n",
            "Train Loss: 0.5412, Train F1: 0.7260, Train Acc: 0.7509\n",
            "Val Loss: 0.7064, Val F1: 0.6656, Val Acc: 0.7039\n",
            "\n",
            "--- Epoch 22/60 ---\n",
            "Train Loss: 0.5435, Train F1: 0.7247, Train Acc: 0.7544\n",
            "Val Loss: 0.6845, Val F1: 0.6526, Val Acc: 0.6877\n",
            "\n",
            "--- Epoch 23/60 ---\n",
            "Train Loss: 0.5389, Train F1: 0.7240, Train Acc: 0.7532\n",
            "Val Loss: 0.6962, Val F1: 0.6576, Val Acc: 0.6958\n",
            "\n",
            "--- Epoch 24/60 ---\n",
            "Train Loss: 0.5376, Train F1: 0.7257, Train Acc: 0.7526\n",
            "Val Loss: 0.6883, Val F1: 0.6555, Val Acc: 0.6990\n",
            "\n",
            "--- Epoch 25/60 ---\n",
            "Train Loss: 0.5399, Train F1: 0.7261, Train Acc: 0.7521\n",
            "Val Loss: 0.6933, Val F1: 0.6601, Val Acc: 0.7039\n",
            "\n",
            "--- Epoch 26/60 ---\n",
            "Train Loss: 0.5350, Train F1: 0.7238, Train Acc: 0.7501\n",
            "Val Loss: 0.6921, Val F1: 0.6554, Val Acc: 0.7006\n",
            "\n",
            "--- Epoch 27/60 ---\n",
            "Train Loss: 0.5275, Train F1: 0.7307, Train Acc: 0.7575\n",
            "Val Loss: 0.6975, Val F1: 0.6628, Val Acc: 0.7055\n",
            "\n",
            "--- Epoch 28/60 ---\n",
            "Train Loss: 0.5344, Train F1: 0.7294, Train Acc: 0.7578\n",
            "Val Loss: 0.6896, Val F1: 0.6535, Val Acc: 0.6958\n",
            "\n",
            "--- Epoch 29/60 ---\n",
            "Train Loss: 0.5314, Train F1: 0.7269, Train Acc: 0.7512\n",
            "Val Loss: 0.6942, Val F1: 0.6658, Val Acc: 0.7120\n",
            "\n",
            "--- Epoch 30/60 ---\n",
            "Train Loss: 0.5291, Train F1: 0.7319, Train Acc: 0.7572\n",
            "Val Loss: 0.6936, Val F1: 0.6652, Val Acc: 0.7104\n",
            "Early stopping triggered at epoch 30 (best epoch 12).\n",
            "\n",
            "========== Plotting Learning Curves ==========\n",
            "Learning curves saved as 'outputs/mlp_learning_curves.png'.\n",
            "Saved: outputs/mlp_loss_curve.png\n",
            "Saved: outputs/mlp_f1_curve.png\n",
            "Saved: outputs/mlp_accuracy_curve.png\n",
            "\n",
            "========== Evaluating on Test Set ==========\n",
            "                                   \n",
            "==================================================\n",
            "Best Val F1: 0.6670 at epoch 12\n",
            "Final Test Loss: 0.6482\n",
            "Final Test Accuracy: 0.7276\n",
            "Test F1 Macro: 0.6854\n",
            "Test F1 Weighted: 0.7336\n",
            "==================================================\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "Negative (0)     0.5349    0.7582    0.6273        91\n",
            " Neutral (1)     0.8597    0.7662    0.8103       432\n",
            "Positive (2)     0.6056    0.6324    0.6187       204\n",
            "\n",
            "    accuracy                         0.7276       727\n",
            "   macro avg     0.6668    0.7189    0.6854       727\n",
            "weighted avg     0.7478    0.7276    0.7336       727\n",
            "\n",
            "Confusion matrix saved as 'outputs/mlp_confusion_matrix.png'.\n",
            "\n",
            "========== Script Complete ==========\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python train_sentiment_rnn_classifier.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2uw2-BSERXbG",
        "outputId": "93961d31-ba6c-458c-cde2-ebdafa9db5f9"
      },
      "id": "2uw2-BSERXbG",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "========== Loading Dataset ==========\n",
            "Dataset loaded. Example: {'sentence': 'According to Gran , the company has no plans to move all production to Russia , although that is where the company is growing .', 'label': 1}\n",
            "\n",
            "========== Preparing DataFrame ==========\n",
            "DataFrame shape: (4846, 3)\n",
            "\n",
            "Sentence length statistics:\n",
            "count    4846.000000\n",
            "mean       23.101114\n",
            "std         9.958474\n",
            "min         2.000000\n",
            "25%        16.000000\n",
            "50%        21.000000\n",
            "75%        29.000000\n",
            "max        81.000000\n",
            "Name: sentence, dtype: float64\n",
            "Figure(1000x600)\n",
            "\n",
            "========== Loading SentenceTransformer Model ==========\n",
            "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n",
            "Loading weights: 100% 103/103 [00:00<00:00, 2491.44it/s, Materializing param=pooler.dense.weight]\n",
            "\u001b[1mBertModel LOAD REPORT\u001b[0m from: sentence-transformers/all-MiniLM-L6-v2\n",
            "Key                     | Status     |  | \n",
            "------------------------+------------+--+-\n",
            "embeddings.position_ids | \u001b[38;5;208mUNEXPECTED\u001b[0m |  | \n",
            "\n",
            "\u001b[3mNotes:\n",
            "- \u001b[38;5;208mUNEXPECTED\u001b[0m\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n",
            "Model loaded.\n",
            "\n",
            "========== Encoding Sentences as Sequences ==========\n",
            "Tokenizing & encoding: 100% 4846/4846 [00:29<00:00, 165.84it/s]\n",
            "X_seq shape: (4846, 32, 384), y shape: (4846,)\n",
            "\n",
            "========== Splitting Data ==========\n",
            "X_train shape: (3501, 32, 384), y_train shape: (3501,)\n",
            "X_val shape: (618, 32, 384), y_val shape: (618,)\n",
            "X_test shape: (727, 32, 384), y_test shape: (727,)\n",
            "DataLoaders created.\n",
            "\n",
            "========== Defining RNN Model ==========\n",
            "Model initialized with input_dim=384, hidden_dim=128, num_layers=2, num_classes=3\n",
            "\n",
            "========== Setting Up Training ==========\n",
            "Using device: cuda\n",
            "Training setup complete.\n",
            "\n",
            "========== Starting Training Loop ==========\n",
            "\n",
            "--- Epoch 1/30 ---\n",
            "Train Loss: 0.8941, Train F1: 0.5199, Train Acc: 0.5456\n",
            "Val Loss: 0.7106, Val F1: 0.6451, Val Acc: 0.6424\n",
            "Epoch 1/30: Train Loss: 0.8941 | Train F1: 0.5199 | Train Acc: 0.5456 | Val Loss: 0.7106 | Val F1: 0.6451 | Val Acc: 0.6424\n",
            ">>> Saved new best model (Val F1: 0.6451)\n",
            "\n",
            "--- Epoch 2/30 ---\n",
            "Train Loss: 0.7193, Train F1: 0.6324, Train Acc: 0.6510\n",
            "Val Loss: 0.7860, Val F1: 0.5806, Val Acc: 0.6715\n",
            "Epoch 2/30: Train Loss: 0.7193 | Train F1: 0.6324 | Train Acc: 0.6510 | Val Loss: 0.7860 | Val F1: 0.5806 | Val Acc: 0.6715\n",
            "\n",
            "--- Epoch 3/30 ---\n",
            "Train Loss: 0.6521, Train F1: 0.6800, Train Acc: 0.6984\n",
            "Val Loss: 0.6790, Val F1: 0.7016, Val Acc: 0.7298\n",
            "Epoch 3/30: Train Loss: 0.6521 | Train F1: 0.6800 | Train Acc: 0.6984 | Val Loss: 0.6790 | Val F1: 0.7016 | Val Acc: 0.7298\n",
            ">>> Saved new best model (Val F1: 0.7016)\n",
            "\n",
            "--- Epoch 4/30 ---\n",
            "Train Loss: 0.6020, Train F1: 0.7033, Train Acc: 0.7204\n",
            "Val Loss: 0.7507, Val F1: 0.6755, Val Acc: 0.7265\n",
            "Epoch 4/30: Train Loss: 0.6020 | Train F1: 0.7033 | Train Acc: 0.7204 | Val Loss: 0.7507 | Val F1: 0.6755 | Val Acc: 0.7265\n",
            "\n",
            "--- Epoch 5/30 ---\n",
            "Train Loss: 0.5787, Train F1: 0.7156, Train Acc: 0.7278\n",
            "Val Loss: 0.7355, Val F1: 0.6908, Val Acc: 0.7233\n",
            "Epoch 5/30: Train Loss: 0.5787 | Train F1: 0.7156 | Train Acc: 0.7278 | Val Loss: 0.7355 | Val F1: 0.6908 | Val Acc: 0.7233\n",
            "\n",
            "--- Epoch 6/30 ---\n",
            "Train Loss: 0.5394, Train F1: 0.7310, Train Acc: 0.7435\n",
            "Val Loss: 0.7533, Val F1: 0.6589, Val Acc: 0.6845\n",
            "Epoch 6/30: Train Loss: 0.5394 | Train F1: 0.7310 | Train Acc: 0.7435 | Val Loss: 0.7533 | Val F1: 0.6589 | Val Acc: 0.6845\n",
            "\n",
            "--- Epoch 7/30 ---\n",
            "Train Loss: 0.5107, Train F1: 0.7474, Train Acc: 0.7532\n",
            "Val Loss: 0.8688, Val F1: 0.6633, Val Acc: 0.7168\n",
            "Epoch 7/30: Train Loss: 0.5107 | Train F1: 0.7474 | Train Acc: 0.7532 | Val Loss: 0.8688 | Val F1: 0.6633 | Val Acc: 0.7168\n",
            "\n",
            "--- Epoch 8/30 ---\n",
            "Train Loss: 0.4539, Train F1: 0.7793, Train Acc: 0.7846\n",
            "Val Loss: 0.8011, Val F1: 0.6837, Val Acc: 0.7039\n",
            "Epoch 8/30: Train Loss: 0.4539 | Train F1: 0.7793 | Train Acc: 0.7846 | Val Loss: 0.8011 | Val F1: 0.6837 | Val Acc: 0.7039\n",
            "\n",
            "--- Epoch 9/30 ---\n",
            "Train Loss: 0.4064, Train F1: 0.8017, Train Acc: 0.8012\n",
            "Val Loss: 0.8799, Val F1: 0.6549, Val Acc: 0.6699\n",
            "Epoch 9/30: Train Loss: 0.4064 | Train F1: 0.8017 | Train Acc: 0.8012 | Val Loss: 0.8799 | Val F1: 0.6549 | Val Acc: 0.6699\n",
            "\n",
            "--- Epoch 10/30 ---\n",
            "Train Loss: 0.3850, Train F1: 0.8073, Train Acc: 0.8075\n",
            "Val Loss: 0.9918, Val F1: 0.6780, Val Acc: 0.7152\n",
            "Epoch 10/30: Train Loss: 0.3850 | Train F1: 0.8073 | Train Acc: 0.8075 | Val Loss: 0.9918 | Val F1: 0.6780 | Val Acc: 0.7152\n",
            "\n",
            "--- Epoch 11/30 ---\n",
            "Train Loss: 0.3973, Train F1: 0.8125, Train Acc: 0.8158\n",
            "Val Loss: 0.9899, Val F1: 0.6413, Val Acc: 0.6812\n",
            "Epoch 11/30: Train Loss: 0.3973 | Train F1: 0.8125 | Train Acc: 0.8158 | Val Loss: 0.9899 | Val F1: 0.6413 | Val Acc: 0.6812\n",
            "\n",
            "--- Epoch 12/30 ---\n",
            "Train Loss: 0.3680, Train F1: 0.8269, Train Acc: 0.8292\n",
            "Val Loss: 0.9943, Val F1: 0.6570, Val Acc: 0.6845\n",
            "Epoch 12/30: Train Loss: 0.3680 | Train F1: 0.8269 | Train Acc: 0.8292 | Val Loss: 0.9943 | Val F1: 0.6570 | Val Acc: 0.6845\n",
            "\n",
            "--- Epoch 13/30 ---\n",
            "Train Loss: 0.3034, Train F1: 0.8541, Train Acc: 0.8515\n",
            "Val Loss: 1.0788, Val F1: 0.6707, Val Acc: 0.6990\n",
            "Epoch 13/30: Train Loss: 0.3034 | Train F1: 0.8541 | Train Acc: 0.8515 | Val Loss: 1.0788 | Val F1: 0.6707 | Val Acc: 0.6990\n",
            "\n",
            "--- Epoch 14/30 ---\n",
            "Train Loss: 0.3062, Train F1: 0.8495, Train Acc: 0.8503\n",
            "Val Loss: 1.0997, Val F1: 0.6576, Val Acc: 0.6877\n",
            "Epoch 14/30: Train Loss: 0.3062 | Train F1: 0.8495 | Train Acc: 0.8503 | Val Loss: 1.0997 | Val F1: 0.6576 | Val Acc: 0.6877\n",
            "\n",
            "--- Epoch 15/30 ---\n",
            "Train Loss: 0.2856, Train F1: 0.8630, Train Acc: 0.8612\n",
            "Val Loss: 1.1207, Val F1: 0.6450, Val Acc: 0.6667\n",
            "Epoch 15/30: Train Loss: 0.2856 | Train F1: 0.8630 | Train Acc: 0.8612 | Val Loss: 1.1207 | Val F1: 0.6450 | Val Acc: 0.6667\n",
            "\n",
            "--- Epoch 16/30 ---\n",
            "Train Loss: 0.2552, Train F1: 0.8844, Train Acc: 0.8803\n",
            "Val Loss: 1.1510, Val F1: 0.6706, Val Acc: 0.7023\n",
            "Epoch 16/30: Train Loss: 0.2552 | Train F1: 0.8844 | Train Acc: 0.8803 | Val Loss: 1.1510 | Val F1: 0.6706 | Val Acc: 0.7023\n",
            "\n",
            "--- Epoch 17/30 ---\n",
            "Train Loss: 0.2351, Train F1: 0.8894, Train Acc: 0.8872\n",
            "Val Loss: 1.2117, Val F1: 0.6671, Val Acc: 0.6958\n",
            "Epoch 17/30: Train Loss: 0.2351 | Train F1: 0.8894 | Train Acc: 0.8872 | Val Loss: 1.2117 | Val F1: 0.6671 | Val Acc: 0.6958\n",
            "\n",
            "--- Epoch 18/30 ---\n",
            "Train Loss: 0.2202, Train F1: 0.8985, Train Acc: 0.8943\n",
            "Val Loss: 1.2646, Val F1: 0.6705, Val Acc: 0.6974\n",
            "Epoch 18/30: Train Loss: 0.2202 | Train F1: 0.8985 | Train Acc: 0.8943 | Val Loss: 1.2646 | Val F1: 0.6705 | Val Acc: 0.6974\n",
            "\n",
            "--- Epoch 19/30 ---\n",
            "Train Loss: 0.2135, Train F1: 0.9016, Train Acc: 0.8975\n",
            "Val Loss: 1.2589, Val F1: 0.6733, Val Acc: 0.6990\n",
            "Epoch 19/30: Train Loss: 0.2135 | Train F1: 0.9016 | Train Acc: 0.8975 | Val Loss: 1.2589 | Val F1: 0.6733 | Val Acc: 0.6990\n",
            "\n",
            "--- Epoch 20/30 ---\n",
            "Train Loss: 0.2040, Train F1: 0.9072, Train Acc: 0.9040\n",
            "Val Loss: 1.2879, Val F1: 0.6689, Val Acc: 0.6974\n",
            "Epoch 20/30: Train Loss: 0.2040 | Train F1: 0.9072 | Train Acc: 0.9040 | Val Loss: 1.2879 | Val F1: 0.6689 | Val Acc: 0.6974\n",
            "\n",
            "--- Epoch 21/30 ---\n",
            "Train Loss: 0.1923, Train F1: 0.9160, Train Acc: 0.9129\n",
            "Val Loss: 1.3521, Val F1: 0.6787, Val Acc: 0.7087\n",
            "Epoch 21/30: Train Loss: 0.1923 | Train F1: 0.9160 | Train Acc: 0.9129 | Val Loss: 1.3521 | Val F1: 0.6787 | Val Acc: 0.7087\n",
            "\n",
            "--- Epoch 22/30 ---\n",
            "Train Loss: 0.1928, Train F1: 0.9081, Train Acc: 0.9060\n",
            "Val Loss: 1.3472, Val F1: 0.6701, Val Acc: 0.6974\n",
            "Epoch 22/30: Train Loss: 0.1928 | Train F1: 0.9081 | Train Acc: 0.9060 | Val Loss: 1.3472 | Val F1: 0.6701 | Val Acc: 0.6974\n",
            "\n",
            "--- Epoch 23/30 ---\n",
            "Train Loss: 0.1787, Train F1: 0.9173, Train Acc: 0.9155\n",
            "Val Loss: 1.4201, Val F1: 0.6739, Val Acc: 0.7023\n",
            "Epoch 23/30: Train Loss: 0.1787 | Train F1: 0.9173 | Train Acc: 0.9155 | Val Loss: 1.4201 | Val F1: 0.6739 | Val Acc: 0.7023\n",
            "\n",
            "--- Epoch 24/30 ---\n",
            "Train Loss: 0.1832, Train F1: 0.9124, Train Acc: 0.9106\n",
            "Val Loss: 1.4240, Val F1: 0.6749, Val Acc: 0.7039\n",
            "Epoch 24/30: Train Loss: 0.1832 | Train F1: 0.9124 | Train Acc: 0.9106 | Val Loss: 1.4240 | Val F1: 0.6749 | Val Acc: 0.7039\n",
            "\n",
            "--- Epoch 25/30 ---\n",
            "Train Loss: 0.1772, Train F1: 0.9156, Train Acc: 0.9126\n",
            "Val Loss: 1.4332, Val F1: 0.6736, Val Acc: 0.7023\n",
            "Epoch 25/30: Train Loss: 0.1772 | Train F1: 0.9156 | Train Acc: 0.9126 | Val Loss: 1.4332 | Val F1: 0.6736 | Val Acc: 0.7023\n",
            "\n",
            "--- Epoch 26/30 ---\n",
            "Train Loss: 0.1761, Train F1: 0.9232, Train Acc: 0.9209\n",
            "Val Loss: 1.4328, Val F1: 0.6720, Val Acc: 0.7006\n",
            "Epoch 26/30: Train Loss: 0.1761 | Train F1: 0.9232 | Train Acc: 0.9209 | Val Loss: 1.4328 | Val F1: 0.6720 | Val Acc: 0.7006\n",
            "\n",
            "--- Epoch 27/30 ---\n",
            "Train Loss: 0.1741, Train F1: 0.9204, Train Acc: 0.9177\n",
            "Val Loss: 1.4520, Val F1: 0.6712, Val Acc: 0.6990\n",
            "Epoch 27/30: Train Loss: 0.1741 | Train F1: 0.9204 | Train Acc: 0.9177 | Val Loss: 1.4520 | Val F1: 0.6712 | Val Acc: 0.6990\n",
            "\n",
            "--- Epoch 28/30 ---\n",
            "Train Loss: 0.1753, Train F1: 0.9200, Train Acc: 0.9166\n",
            "Val Loss: 1.4428, Val F1: 0.6727, Val Acc: 0.7006\n",
            "Epoch 28/30: Train Loss: 0.1753 | Train F1: 0.9200 | Train Acc: 0.9166 | Val Loss: 1.4428 | Val F1: 0.6727 | Val Acc: 0.7006\n",
            "\n",
            "--- Epoch 29/30 ---\n",
            "Train Loss: 0.1700, Train F1: 0.9219, Train Acc: 0.9209\n",
            "Val Loss: 1.4563, Val F1: 0.6721, Val Acc: 0.6990\n",
            "Epoch 29/30: Train Loss: 0.1700 | Train F1: 0.9219 | Train Acc: 0.9209 | Val Loss: 1.4563 | Val F1: 0.6721 | Val Acc: 0.6990\n",
            "\n",
            "--- Epoch 30/30 ---\n",
            "Train Loss: 0.1643, Train F1: 0.9268, Train Acc: 0.9243\n",
            "Val Loss: 1.4538, Val F1: 0.6738, Val Acc: 0.7023\n",
            "Epoch 30/30: Train Loss: 0.1643 | Train F1: 0.9268 | Train Acc: 0.9243 | Val Loss: 1.4538 | Val F1: 0.6738 | Val Acc: 0.7023\n",
            "\n",
            "========== Plotting Learning Curves ==========\n",
            "Figure(1200x1500)\n",
            "Learning curves saved as 'outputs/rnn_f1_learning_curves.png'.\n",
            "Figure(800x600)\n",
            "Accuracy curve saved as 'outputs/rnn_accuracy_learning_curve.png'.\n",
            "\n",
            "========== Evaluating on Test Set ==========\n",
            "                                   \n",
            "==================================================\n",
            "Final Test Accuracy: 0.7001\n",
            "Test F1 Macro: 0.6696\n",
            "Test F1 Weighted: 0.7066\n",
            "==================================================\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "Negative (0)     0.4967    0.8352    0.6230        91\n",
            " Neutral (1)     0.8333    0.7060    0.7644       432\n",
            "Positive (2)     0.6154    0.6275    0.6214       204\n",
            "\n",
            "    accuracy                         0.7001       727\n",
            "   macro avg     0.6485    0.7229    0.6696       727\n",
            "weighted avg     0.7300    0.7001    0.7066       727\n",
            "\n",
            "Figure(800x600)\n",
            "Confusion matrix saved as 'outputs/rnn_confusion_matrix.png'.\n",
            "\n",
            "Per-class F1 Scores:\n",
            "Negative (0): 0.6230\n",
            "Neutral (1): 0.7644\n",
            "Positive (2): 0.6214\n",
            "\n",
            "========== Script Complete ==========\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r outputs.zip outputs\n",
        "from google.colab import files\n",
        "files.download(\"outputs.zip\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        },
        "id": "rSLeOm7cZZyR",
        "outputId": "f9ee1220-d843-46cb-c827-b6bfae5a61f3"
      },
      "id": "rSLeOm7cZZyR",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "updating: outputs/ (stored 0%)\n",
            "updating: outputs/rnn_accuracy_learning_curve.png (deflated 7%)\n",
            "updating: outputs/mlp_learning_curves.png (deflated 12%)\n",
            "updating: outputs/best_mlp_fasttext.pt (deflated 8%)\n",
            "updating: outputs/mlp_confusion_matrix.png (deflated 19%)\n",
            "updating: outputs/mlp_f1_curve.png (deflated 10%)\n",
            "updating: outputs/rnn_confusion_matrix.png (deflated 15%)\n",
            "updating: outputs/mlp_accuracy_curve.png (deflated 9%)\n",
            "updating: outputs/best_rnn_model.pth (deflated 8%)\n",
            "updating: outputs/mlp_loss_curve.png (deflated 12%)\n",
            "updating: outputs/mlp_confusion_matrix.txt (stored 0%)\n",
            "updating: outputs/rnn_f1_learning_curves.png (deflated 9%)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_c411179a-2bdf-4f25-8bfb-0105dadeb90f\", \"outputs.zip\", 1726944)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f1511ab5",
      "metadata": {
        "id": "f1511ab5"
      },
      "source": [
        "###  Evaluation Requirements **(10 points)**\n",
        "\n",
        "For **both models (MLP and LSTM)**, you must:\n",
        "\n",
        "- Train for **at least 30 epochs**. You may train longer and select the best checkpoint based on validation performance (early stopping is allowed **after** epoch 30).\n",
        "- Track and plot the following metrics for **both training and validation** sets:\n",
        "  - **Loss vs. Epochs**\n",
        "  - **Accuracy vs. Epochs**\n",
        "  - **Macro F1 Score vs. Epochs**\n",
        "\n",
        "Plotting both training and validation curves helps you identify potential issues like **underfitting** or **overfitting**.\n",
        "\n",
        "- After training, evaluate your model on the **test set** and report the **confusion matrix**.\n",
        "- Save plots (training/validation curves and confusion matrix) to disk from your **.py scripts** so they can be embedded in `open_questions.ipynb`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b3c6bc92",
      "metadata": {
        "id": "b3c6bc92"
      },
      "source": [
        "## Provided Models (Required) **(12 points)**\n",
        "\n",
        "The following scripts are provided to support comparison between classical baselines and fine-tuned LLMs:\n",
        "\n",
        "- **`train_sentiment_rnn_classifier.py`** - Sentiment classifier using a basic RNN architecture  \n",
        "- **`train_sentiment_gru_classifier.py`** - Sentiment classifier using a GRU architecture  \n",
        "- **`train_sentiment_bert_classifier.py`** - Sentiment classifier using a BERT-based model  \n",
        "- **`train_sentiment_gpt_classifier.py`** - Sentiment classifier using a GPT-based model  \n",
        "\n",
        "You must run these models and include their results in your analysis (metrics, plots, and a brief comparison). BERT and GPT are pretrained LLMs that you will **fine-tune** for classification using these scripts. These scripts are **not** submissions and may use different training settings (e.g., fewer epochs).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c1293a37",
      "metadata": {
        "id": "c1293a37"
      },
      "source": [
        "## Open-Ended Reflection Questions **(23 points)**\n",
        "\n",
        "After completing your implementations and running all provided scripts, in the notebook named `open_questions.ipynb` to address the following. You may **Include plots** from your training scripts in the notebook output to justify your answers.\n",
        "\n",
        "### 1. Training Dynamics\n",
        "*Focus on your MLP and LSTM implementations*\n",
        "\n",
        "- Did your models show signs of **overfitting** or **underfitting**? What architectural or training changes could address this?\n",
        "- How did using **class weights** affect training stability and final performance?\n",
        "\n",
        "### 2. Model Performance and Error Analysis\n",
        "*Focus on your MLP and LSTM implementations*\n",
        "\n",
        "- Which of your two models **generalized better** to the test set? Provide evidence from your metrics.\n",
        "- Which **sentiment class** was most frequently misclassified? Propose reasons for this pattern.\n",
        "\n",
        "### 3. Cross-Model Comparison\n",
        "*Compare all six models: MLP, RNN, LSTM, GRU, BERT, GPT*\n",
        "\n",
        "- How did **mean-pooled FastText embeddings** limit the MLP compared to sequence-based models?\n",
        "- What advantage did the LSTM's **sequential processing** provide over the MLP?\n",
        "- Did **fine-tuned LLMs** (BERT/GPT) outperform classical baselines? Explain the performance gap in terms of pretraining and contextual representations.\n",
        "- **Rank all six models** by test performance. What architectural or representational factors explain the ranking?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e68d2d95",
      "metadata": {
        "id": "e68d2d95"
      },
      "source": [
        "## AI Use Disclosure **(5 points)**\n",
        "\n",
        "Complete the **AI Use Disclosure** section in `open_questions.ipynb`. This item is graded separately.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "83029649",
      "metadata": {
        "id": "83029649"
      },
      "source": [
        "## Deliverables\n",
        "\n",
        "You must submit the following files:\n",
        "\n",
        "1. `train_sentiment_mlp_classifier.py`  \n",
        "   Implementation of **Task 1** using an MLP with **mean-pooled FastText sentence embeddings**.\n",
        "\n",
        "2. `train_sentiment_lstm_classifier.py`  \n",
        "   Implementation of **Task 2** using an LSTM with **padded/truncated FastText word vectors** (32 tokens per sentence).\n",
        "\n",
        "3. `outputs/` containing PNGs for loss/accuracy/F1 curves and confusion matrices for **all models you ran** (MLP, LSTM, RNN, GRU, BERT, GPT).\n",
        "\n",
        "4. `open_questions.ipynb` and `open_questions.html`  \n",
        "   Your written responses to the **open-ended questions** related to modeling choices, performance comparisons, and reflections. The HTML must include the **plots embedded in the notebook output**, plus your **AI Use Disclosure**.\n",
        "\n",
        "Submission Instructions\n",
        "\n",
        "- Submit `open_questions.html` to **Canvas**.\n",
        "- Push **all `.py`, `.ipynb`, `.html`, and `outputs/` files** to your **GitHub repository**.\n",
        "- Make sure the `.html` file contains **both code and output** so it can be viewed without rerunning the notebook.\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}