{
  "total_epochs": 10,
  "total_steps": 22500,
  "final_train_loss": 0.007845493964850903,
  "final_val_loss": 0.004264479037374258,
  "best_val_loss": 0.004264479037374258,
  "model_config": {
    "vocab_size": 276,
    "d_model": 256,
    "nhead": 8,
    "num_layers": 6,
    "dim_feedforward": 1024,
    "dropout": 0.1,
    "max_seq_length": 512
  },
  "training_config": {
    "learning_rate": 5e-05,
    "batch_size": 16,
    "num_epochs": 10,
    "warmup_steps": 500,
    "gradient_clip": 1.0,
    "save_every": 500,
    "eval_every": 500,
    "device": "cuda",
    "lora_config": null
  },
  "tokenizer_vocab_size": 276,
  "foundational_checkpoint": "arithmetic_llm/models/foundational_20260220_223759_565501/best_model.pt"
}