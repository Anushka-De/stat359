{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "021690bc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "021690bc",
        "outputId": "9d0561c5-0a3b-4454-bb4c-78b68d956e56"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'stat359'...\n",
            "remote: Enumerating objects: 371, done.\u001b[K\n",
            "remote: Counting objects: 100% (169/169), done.\u001b[K\n",
            "remote: Compressing objects: 100% (81/81), done.\u001b[K\n",
            "remote: Total 371 (delta 137), reused 93 (delta 88), pack-reused 202 (from 1)\u001b[K\n",
            "Receiving objects: 100% (371/371), 6.01 MiB | 17.23 MiB/s, done.\n",
            "Resolving deltas: 100% (204/204), done.\n",
            "/content/stat359/student/Final_Project\n",
            "arithmetic_llm\tProposal.docx\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/Anushka-De/stat359.git\n",
        "%cd stat359/student/Final_Project\n",
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install numpy pandas gensim torch scikit-learn matplotlib ipywidgets nltk tqdm"
      ],
      "metadata": {
        "id": "mho6YpH99zcy",
        "outputId": "f03e4d81-bbb6-4b22-a2f5-b2bc5d5231ed",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "mho6YpH99zcy",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.9/27.9 MB\u001b[0m \u001b[31m37.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m91.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p arithmetic_llm/data\n",
        "!mkdir -p arithmetic_llm/models\n",
        "!mkdir -p arithmetic_llm/evaluation_results\n",
        "!mkdir -p arithmetic_llm/analysis\n",
        "!mkdir -p arithmetic_llm/experiments\n"
      ],
      "metadata": {
        "id": "3n3QmUV9BTP5"
      },
      "id": "3n3QmUV9BTP5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Generate training corpus (100,000 samples recommended)\n",
        "# Generate foundational training corpus with 100K samples (plain text)\n",
        "# This large corpus provides the base model with extensive arithmetic patterns\n",
        "!python -m arithmetic_llm.generate_foundational_plaintext \\\n",
        "  --num-samples 100000 \\\n",
        "  --max-depth 4 \\\n",
        "  --num-range 1 20 \\\n",
        "  --invalid-rate 0.05 \\\n",
        "  --output-txt arithmetic_llm/data/foundational_corpus.txt\n",
        "\n",
        "# Generate mixed instruction corpus (valid + invalid)\n",
        "# This creates a balanced dataset without writing intermediate files\n",
        "!python -m arithmetic_llm.generate_instruction_corpus_mixed \\\n",
        "  --num-samples 20000 \\\n",
        "  --max-depth 4 \\\n",
        "  --num-range 1 20 \\\n",
        "  --invalid-rate 0 \\\n",
        "  --output-mixed arithmetic_llm/data/instruction_corpus.txt\n",
        "\n",
        "# Generate separate test corpus for evaluation (10K samples, minimal errors)\n",
        "# This provides a clean test set with only 1% invalid expressions\n",
        "!python -m arithmetic_llm.generate_corpus \\\n",
        "  --instruction-only \\\n",
        "  --num-samples 1000 \\\n",
        "  --max-depth 4 \\\n",
        "  --output-instruction arithmetic_llm/data/instruction_corpus_test.txt \\\n",
        "  --num-range 1 20 \\\n",
        "  --invalid-rate 0\n",
        "\n",
        "#check line counts\n",
        "!python -c \"import sys; [print(f'{sum(1 for _ in open(f))} {f}') for f in ['arithmetic_llm/data/foundational_corpus.txt', 'arithmetic_llm/data/instruction_corpus.txt', 'arithmetic_llm/data/instruction_corpus_test.txt']]\"\n",
        "\n"
      ],
      "metadata": {
        "id": "jc07_tF0_P3d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "477f53a1-9e53-4d83-8686-ec7fcaf8fd73"
      },
      "id": "jc07_tF0_P3d",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating instruction corpus with 1000 samples...\n",
            "Instruction corpus saved to: arithmetic_llm/data/instruction_corpus_test.txt\n",
            "Corpus generation complete!\n",
            "200000 arithmetic_llm/data/foundational_corpus.txt\n",
            "40000 arithmetic_llm/data/instruction_corpus.txt\n",
            "1000 arithmetic_llm/data/instruction_corpus_test.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#2. Tokenizer Traning\n",
        "!python -m arithmetic_llm.train_tokenizer \\\n",
        "  --corpus-path arithmetic_llm/data/foundational_corpus.txt \\\n",
        "  --vocab-size 1000 \\\n",
        "  --output-dir arithmetic_llm/data/tokenizer\n",
        "\n",
        "# show tokenizer table\n",
        "!python -m arithmetic_llm.print_token_table \\\n",
        "  --tokenizer_path arithmetic_llm/data/tokenizer/tokenizer.pkl \\\n",
        "  > tokens.csv\n",
        "\n",
        "# Analyze your instruction corpus\n",
        "!python -m arithmetic_llm.check_sequence_lengths \\\n",
        "  --corpus-path arithmetic_llm/data/instruction_corpus.txt \\\n",
        "  --tokenizer-path arithmetic_llm/data/tokenizer\n"
      ],
      "metadata": {
        "id": "tcxuzdRYEIu2",
        "outputId": "f8c17e89-d9a4-418c-f0a4-b2029df20091",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "tcxuzdRYEIu2",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training BPE tokenizer with vocabulary size 1000...\n",
            "Corpus: arithmetic_llm/data/foundational_corpus.txt\n",
            "Building corpus: 200000it [00:07, 26710.35it/s]\n",
            "BPE merges:  19% 186/1000 [00:00<00:00, 4371.34it/s]\n",
            "Saving tokenizer to: arithmetic_llm/data/tokenizer\n",
            "\n",
            "Tokenizer Statistics:\n",
            "  Vocabulary size: 274\n",
            "  BPE merge operations: 186\n",
            "  Special tokens: <pad>, <unk>, <bos>, <eos>, <think>, </think>\n",
            "\n",
            "Test encoding:\n",
            "  Input: 5 + 10 - 3\n",
            "  Encoded (with BOS/EOS): [174, 120, 8, 28, 11, 96, 175]\n",
            "  Decoded: <bos> 5 + 10 - 3 <eos>\n",
            "  Encoded (without BOS/EOS): [120, 8, 28, 11, 96]\n",
            "\n",
            "Test encoding:\n",
            "  Input: 12 - (4 + 2)\n",
            "  Encoded (with BOS/EOS): [174, 49, 11, 4, 108, 8, 84, 6, 175]\n",
            "  Decoded: <bos> 12 - ( 4 + 2 ) <eos>\n",
            "  Encoded (without BOS/EOS): [49, 11, 4, 108, 8, 84, 6]\n",
            "\n",
            "Test encoding:\n",
            "  Input: ((7+3)-(3+5))\n",
            "  Encoded (with BOS/EOS): [174, 4, 4, 144, 8, 96, 6, 11, 4, 96, 8, 120, 6, 6, 175]\n",
            "  Decoded: <bos> ( ( 7 + 3 ) - ( 3 + 5 ) ) <eos>\n",
            "  Encoded (without BOS/EOS): [4, 4, 144, 8, 96, 6, 11, 4, 96, 8, 120, 6, 6]\n",
            "\n",
            "Tokenizer training complete!\n",
            "Auto-detected corpus type: instruction\n",
            "Loading tokenizer from: arithmetic_llm/data/tokenizer\n",
            "\n",
            "Analyzing corpus: arithmetic_llm/data/instruction_corpus.txt\n",
            "Corpus type: instruction\n",
            "Analyzing: all lines\n",
            "\n",
            "============================================================\n",
            "SEQUENCE LENGTH ANALYSIS\n",
            "============================================================\n",
            "\n",
            "Total sequences analyzed: 40000\n",
            "\n",
            "Basic Statistics:\n",
            "  Min length:         12 tokens\n",
            "  Max length:        691 tokens\n",
            "  Mean length:     165.0 tokens\n",
            "  Median length:   126.0 tokens\n",
            "  Std deviation:   159.6 tokens\n",
            "\n",
            "Percentiles:\n",
            "   50.0th percentile:    126 tokens\n",
            "   75.0th percentile:    250 tokens\n",
            "   90.0th percentile:    402 tokens\n",
            "   95.0th percentile:    470 tokens\n",
            "   99.0th percentile:    594 tokens\n",
            "   99.5th percentile:    612 tokens\n",
            "  100.0th percentile:    691 tokens\n",
            "\n",
            "Coverage by max_seq_length:\n",
            "  max_seq_length=  64:  15491/40000 ( 38.7%) | 24509 truncated\n",
            "  max_seq_length= 128:  20651/40000 ( 51.6%) | 19349 truncated\n",
            "  max_seq_length= 192:  23968/40000 ( 59.9%) | 16032 truncated\n",
            "  max_seq_length= 256:  30518/40000 ( 76.3%) |  9482 truncated\n",
            "  max_seq_length= 384:  34553/40000 ( 86.4%) |  5447 truncated\n",
            "  max_seq_length= 512:  38338/40000 ( 95.8%) |  1662 truncated\n",
            "  max_seq_length= 768:  40000/40000 (100.0%) |     0 truncated\n",
            "  max_seq_length=1024:  40000/40000 (100.0%) |     0 truncated\n",
            "\n",
            "Recommendations:\n",
            "  • For 95% coverage: max_seq_length >= 470\n",
            "  • For 99% coverage: max_seq_length >= 594\n",
            "  • For 100% coverage: max_seq_length >= 691\n",
            "\n",
            "  Practical suggestions:\n",
            "  • Balanced (95% coverage): --max-seq-length 512\n",
            "  • Conservative (99% coverage): --max-seq-length 768\n",
            "============================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls arithmetic_llm/data\n"
      ],
      "metadata": {
        "id": "7QSTAlP4EdLq",
        "outputId": "11c16c9d-9df9-4a95-8f30-04e1ad9c9227",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "7QSTAlP4EdLq",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "foundational_corpus.txt      instruction_corpus.txt\n",
            "instruction_corpus_test.txt  tokenizer\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Train foundational model\n",
        "!python -m arithmetic_llm.run_foundational_training \\\n",
        "  --corpus-path arithmetic_llm/data/foundational_corpus.txt \\\n",
        "  --output-dir arithmetic_llm/models \\\n",
        "  --tokenizer-path arithmetic_llm/data/tokenizer \\\n",
        "  --num-epochs 10 \\\n",
        "  --max-seq-length 512 \\\n",
        "  --batch-size 16 \\\n",
        "  --device auto\n"
      ],
      "metadata": {
        "id": "PnfIUWSAHJ8g",
        "outputId": "c66e5890-7b23-4fa0-b56c-53e76feab155",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "PnfIUWSAHJ8g",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "FOUNDATIONAL MODEL TRAINING\n",
            "============================================================\n",
            "\n",
            "Corpus: arithmetic_llm/data/foundational_corpus.txt\n",
            "Tokenizer: arithmetic_llm/data/tokenizer\n",
            "Output directory: arithmetic_llm/models\n",
            "\n",
            "Training Configuration:\n",
            "  Learning rate: 0.0001\n",
            "  Batch size: 16\n",
            "  Epochs: 10\n",
            "  Warmup steps: 1000\n",
            "  Gradient clip: 1.0\n",
            "  Save every: 1000 steps\n",
            "  Device: cpu\n",
            "\n",
            "Model Configuration:\n",
            "  d_model: 256\n",
            "  nhead: 8\n",
            "  num_layers: 6\n",
            "  dim_feedforward: 1024\n",
            "  dropout: 0.1\n",
            "  max_seq_length: 512\n",
            "============================================================\n",
            "\n",
            "Training output directory: arithmetic_llm/models/foundational_20260219_234605_495185\n",
            "Configuration: {'learning_rate': 0.0001, 'batch_size': 16, 'num_epochs': 10, 'warmup_steps': 1000, 'gradient_clip': 1.0, 'save_every': 1000, 'eval_every': 500, 'device': 'cpu', 'lora_config': None}\n",
            "Loading tokenizer...\n",
            "Tokenizer vocabulary size: 271\n",
            "Initializing model configuration...\n",
            "Creating dataloaders...\n",
            "Training batches: 11250\n",
            "Validation batches: 1250\n",
            "Initializing model...\n",
            "Model parameters: 4,939,520\n",
            "\n",
            "Starting training...\n",
            "\n",
            "============================================================\n",
            "Epoch 1/10\n",
            "============================================================\n",
            "Epoch 1:   5% 539/11250 [1:12:28<26:17:36,  8.84s/it, loss=2.12, avg_loss=3.29, lr=5.31e-5]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#3.1 Evaluate the foundational model, performance would be bad\n",
        "!python -m arithmetic_llm.run_evaluation \\\n",
        "  --model-path arithmetic_llm/models/foundational_20260219_182844_242048/best_model.pt \\\n",
        "  --tokenizer-path arithmetic_llm/data/tokenizer \\\n",
        "  --max-gen-length 512 \\\n",
        "  --batch-size 1 \\\n",
        "  --num-samples 100 \\\n",
        "  --output-dir arithmetic_llm/evaluation_results/foundational_eval"
      ],
      "metadata": {
        "id": "4CAD-ndT6PHP",
        "outputId": "8b6cc711-a34e-47eb-8db6-ddbc46d3a09d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "4CAD-ndT6PHP",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "MODEL EVALUATION\n",
            "============================================================\n",
            "\n",
            "Model: arithmetic_llm/models/foundational_20260219_182844_242048/best_model.pt\n",
            "Tokenizer: arithmetic_llm/data/tokenizer\n",
            "Device: cuda\n",
            "\n",
            "Evaluation Configuration:\n",
            "  Test samples: 100\n",
            "  Max depth: 5\n",
            "  Number range: 1 to 20\n",
            "  Batch size: 1\n",
            "  Max generation length: 512\n",
            "  Output directory: arithmetic_llm/evaluation_results/foundational_eval\n",
            "============================================================\n",
            "\n",
            "Loading model and tokenizer...\n",
            "Model loaded successfully!\n",
            "\n",
            "Starting evaluation...\n",
            "Generating 100 test expressions...\n",
            "Generated 100 valid test expressions\n",
            "Evaluating model with batch size 1...\n",
            "Evaluated 1/100 samples\n",
            "Evaluated 2/100 samples\n",
            "Evaluated 3/100 samples\n",
            "Evaluated 4/100 samples\n",
            "Evaluated 5/100 samples\n",
            "Evaluated 6/100 samples\n",
            "Evaluated 7/100 samples\n",
            "Evaluated 8/100 samples\n",
            "Evaluated 9/100 samples\n",
            "Evaluated 10/100 samples\n",
            "Evaluated 11/100 samples\n",
            "Evaluated 12/100 samples\n",
            "Evaluated 13/100 samples\n",
            "Evaluated 14/100 samples\n",
            "Evaluated 15/100 samples\n",
            "Evaluated 16/100 samples\n",
            "Evaluated 17/100 samples\n",
            "Evaluated 18/100 samples\n",
            "Evaluated 19/100 samples\n",
            "Evaluated 20/100 samples\n",
            "Evaluated 21/100 samples\n",
            "Evaluated 22/100 samples\n",
            "Evaluated 23/100 samples\n",
            "Evaluated 24/100 samples\n",
            "Evaluated 25/100 samples\n",
            "Evaluated 26/100 samples\n",
            "Evaluated 27/100 samples\n",
            "Evaluated 28/100 samples\n",
            "Evaluated 29/100 samples\n",
            "Evaluated 30/100 samples\n",
            "Evaluated 31/100 samples\n",
            "Evaluated 32/100 samples\n",
            "Evaluated 33/100 samples\n",
            "Evaluated 34/100 samples\n",
            "Evaluated 35/100 samples\n",
            "Evaluated 36/100 samples\n",
            "Evaluated 37/100 samples\n",
            "Evaluated 38/100 samples\n",
            "Evaluated 39/100 samples\n",
            "Evaluated 40/100 samples\n",
            "Evaluated 41/100 samples\n",
            "Evaluated 42/100 samples\n",
            "Evaluated 43/100 samples\n",
            "Evaluated 44/100 samples\n",
            "Evaluated 45/100 samples\n",
            "Evaluated 46/100 samples\n",
            "Evaluated 47/100 samples\n",
            "Evaluated 48/100 samples\n",
            "Evaluated 49/100 samples\n",
            "Evaluated 50/100 samples\n",
            "Evaluated 51/100 samples\n",
            "Evaluated 52/100 samples\n",
            "Evaluated 53/100 samples\n",
            "Evaluated 54/100 samples\n",
            "Evaluated 55/100 samples\n",
            "Evaluated 56/100 samples\n",
            "Evaluated 57/100 samples\n",
            "Evaluated 58/100 samples\n",
            "Evaluated 59/100 samples\n",
            "Evaluated 60/100 samples\n",
            "Evaluated 61/100 samples\n",
            "Evaluated 62/100 samples\n",
            "Evaluated 63/100 samples\n",
            "Evaluated 64/100 samples\n",
            "Evaluated 65/100 samples\n",
            "Evaluated 66/100 samples\n",
            "Evaluated 67/100 samples\n",
            "Evaluated 68/100 samples\n",
            "Evaluated 69/100 samples\n",
            "Evaluated 70/100 samples\n",
            "Evaluated 71/100 samples\n",
            "Evaluated 72/100 samples\n",
            "Evaluated 73/100 samples\n",
            "Evaluated 74/100 samples\n",
            "Evaluated 75/100 samples\n",
            "Evaluated 76/100 samples\n",
            "Evaluated 77/100 samples\n",
            "Evaluated 78/100 samples\n",
            "Evaluated 79/100 samples\n",
            "Evaluated 80/100 samples\n",
            "Evaluated 81/100 samples\n",
            "Evaluated 82/100 samples\n",
            "Evaluated 83/100 samples\n",
            "Evaluated 84/100 samples\n",
            "Evaluated 85/100 samples\n",
            "Evaluated 86/100 samples\n",
            "Evaluated 87/100 samples\n",
            "Evaluated 88/100 samples\n",
            "Evaluated 89/100 samples\n",
            "Evaluated 90/100 samples\n",
            "Evaluated 91/100 samples\n",
            "Evaluated 92/100 samples\n",
            "Evaluated 93/100 samples\n",
            "Evaluated 94/100 samples\n",
            "Evaluated 95/100 samples\n",
            "Evaluated 96/100 samples\n",
            "Evaluated 97/100 samples\n",
            "Evaluated 98/100 samples\n",
            "Evaluated 99/100 samples\n",
            "Evaluated 100/100 samples\n",
            "\n",
            "Evaluation results saved to arithmetic_llm/evaluation_results/foundational_eval\n",
            "  - Metrics: arithmetic_llm/evaluation_results/foundational_eval/evaluation_metrics_20260219_215829.json\n",
            "  - Samples: arithmetic_llm/evaluation_results/foundational_eval/sample_outputs_20260219_215829.json\n",
            "  - Summary: arithmetic_llm/evaluation_results/foundational_eval/evaluation_summary_20260219_215829.txt\n",
            "\n",
            "============================================================\n",
            "EVALUATION RESULTS\n",
            "============================================================\n",
            "\n",
            "Total Samples: 100\n",
            "Correct Samples: 0\n",
            "Parseable Samples: 0\n",
            "\n",
            "Exact Match Accuracy: 0.00%\n",
            "Parse Success Rate: 0.00%\n",
            "Avg Generation Length: 45.44 tokens\n",
            "============================================================\n",
            "\n",
            "Interpretation:\n",
            "  ✗ Poor performance - model needs more training or debugging\n",
            "  ! Low parse success rate - model may need better instruction tuning\n",
            "============================================================\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls arithmetic_llm/models"
      ],
      "metadata": {
        "id": "7P_K1Aq9S214",
        "outputId": "3bd1fde3-4c9b-4dfd-e152-8d563f100559",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "7P_K1Aq9S214",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "foundational_20260219_182844_242048\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Fine-tune instruction model\n",
        "!python -m arithmetic_llm.run_instruction_training \\\n",
        "  --instruction-corpus-path arithmetic_llm/data/instruction_corpus.txt \\\n",
        "  --output-dir arithmetic_llm/models \\\n",
        "  --tokenizer-path arithmetic_llm/data/tokenizer \\\n",
        "  --foundational-checkpoint arithmetic_llm/models/foundational_20260219_182844_242048/best_model.pt \\\n",
        "  --num-epochs 10 \\\n",
        "  --batch-size 16 \\\n",
        "  --device auto"
      ],
      "metadata": {
        "id": "CoqjxRPe7ThA",
        "outputId": "81042ff4-aec8-4a98-ea4b-700bcf0b9ca9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "CoqjxRPe7ThA",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "INSTRUCTION FINE-TUNING\n",
            "============================================================\n",
            "\n",
            "Instruction corpus: arithmetic_llm/data/instruction_corpus.txt\n",
            "Tokenizer: arithmetic_llm/data/tokenizer\n",
            "Foundational checkpoint: arithmetic_llm/models/foundational_20260219_182844_242048/best_model.pt\n",
            "Output directory: arithmetic_llm/models\n",
            "\n",
            "Training Configuration:\n",
            "  Learning rate: 5e-05\n",
            "  Batch size: 16\n",
            "  Epochs: 10\n",
            "  Warmup steps: 500\n",
            "  Gradient clip: 1.0\n",
            "  Save every: 500 steps\n",
            "  Device: cuda\n",
            "============================================================\n",
            "\n",
            "Fine-tuning output directory: arithmetic_llm/models/instruction_20260219_220640_551798\n",
            "Configuration: {'learning_rate': 5e-05, 'batch_size': 16, 'num_epochs': 10, 'warmup_steps': 500, 'gradient_clip': 1.0, 'save_every': 500, 'eval_every': 500, 'device': 'cuda', 'lora_config': None}\n",
            "Loading tokenizer...\n",
            "Tokenizer vocabulary size: 274\n",
            "Initializing model architecture...\n",
            "Creating dataloaders...\n",
            "Training batches: 2250\n",
            "Validation batches: 250\n",
            "Loading foundational model from: arithmetic_llm/models/foundational_20260219_182844_242048/best_model.pt\n",
            "Loaded checkpoint from epoch 10, step 112500\n",
            "Model parameters: 4,940,288\n",
            "\n",
            "Starting instruction fine-tuning...\n",
            "\n",
            "============================================================\n",
            "Epoch 1/10\n",
            "============================================================\n",
            "Epoch 1:  22% 499/2250 [01:20<05:00,  5.83it/s, loss=0.0103, avg_loss=0.0939, lr=4.91e-5]\n",
            "Checkpoint saved at step 500: arithmetic_llm/models/instruction_20260219_220640_551798/checkpoint_step_500.pt\n",
            "Epoch 1:  44% 999/2250 [02:44<03:44,  5.57it/s, loss=0.00963, avg_loss=0.0533, lr=4.89e-5]\n",
            "Checkpoint saved at step 1000: arithmetic_llm/models/instruction_20260219_220640_551798/checkpoint_step_1000.pt\n",
            "Epoch 1:  67% 1499/2250 [04:11<02:02,  6.14it/s, loss=0.00861, avg_loss=0.0398, lr=4.77e-5]\n",
            "Checkpoint saved at step 1500: arithmetic_llm/models/instruction_20260219_220640_551798/checkpoint_step_1500.pt\n",
            "Epoch 1:  89% 1999/2250 [05:35<00:44,  5.64it/s, loss=0.0137, avg_loss=0.033, lr=4.66e-5]\n",
            "Checkpoint saved at step 2000: arithmetic_llm/models/instruction_20260219_220640_551798/checkpoint_step_2000.pt\n",
            "Epoch 1: 100% 2250/2250 [06:19<00:00,  5.93it/s, loss=0.00826, avg_loss=0.0307, lr=4.6e-5]\n",
            "\n",
            "Evaluating on validation set...\n",
            "\n",
            "Epoch 1 Summary:\n",
            "  Training Loss: 0.0306\n",
            "  Validation Loss: 0.0050\n",
            "  New best model saved: arithmetic_llm/models/instruction_20260219_220640_551798/best_model.pt\n",
            "\n",
            "============================================================\n",
            "Epoch 2/10\n",
            "============================================================\n",
            "Epoch 2:  11% 249/2250 [00:42<05:48,  5.74it/s, loss=0.0141, avg_loss=0.0112, lr=4.55e-5]\n",
            "Checkpoint saved at step 2500: arithmetic_llm/models/instruction_20260219_220640_551798/checkpoint_step_2500.pt\n",
            "Epoch 2:  33% 749/2250 [02:06<04:18,  5.81it/s, loss=0.00635, avg_loss=0.0115, lr=4.43e-5]\n",
            "Checkpoint saved at step 3000: arithmetic_llm/models/instruction_20260219_220640_551798/checkpoint_step_3000.pt\n",
            "Epoch 2:  56% 1249/2250 [03:32<02:30,  6.63it/s, loss=0.0129, avg_loss=0.0112, lr=4.32e-5]\n",
            "Checkpoint saved at step 3500: arithmetic_llm/models/instruction_20260219_220640_551798/checkpoint_step_3500.pt\n",
            "Epoch 2:  78% 1749/2250 [04:58<01:29,  5.61it/s, loss=0.014, avg_loss=0.0111, lr=4.21e-5]\n",
            "Checkpoint saved at step 4000: arithmetic_llm/models/instruction_20260219_220640_551798/checkpoint_step_4000.pt\n",
            "Epoch 2: 100% 2249/2250 [06:24<00:00,  5.89it/s, loss=0.00834, avg_loss=0.0108, lr=4.09e-5]\n",
            "Checkpoint saved at step 4500: arithmetic_llm/models/instruction_20260219_220640_551798/checkpoint_step_4500.pt\n",
            "Epoch 2: 100% 2250/2250 [06:24<00:00,  5.85it/s, loss=0.00834, avg_loss=0.0108, lr=4.09e-5]\n",
            "\n",
            "Evaluating on validation set...\n",
            "\n",
            "Epoch 2 Summary:\n",
            "  Training Loss: 0.0108\n",
            "  Validation Loss: 0.0043\n",
            "  New best model saved: arithmetic_llm/models/instruction_20260219_220640_551798/best_model.pt\n",
            "\n",
            "============================================================\n",
            "Epoch 3/10\n",
            "============================================================\n",
            "Epoch 3:  22% 499/2250 [01:25<05:00,  5.83it/s, loss=0.009, avg_loss=0.0102, lr=3.98e-5]\n",
            "Checkpoint saved at step 5000: arithmetic_llm/models/instruction_20260219_220640_551798/checkpoint_step_5000.pt\n",
            "Epoch 3:  44% 999/2250 [02:52<03:33,  5.85it/s, loss=0.00511, avg_loss=0.01, lr=3.87e-5]\n",
            "Checkpoint saved at step 5500: arithmetic_llm/models/instruction_20260219_220640_551798/checkpoint_step_5500.pt\n",
            "Epoch 3:  67% 1499/2250 [04:17<02:14,  5.58it/s, loss=0.012, avg_loss=0.00987, lr=3.75e-5]\n",
            "Checkpoint saved at step 6000: arithmetic_llm/models/instruction_20260219_220640_551798/checkpoint_step_6000.pt\n",
            "Epoch 3:  89% 1999/2250 [05:42<00:46,  5.45it/s, loss=0.00453, avg_loss=0.00969, lr=3.64e-5]\n",
            "Checkpoint saved at step 6500: arithmetic_llm/models/instruction_20260219_220640_551798/checkpoint_step_6500.pt\n",
            "Epoch 3: 100% 2250/2250 [06:24<00:00,  5.85it/s, loss=0.0101, avg_loss=0.00958, lr=3.58e-5]\n",
            "\n",
            "Evaluating on validation set...\n",
            "\n",
            "Epoch 3 Summary:\n",
            "  Training Loss: 0.0096\n",
            "  Validation Loss: 0.0038\n",
            "  New best model saved: arithmetic_llm/models/instruction_20260219_220640_551798/best_model.pt\n",
            "\n",
            "============================================================\n",
            "Epoch 4/10\n",
            "============================================================\n",
            "Epoch 4:  11% 249/2250 [00:43<05:34,  5.97it/s, loss=0.00475, avg_loss=0.00822, lr=3.52e-5]\n",
            "Checkpoint saved at step 7000: arithmetic_llm/models/instruction_20260219_220640_551798/checkpoint_step_7000.pt\n",
            "Epoch 4:  33% 749/2250 [02:09<04:26,  5.63it/s, loss=0.00872, avg_loss=0.00856, lr=3.41e-5]\n",
            "Checkpoint saved at step 7500: arithmetic_llm/models/instruction_20260219_220640_551798/checkpoint_step_7500.pt\n",
            "Epoch 4:  56% 1249/2250 [03:35<02:49,  5.91it/s, loss=0.0137, avg_loss=0.0084, lr=3.3e-5]\n",
            "Checkpoint saved at step 8000: arithmetic_llm/models/instruction_20260219_220640_551798/checkpoint_step_8000.pt\n",
            "Epoch 4:  78% 1749/2250 [05:00<01:26,  5.78it/s, loss=0.00871, avg_loss=0.00833, lr=3.18e-5]\n",
            "Checkpoint saved at step 8500: arithmetic_llm/models/instruction_20260219_220640_551798/checkpoint_step_8500.pt\n",
            "Epoch 4: 100% 2249/2250 [06:26<00:00,  5.42it/s, loss=0.00508, avg_loss=0.00828, lr=3.07e-5]\n",
            "Checkpoint saved at step 9000: arithmetic_llm/models/instruction_20260219_220640_551798/checkpoint_step_9000.pt\n",
            "Epoch 4: 100% 2250/2250 [06:26<00:00,  5.82it/s, loss=0.00508, avg_loss=0.00828, lr=3.07e-5]\n",
            "\n",
            "Evaluating on validation set...\n",
            "\n",
            "Epoch 4 Summary:\n",
            "  Training Loss: 0.0083\n",
            "  Validation Loss: 0.0032\n",
            "  New best model saved: arithmetic_llm/models/instruction_20260219_220640_551798/best_model.pt\n",
            "\n",
            "============================================================\n",
            "Epoch 5/10\n",
            "============================================================\n",
            "Epoch 5:  22% 499/2250 [01:25<04:52,  5.98it/s, loss=0.0112, avg_loss=0.00739, lr=2.96e-5]\n",
            "Checkpoint saved at step 9500: arithmetic_llm/models/instruction_20260219_220640_551798/checkpoint_step_9500.pt\n",
            "Epoch 5:  44% 999/2250 [02:50<03:13,  6.46it/s, loss=0.00578, avg_loss=0.0074, lr=2.84e-5]\n",
            "Checkpoint saved at step 10000: arithmetic_llm/models/instruction_20260219_220640_551798/checkpoint_step_10000.pt\n",
            "Epoch 5:  67% 1499/2250 [04:15<01:47,  7.00it/s, loss=0.00962, avg_loss=0.00725, lr=2.73e-5]\n",
            "Checkpoint saved at step 10500: arithmetic_llm/models/instruction_20260219_220640_551798/checkpoint_step_10500.pt\n",
            "Epoch 5:  89% 1999/2250 [05:42<00:39,  6.29it/s, loss=0.00989, avg_loss=0.00721, lr=2.62e-5]\n",
            "Checkpoint saved at step 11000: arithmetic_llm/models/instruction_20260219_220640_551798/checkpoint_step_11000.pt\n",
            "Epoch 5: 100% 2250/2250 [06:25<00:00,  5.84it/s, loss=0.0126, avg_loss=0.00715, lr=2.56e-5]\n",
            "\n",
            "Evaluating on validation set...\n",
            "\n",
            "Epoch 5 Summary:\n",
            "  Training Loss: 0.0072\n",
            "  Validation Loss: 0.0029\n",
            "  New best model saved: arithmetic_llm/models/instruction_20260219_220640_551798/best_model.pt\n",
            "\n",
            "============================================================\n",
            "Epoch 6/10\n",
            "============================================================\n",
            "Epoch 6:  11% 249/2250 [00:41<04:41,  7.11it/s, loss=0.00477, avg_loss=0.00679, lr=2.5e-5]\n",
            "Checkpoint saved at step 11500: arithmetic_llm/models/instruction_20260219_220640_551798/checkpoint_step_11500.pt\n",
            "Epoch 6:  33% 749/2250 [02:07<04:39,  5.38it/s, loss=0.00668, avg_loss=0.00673, lr=2.39e-5]\n",
            "Checkpoint saved at step 12000: arithmetic_llm/models/instruction_20260219_220640_551798/checkpoint_step_12000.pt\n",
            "Epoch 6:  56% 1249/2250 [03:33<02:50,  5.86it/s, loss=0.00451, avg_loss=0.00662, lr=2.27e-5]\n",
            "Checkpoint saved at step 12500: arithmetic_llm/models/instruction_20260219_220640_551798/checkpoint_step_12500.pt\n",
            "Epoch 6:  78% 1748/2250 [04:59<01:30,  5.56it/s, loss=0.00385, avg_loss=0.0065, lr=2.16e-5]\n",
            "Checkpoint saved at step 13000: arithmetic_llm/models/instruction_20260219_220640_551798/checkpoint_step_13000.pt\n",
            "Epoch 6: 100% 2249/2250 [06:26<00:00,  5.76it/s, loss=0.00417, avg_loss=0.00641, lr=2.05e-5]\n",
            "Checkpoint saved at step 13500: arithmetic_llm/models/instruction_20260219_220640_551798/checkpoint_step_13500.pt\n",
            "Epoch 6: 100% 2250/2250 [06:26<00:00,  5.82it/s, loss=0.00417, avg_loss=0.00641, lr=2.05e-5]\n",
            "\n",
            "Evaluating on validation set...\n",
            "\n",
            "Epoch 6 Summary:\n",
            "  Training Loss: 0.0064\n",
            "  Validation Loss: 0.0026\n",
            "  New best model saved: arithmetic_llm/models/instruction_20260219_220640_551798/best_model.pt\n",
            "\n",
            "============================================================\n",
            "Epoch 7/10\n",
            "============================================================\n",
            "Epoch 7:  22% 499/2250 [01:25<05:21,  5.45it/s, loss=0.00846, avg_loss=0.00614, lr=1.93e-5]\n",
            "Checkpoint saved at step 14000: arithmetic_llm/models/instruction_20260219_220640_551798/checkpoint_step_14000.pt\n",
            "Epoch 7:  44% 999/2250 [02:50<03:29,  5.96it/s, loss=0.0105, avg_loss=0.006, lr=1.82e-5]\n",
            "Checkpoint saved at step 14500: arithmetic_llm/models/instruction_20260219_220640_551798/checkpoint_step_14500.pt\n",
            "Epoch 7:  67% 1499/2250 [04:16<02:16,  5.51it/s, loss=0.00518, avg_loss=0.00589, lr=1.71e-5]\n",
            "Checkpoint saved at step 15000: arithmetic_llm/models/instruction_20260219_220640_551798/checkpoint_step_15000.pt\n",
            "Epoch 7:  89% 1999/2250 [05:42<00:43,  5.77it/s, loss=0.00717, avg_loss=0.0058, lr=1.59e-5]\n",
            "Checkpoint saved at step 15500: arithmetic_llm/models/instruction_20260219_220640_551798/checkpoint_step_15500.pt\n",
            "Epoch 7: 100% 2250/2250 [06:24<00:00,  5.86it/s, loss=0.00302, avg_loss=0.0058, lr=1.54e-5]\n",
            "\n",
            "Evaluating on validation set...\n",
            "\n",
            "Epoch 7 Summary:\n",
            "  Training Loss: 0.0058\n",
            "  Validation Loss: 0.0024\n",
            "  New best model saved: arithmetic_llm/models/instruction_20260219_220640_551798/best_model.pt\n",
            "\n",
            "============================================================\n",
            "Epoch 8/10\n",
            "============================================================\n",
            "Epoch 8:  11% 249/2250 [00:43<06:23,  5.22it/s, loss=0.00348, avg_loss=0.00533, lr=1.48e-5]\n",
            "Checkpoint saved at step 16000: arithmetic_llm/models/instruction_20260219_220640_551798/checkpoint_step_16000.pt\n",
            "Epoch 8:  33% 749/2250 [02:07<04:14,  5.91it/s, loss=0.00327, avg_loss=0.00528, lr=1.37e-5]\n",
            "Checkpoint saved at step 16500: arithmetic_llm/models/instruction_20260219_220640_551798/checkpoint_step_16500.pt\n",
            "Epoch 8:  56% 1249/2250 [03:33<03:01,  5.51it/s, loss=0.00561, avg_loss=0.00526, lr=1.25e-5]\n",
            "Checkpoint saved at step 17000: arithmetic_llm/models/instruction_20260219_220640_551798/checkpoint_step_17000.pt\n",
            "Epoch 8:  78% 1749/2250 [04:58<01:35,  5.25it/s, loss=0.00334, avg_loss=0.0052, lr=1.14e-5]\n",
            "Checkpoint saved at step 17500: arithmetic_llm/models/instruction_20260219_220640_551798/checkpoint_step_17500.pt\n",
            "Epoch 8: 100% 2249/2250 [06:24<00:00,  5.60it/s, loss=0.00726, avg_loss=0.0052, lr=1.02e-5]\n",
            "Checkpoint saved at step 18000: arithmetic_llm/models/instruction_20260219_220640_551798/checkpoint_step_18000.pt\n",
            "Epoch 8: 100% 2250/2250 [06:24<00:00,  5.85it/s, loss=0.00726, avg_loss=0.0052, lr=1.02e-5]\n",
            "\n",
            "Evaluating on validation set...\n",
            "\n",
            "Epoch 8 Summary:\n",
            "  Training Loss: 0.0052\n",
            "  Validation Loss: 0.0022\n",
            "  New best model saved: arithmetic_llm/models/instruction_20260219_220640_551798/best_model.pt\n",
            "\n",
            "============================================================\n",
            "Epoch 9/10\n",
            "============================================================\n",
            "Epoch 9:  22% 499/2250 [01:25<04:28,  6.53it/s, loss=0.00381, avg_loss=0.00492, lr=9.11e-6]\n",
            "Checkpoint saved at step 18500: arithmetic_llm/models/instruction_20260219_220640_551798/checkpoint_step_18500.pt\n",
            "Epoch 9:  44% 999/2250 [02:50<03:43,  5.60it/s, loss=0.0102, avg_loss=0.0049, lr=7.98e-6]\n",
            "Checkpoint saved at step 19000: arithmetic_llm/models/instruction_20260219_220640_551798/checkpoint_step_19000.pt\n",
            "Epoch 9:  67% 1499/2250 [04:14<02:15,  5.56it/s, loss=0.00402, avg_loss=0.00486, lr=6.84e-6]\n",
            "Checkpoint saved at step 19500: arithmetic_llm/models/instruction_20260219_220640_551798/checkpoint_step_19500.pt\n",
            "Epoch 9:  89% 1999/2250 [05:40<00:46,  5.37it/s, loss=0.00201, avg_loss=0.00481, lr=5.7e-6]\n",
            "Checkpoint saved at step 20000: arithmetic_llm/models/instruction_20260219_220640_551798/checkpoint_step_20000.pt\n",
            "Epoch 9: 100% 2250/2250 [06:23<00:00,  5.87it/s, loss=0.00401, avg_loss=0.00479, lr=5.13e-6]\n",
            "\n",
            "Evaluating on validation set...\n",
            "\n",
            "Epoch 9 Summary:\n",
            "  Training Loss: 0.0048\n",
            "  Validation Loss: 0.0021\n",
            "  New best model saved: arithmetic_llm/models/instruction_20260219_220640_551798/best_model.pt\n",
            "\n",
            "============================================================\n",
            "Epoch 10/10\n",
            "============================================================\n",
            "Epoch 10:  11% 249/2250 [00:42<06:10,  5.40it/s, loss=0.00392, avg_loss=0.00455, lr=4.57e-6]\n",
            "Checkpoint saved at step 20500: arithmetic_llm/models/instruction_20260219_220640_551798/checkpoint_step_20500.pt\n",
            "Epoch 10:  33% 749/2250 [02:06<04:18,  5.81it/s, loss=0.0049, avg_loss=0.00456, lr=3.43e-6]\n",
            "Checkpoint saved at step 21000: arithmetic_llm/models/instruction_20260219_220640_551798/checkpoint_step_21000.pt\n",
            "Epoch 10:  56% 1249/2250 [03:33<02:56,  5.67it/s, loss=0.00242, avg_loss=0.00452, lr=2.29e-6]\n",
            "Checkpoint saved at step 21500: arithmetic_llm/models/instruction_20260219_220640_551798/checkpoint_step_21500.pt\n",
            "Epoch 10:  78% 1749/2250 [04:58<01:16,  6.55it/s, loss=0.00375, avg_loss=0.0045, lr=1.16e-6]\n",
            "Checkpoint saved at step 22000: arithmetic_llm/models/instruction_20260219_220640_551798/checkpoint_step_22000.pt\n",
            "Epoch 10: 100% 2249/2250 [06:23<00:00,  5.77it/s, loss=0.00459, avg_loss=0.00448, lr=2.05e-8]\n",
            "Checkpoint saved at step 22500: arithmetic_llm/models/instruction_20260219_220640_551798/checkpoint_step_22500.pt\n",
            "Epoch 10: 100% 2250/2250 [06:23<00:00,  5.86it/s, loss=0.00459, avg_loss=0.00448, lr=2.05e-8]\n",
            "\n",
            "Evaluating on validation set...\n",
            "\n",
            "Epoch 10 Summary:\n",
            "  Training Loss: 0.0045\n",
            "  Validation Loss: 0.0020\n",
            "  New best model saved: arithmetic_llm/models/instruction_20260219_220640_551798/best_model.pt\n",
            "\n",
            "Saving final fine-tuned model...\n",
            "Final model saved: arithmetic_llm/models/instruction_20260219_220640_551798/final_model.pt\n",
            "Training log saved: arithmetic_llm/models/instruction_20260219_220640_551798/training_log.json\n",
            "Training summary saved: arithmetic_llm/models/instruction_20260219_220640_551798/training_summary.json\n",
            "\n",
            "============================================================\n",
            "Instruction fine-tuning completed successfully!\n",
            "Output directory: arithmetic_llm/models/instruction_20260219_220640_551798\n",
            "============================================================\n",
            "\n",
            "============================================================\n",
            "FINE-TUNING COMPLETED SUCCESSFULLY!\n",
            "============================================================\n",
            "Final checkpoint: arithmetic_llm/models/instruction_20260219_220640_551798/final_model.pt\n",
            "============================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls arithmetic_llm/models"
      ],
      "metadata": {
        "id": "1NNj8FQQLVy9",
        "outputId": "954d75b8-dbcb-4128-8d9c-e9c8cf93de49",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "1NNj8FQQLVy9",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "foundational_20260219_182844_242048  instruction_20260219_220640_551798\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 4.1 Evaluate the model\n",
        "!python -m arithmetic_llm.run_evaluation \\\n",
        "  --model-path arithmetic_llm/models/instruction_20260219_220640_551798/best_model.pt \\\n",
        "  --tokenizer-path arithmetic_llm/data/tokenizer \\\n",
        "  --max-gen-length 512 \\\n",
        "  --batch-size 1 \\\n",
        "  --num-samples 1000 \\\n",
        "  --output-dir arithmetic_llm/evaluation_results/instruction_id_eval"
      ],
      "metadata": {
        "collapsed": true,
        "id": "w4LFiyQC7sXx",
        "outputId": "8942299f-718a-4622-8b5b-b3a0cf7f4dd2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "w4LFiyQC7sXx",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "MODEL EVALUATION\n",
            "============================================================\n",
            "\n",
            "Model: arithmetic_llm/models/instruction_20260219_220640_551798/best_model.pt\n",
            "Tokenizer: arithmetic_llm/data/tokenizer\n",
            "Device: cuda\n",
            "\n",
            "Evaluation Configuration:\n",
            "  Test samples: 1000\n",
            "  Max depth: 5\n",
            "  Number range: 1 to 20\n",
            "  Batch size: 1\n",
            "  Max generation length: 512\n",
            "  Output directory: arithmetic_llm/evaluation_results/instruction_id_eval\n",
            "============================================================\n",
            "\n",
            "Loading model and tokenizer...\n",
            "Model loaded successfully!\n",
            "\n",
            "Starting evaluation...\n",
            "Generating 1000 test expressions...\n",
            "Generated 1000 valid test expressions\n",
            "Evaluating model with batch size 1...\n",
            "Evaluated 1/1000 samples\n",
            "Evaluated 2/1000 samples\n",
            "Evaluated 3/1000 samples\n",
            "Evaluated 4/1000 samples\n",
            "Evaluated 5/1000 samples\n",
            "Evaluated 6/1000 samples\n",
            "Evaluated 7/1000 samples\n",
            "Evaluated 8/1000 samples\n",
            "Evaluated 9/1000 samples\n",
            "Evaluated 10/1000 samples\n",
            "Evaluated 11/1000 samples\n",
            "Evaluated 12/1000 samples\n",
            "Evaluated 13/1000 samples\n",
            "Evaluated 14/1000 samples\n",
            "Evaluated 15/1000 samples\n",
            "Evaluated 16/1000 samples\n",
            "Evaluated 17/1000 samples\n",
            "Evaluated 18/1000 samples\n",
            "Evaluated 19/1000 samples\n",
            "Evaluated 20/1000 samples\n",
            "Evaluated 21/1000 samples\n",
            "Evaluated 22/1000 samples\n",
            "Evaluated 23/1000 samples\n",
            "Evaluated 24/1000 samples\n",
            "Evaluated 25/1000 samples\n",
            "Evaluated 26/1000 samples\n",
            "Evaluated 27/1000 samples\n",
            "Evaluated 28/1000 samples\n",
            "Evaluated 29/1000 samples\n",
            "Evaluated 30/1000 samples\n",
            "Evaluated 31/1000 samples\n",
            "Evaluated 32/1000 samples\n",
            "Evaluated 33/1000 samples\n",
            "Evaluated 34/1000 samples\n",
            "Evaluated 35/1000 samples\n",
            "Evaluated 36/1000 samples\n",
            "Evaluated 37/1000 samples\n",
            "Evaluated 38/1000 samples\n",
            "Evaluated 39/1000 samples\n",
            "Evaluated 40/1000 samples\n",
            "Evaluated 41/1000 samples\n",
            "Evaluated 42/1000 samples\n",
            "Evaluated 43/1000 samples\n",
            "Evaluated 44/1000 samples\n",
            "Evaluated 45/1000 samples\n",
            "Evaluated 46/1000 samples\n",
            "Evaluated 47/1000 samples\n",
            "Evaluated 48/1000 samples\n",
            "Evaluated 49/1000 samples\n",
            "Evaluated 50/1000 samples\n",
            "Evaluated 51/1000 samples\n",
            "Evaluated 52/1000 samples\n",
            "Evaluated 53/1000 samples\n",
            "Evaluated 54/1000 samples\n",
            "Evaluated 55/1000 samples\n",
            "Evaluated 56/1000 samples\n",
            "Evaluated 57/1000 samples\n",
            "Evaluated 58/1000 samples\n",
            "Evaluated 59/1000 samples\n",
            "Evaluated 60/1000 samples\n",
            "Evaluated 61/1000 samples\n",
            "Evaluated 62/1000 samples\n",
            "Evaluated 63/1000 samples\n",
            "Evaluated 64/1000 samples\n",
            "Evaluated 65/1000 samples\n",
            "Evaluated 66/1000 samples\n",
            "Evaluated 67/1000 samples\n",
            "Evaluated 68/1000 samples\n",
            "Evaluated 69/1000 samples\n",
            "Evaluated 70/1000 samples\n",
            "Evaluated 71/1000 samples\n",
            "Evaluated 72/1000 samples\n",
            "Evaluated 73/1000 samples\n",
            "Evaluated 74/1000 samples\n",
            "Evaluated 75/1000 samples\n",
            "Evaluated 76/1000 samples\n",
            "Evaluated 77/1000 samples\n",
            "Evaluated 78/1000 samples\n",
            "Evaluated 79/1000 samples\n",
            "Evaluated 80/1000 samples\n",
            "Evaluated 81/1000 samples\n",
            "Evaluated 82/1000 samples\n",
            "Evaluated 83/1000 samples\n",
            "Evaluated 84/1000 samples\n",
            "Evaluated 85/1000 samples\n",
            "Evaluated 86/1000 samples\n",
            "Evaluated 87/1000 samples\n",
            "Evaluated 88/1000 samples\n",
            "Evaluated 89/1000 samples\n",
            "Evaluated 90/1000 samples\n",
            "Evaluated 91/1000 samples\n",
            "Evaluated 92/1000 samples\n",
            "Evaluated 93/1000 samples\n",
            "Evaluated 94/1000 samples\n",
            "Evaluated 95/1000 samples\n",
            "Evaluated 96/1000 samples\n",
            "Evaluated 97/1000 samples\n",
            "Evaluated 98/1000 samples\n",
            "Evaluated 99/1000 samples\n",
            "Evaluated 100/1000 samples\n",
            "Evaluated 101/1000 samples\n",
            "Evaluated 102/1000 samples\n",
            "Evaluated 103/1000 samples\n",
            "Evaluated 104/1000 samples\n",
            "Evaluated 105/1000 samples\n",
            "Evaluated 106/1000 samples\n",
            "Evaluated 107/1000 samples\n",
            "Evaluated 108/1000 samples\n",
            "Evaluated 109/1000 samples\n",
            "Evaluated 110/1000 samples\n",
            "Evaluated 111/1000 samples\n",
            "Evaluated 112/1000 samples\n",
            "Evaluated 113/1000 samples\n",
            "Evaluated 114/1000 samples\n",
            "Evaluated 115/1000 samples\n",
            "Evaluated 116/1000 samples\n",
            "Evaluated 117/1000 samples\n",
            "Evaluated 118/1000 samples\n",
            "Evaluated 119/1000 samples\n",
            "Evaluated 120/1000 samples\n",
            "Evaluated 121/1000 samples\n",
            "Evaluated 122/1000 samples\n",
            "Evaluated 123/1000 samples\n",
            "Evaluated 124/1000 samples\n",
            "Evaluated 125/1000 samples\n",
            "Evaluated 126/1000 samples\n",
            "Evaluated 127/1000 samples\n",
            "Evaluated 128/1000 samples\n",
            "Evaluated 129/1000 samples\n",
            "Evaluated 130/1000 samples\n",
            "Evaluated 131/1000 samples\n",
            "Evaluated 132/1000 samples\n",
            "Evaluated 133/1000 samples\n",
            "Evaluated 134/1000 samples\n",
            "Evaluated 135/1000 samples\n",
            "Evaluated 136/1000 samples\n",
            "Evaluated 137/1000 samples\n",
            "Evaluated 138/1000 samples\n",
            "Evaluated 139/1000 samples\n",
            "Evaluated 140/1000 samples\n",
            "Evaluated 141/1000 samples\n",
            "Evaluated 142/1000 samples\n",
            "Evaluated 143/1000 samples\n",
            "Evaluated 144/1000 samples\n",
            "Evaluated 145/1000 samples\n",
            "Evaluated 146/1000 samples\n",
            "Evaluated 147/1000 samples\n",
            "Evaluated 148/1000 samples\n",
            "Evaluated 149/1000 samples\n",
            "Evaluated 150/1000 samples\n",
            "Evaluated 151/1000 samples\n",
            "Evaluated 152/1000 samples\n",
            "Evaluated 153/1000 samples\n",
            "Evaluated 154/1000 samples\n",
            "Evaluated 155/1000 samples\n",
            "Evaluated 156/1000 samples\n",
            "Evaluated 157/1000 samples\n",
            "Evaluated 158/1000 samples\n",
            "Evaluated 159/1000 samples\n",
            "Evaluated 160/1000 samples\n",
            "Evaluated 161/1000 samples\n",
            "Evaluated 162/1000 samples\n",
            "Evaluated 163/1000 samples\n",
            "Evaluated 164/1000 samples\n",
            "Evaluated 165/1000 samples\n",
            "Evaluated 166/1000 samples\n",
            "Evaluated 167/1000 samples\n",
            "Evaluated 168/1000 samples\n",
            "Evaluated 169/1000 samples\n",
            "Evaluated 170/1000 samples\n",
            "Evaluated 171/1000 samples\n",
            "Evaluated 172/1000 samples\n",
            "Evaluated 173/1000 samples\n",
            "Evaluated 174/1000 samples\n",
            "Evaluated 175/1000 samples\n",
            "Evaluated 176/1000 samples\n",
            "Evaluated 177/1000 samples\n",
            "Evaluated 178/1000 samples\n",
            "Evaluated 179/1000 samples\n",
            "Evaluated 180/1000 samples\n",
            "Evaluated 181/1000 samples\n",
            "Evaluated 182/1000 samples\n",
            "Evaluated 183/1000 samples\n",
            "Evaluated 184/1000 samples\n",
            "Evaluated 185/1000 samples\n",
            "Evaluated 186/1000 samples\n",
            "Evaluated 187/1000 samples\n",
            "Evaluated 188/1000 samples\n",
            "Evaluated 189/1000 samples\n",
            "Evaluated 190/1000 samples\n",
            "Evaluated 191/1000 samples\n",
            "Evaluated 192/1000 samples\n",
            "Evaluated 193/1000 samples\n",
            "Evaluated 194/1000 samples\n",
            "Evaluated 195/1000 samples\n",
            "Evaluated 196/1000 samples\n",
            "Evaluated 197/1000 samples\n",
            "Evaluated 198/1000 samples\n",
            "Evaluated 199/1000 samples\n",
            "Evaluated 200/1000 samples\n",
            "Evaluated 201/1000 samples\n",
            "Evaluated 202/1000 samples\n",
            "Evaluated 203/1000 samples\n",
            "Evaluated 204/1000 samples\n",
            "Evaluated 205/1000 samples\n",
            "Evaluated 206/1000 samples\n",
            "Evaluated 207/1000 samples\n",
            "Evaluated 208/1000 samples\n",
            "Evaluated 209/1000 samples\n",
            "Evaluated 210/1000 samples\n",
            "Evaluated 211/1000 samples\n",
            "Evaluated 212/1000 samples\n",
            "Evaluated 213/1000 samples\n",
            "Evaluated 214/1000 samples\n",
            "Evaluated 215/1000 samples\n",
            "Evaluated 216/1000 samples\n",
            "Evaluated 217/1000 samples\n",
            "Evaluated 218/1000 samples\n",
            "Evaluated 219/1000 samples\n",
            "Evaluated 220/1000 samples\n",
            "Evaluated 221/1000 samples\n",
            "Evaluated 222/1000 samples\n",
            "Evaluated 223/1000 samples\n",
            "Evaluated 224/1000 samples\n",
            "Evaluated 225/1000 samples\n",
            "Evaluated 226/1000 samples\n",
            "Evaluated 227/1000 samples\n",
            "Evaluated 228/1000 samples\n",
            "Evaluated 229/1000 samples\n",
            "Evaluated 230/1000 samples\n",
            "Evaluated 231/1000 samples\n",
            "Evaluated 232/1000 samples\n",
            "Evaluated 233/1000 samples\n",
            "Evaluated 234/1000 samples\n",
            "Evaluated 235/1000 samples\n",
            "Evaluated 236/1000 samples\n",
            "Evaluated 237/1000 samples\n",
            "Evaluated 238/1000 samples\n",
            "Evaluated 239/1000 samples\n",
            "Evaluated 240/1000 samples\n",
            "Evaluated 241/1000 samples\n",
            "Evaluated 242/1000 samples\n",
            "Evaluated 243/1000 samples\n",
            "Evaluated 244/1000 samples\n",
            "Evaluated 245/1000 samples\n",
            "Evaluated 246/1000 samples\n",
            "Evaluated 247/1000 samples\n",
            "Evaluated 248/1000 samples\n",
            "Evaluated 249/1000 samples\n",
            "Evaluated 250/1000 samples\n",
            "Evaluated 251/1000 samples\n",
            "Evaluated 252/1000 samples\n",
            "Evaluated 253/1000 samples\n",
            "Evaluated 254/1000 samples\n",
            "Evaluated 255/1000 samples\n",
            "Evaluated 256/1000 samples\n",
            "Evaluated 257/1000 samples\n",
            "Evaluated 258/1000 samples\n",
            "Evaluated 259/1000 samples\n",
            "Evaluated 260/1000 samples\n",
            "Evaluated 261/1000 samples\n",
            "Evaluated 262/1000 samples\n",
            "Evaluated 263/1000 samples\n",
            "Evaluated 264/1000 samples\n",
            "Evaluated 265/1000 samples\n",
            "Evaluated 266/1000 samples\n",
            "Evaluated 267/1000 samples\n",
            "Evaluated 268/1000 samples\n",
            "Evaluated 269/1000 samples\n",
            "Evaluated 270/1000 samples\n",
            "Evaluated 271/1000 samples\n",
            "Evaluated 272/1000 samples\n",
            "Evaluated 273/1000 samples\n",
            "Evaluated 274/1000 samples\n",
            "Evaluated 275/1000 samples\n",
            "Evaluated 276/1000 samples\n",
            "Evaluated 277/1000 samples\n",
            "Evaluated 278/1000 samples\n",
            "Evaluated 279/1000 samples\n",
            "Evaluated 280/1000 samples\n",
            "Evaluated 281/1000 samples\n",
            "Evaluated 282/1000 samples\n",
            "Evaluated 283/1000 samples\n",
            "Evaluated 284/1000 samples\n",
            "Evaluated 285/1000 samples\n",
            "Evaluated 286/1000 samples\n",
            "Evaluated 287/1000 samples\n",
            "Evaluated 288/1000 samples\n",
            "Evaluated 289/1000 samples\n",
            "Evaluated 290/1000 samples\n",
            "Evaluated 291/1000 samples\n",
            "Evaluated 292/1000 samples\n",
            "Evaluated 293/1000 samples\n",
            "Evaluated 294/1000 samples\n",
            "Evaluated 295/1000 samples\n",
            "Evaluated 296/1000 samples\n",
            "Evaluated 297/1000 samples\n",
            "Evaluated 298/1000 samples\n",
            "Evaluated 299/1000 samples\n",
            "Evaluated 300/1000 samples\n",
            "Evaluated 301/1000 samples\n",
            "Evaluated 302/1000 samples\n",
            "Evaluated 303/1000 samples\n",
            "Evaluated 304/1000 samples\n",
            "Evaluated 305/1000 samples\n",
            "Evaluated 306/1000 samples\n",
            "Evaluated 307/1000 samples\n",
            "Evaluated 308/1000 samples\n",
            "Evaluated 309/1000 samples\n",
            "Evaluated 310/1000 samples\n",
            "Evaluated 311/1000 samples\n",
            "Evaluated 312/1000 samples\n",
            "Evaluated 313/1000 samples\n",
            "Evaluated 314/1000 samples\n",
            "Evaluated 315/1000 samples\n",
            "Evaluated 316/1000 samples\n",
            "Evaluated 317/1000 samples\n",
            "Evaluated 318/1000 samples\n",
            "Evaluated 319/1000 samples\n",
            "Evaluated 320/1000 samples\n",
            "Evaluated 321/1000 samples\n",
            "Evaluated 322/1000 samples\n",
            "Evaluated 323/1000 samples\n",
            "Evaluated 324/1000 samples\n",
            "Evaluated 325/1000 samples\n",
            "Evaluated 326/1000 samples\n",
            "Evaluated 327/1000 samples\n",
            "Evaluated 328/1000 samples\n",
            "Evaluated 329/1000 samples\n",
            "Evaluated 330/1000 samples\n",
            "Evaluated 331/1000 samples\n",
            "Evaluated 332/1000 samples\n",
            "Evaluated 333/1000 samples\n",
            "Evaluated 334/1000 samples\n",
            "Evaluated 335/1000 samples\n",
            "Evaluated 336/1000 samples\n",
            "Evaluated 337/1000 samples\n",
            "Evaluated 338/1000 samples\n",
            "Evaluated 339/1000 samples\n",
            "Evaluated 340/1000 samples\n",
            "Evaluated 341/1000 samples\n",
            "Evaluated 342/1000 samples\n",
            "Evaluated 343/1000 samples\n",
            "Evaluated 344/1000 samples\n",
            "Evaluated 345/1000 samples\n",
            "Evaluated 346/1000 samples\n",
            "Evaluated 347/1000 samples\n",
            "Evaluated 348/1000 samples\n",
            "Evaluated 349/1000 samples\n",
            "Evaluated 350/1000 samples\n",
            "Evaluated 351/1000 samples\n",
            "Evaluated 352/1000 samples\n",
            "Evaluated 353/1000 samples\n",
            "Evaluated 354/1000 samples\n",
            "Evaluated 355/1000 samples\n",
            "Evaluated 356/1000 samples\n",
            "Evaluated 357/1000 samples\n",
            "Evaluated 358/1000 samples\n",
            "Evaluated 359/1000 samples\n",
            "Evaluated 360/1000 samples\n",
            "Evaluated 361/1000 samples\n",
            "Evaluated 362/1000 samples\n",
            "Evaluated 363/1000 samples\n",
            "Evaluated 364/1000 samples\n",
            "Evaluated 365/1000 samples\n",
            "Evaluated 366/1000 samples\n",
            "Evaluated 367/1000 samples\n",
            "Evaluated 368/1000 samples\n",
            "Evaluated 369/1000 samples\n",
            "Evaluated 370/1000 samples\n",
            "Evaluated 371/1000 samples\n",
            "Evaluated 372/1000 samples\n",
            "Evaluated 373/1000 samples\n",
            "Evaluated 374/1000 samples\n",
            "Evaluated 375/1000 samples\n",
            "Evaluated 376/1000 samples\n",
            "Evaluated 377/1000 samples\n",
            "Evaluated 378/1000 samples\n",
            "Evaluated 379/1000 samples\n",
            "Evaluated 380/1000 samples\n",
            "Evaluated 381/1000 samples\n",
            "Evaluated 382/1000 samples\n",
            "Evaluated 383/1000 samples\n",
            "Evaluated 384/1000 samples\n",
            "Evaluated 385/1000 samples\n",
            "Evaluated 386/1000 samples\n",
            "Evaluated 387/1000 samples\n",
            "Evaluated 388/1000 samples\n",
            "Evaluated 389/1000 samples\n",
            "Evaluated 390/1000 samples\n",
            "Evaluated 391/1000 samples\n",
            "Evaluated 392/1000 samples\n",
            "Evaluated 393/1000 samples\n",
            "Evaluated 394/1000 samples\n",
            "Evaluated 395/1000 samples\n",
            "Evaluated 396/1000 samples\n",
            "Evaluated 397/1000 samples\n",
            "Evaluated 398/1000 samples\n",
            "Evaluated 399/1000 samples\n",
            "Evaluated 400/1000 samples\n",
            "Evaluated 401/1000 samples\n",
            "Evaluated 402/1000 samples\n",
            "Evaluated 403/1000 samples\n",
            "Evaluated 404/1000 samples\n",
            "Evaluated 405/1000 samples\n",
            "Evaluated 406/1000 samples\n",
            "Evaluated 407/1000 samples\n",
            "Evaluated 408/1000 samples\n",
            "Evaluated 409/1000 samples\n",
            "Evaluated 410/1000 samples\n",
            "Evaluated 411/1000 samples\n",
            "Evaluated 412/1000 samples\n",
            "Evaluated 413/1000 samples\n",
            "Evaluated 414/1000 samples\n",
            "Evaluated 415/1000 samples\n",
            "Evaluated 416/1000 samples\n",
            "Evaluated 417/1000 samples\n",
            "Evaluated 418/1000 samples\n",
            "Evaluated 419/1000 samples\n",
            "Evaluated 420/1000 samples\n",
            "Evaluated 421/1000 samples\n",
            "Evaluated 422/1000 samples\n",
            "Evaluated 423/1000 samples\n",
            "Evaluated 424/1000 samples\n",
            "Evaluated 425/1000 samples\n",
            "Evaluated 426/1000 samples\n",
            "Evaluated 427/1000 samples\n",
            "Evaluated 428/1000 samples\n",
            "Evaluated 429/1000 samples\n",
            "Evaluated 430/1000 samples\n",
            "Evaluated 431/1000 samples\n",
            "Evaluated 432/1000 samples\n",
            "Evaluated 433/1000 samples\n",
            "Evaluated 434/1000 samples\n",
            "Evaluated 435/1000 samples\n",
            "Evaluated 436/1000 samples\n",
            "Evaluated 437/1000 samples\n",
            "Evaluated 438/1000 samples\n",
            "Evaluated 439/1000 samples\n",
            "Evaluated 440/1000 samples\n",
            "Evaluated 441/1000 samples\n",
            "Evaluated 442/1000 samples\n",
            "Evaluated 443/1000 samples\n",
            "Evaluated 444/1000 samples\n",
            "Evaluated 445/1000 samples\n",
            "Evaluated 446/1000 samples\n",
            "Evaluated 447/1000 samples\n",
            "Evaluated 448/1000 samples\n",
            "Evaluated 449/1000 samples\n",
            "Evaluated 450/1000 samples\n",
            "Evaluated 451/1000 samples\n",
            "Evaluated 452/1000 samples\n",
            "Evaluated 453/1000 samples\n",
            "Evaluated 454/1000 samples\n",
            "Evaluated 455/1000 samples\n",
            "Evaluated 456/1000 samples\n",
            "Evaluated 457/1000 samples\n",
            "Evaluated 458/1000 samples\n",
            "Evaluated 459/1000 samples\n",
            "Evaluated 460/1000 samples\n",
            "Evaluated 461/1000 samples\n",
            "Evaluated 462/1000 samples\n",
            "Evaluated 463/1000 samples\n",
            "Evaluated 464/1000 samples\n",
            "Evaluated 465/1000 samples\n",
            "Evaluated 466/1000 samples\n",
            "Evaluated 467/1000 samples\n",
            "Evaluated 468/1000 samples\n",
            "Evaluated 469/1000 samples\n",
            "Evaluated 470/1000 samples\n",
            "Evaluated 471/1000 samples\n",
            "Evaluated 472/1000 samples\n",
            "Evaluated 473/1000 samples\n",
            "Evaluated 474/1000 samples\n",
            "Evaluated 475/1000 samples\n",
            "Evaluated 476/1000 samples\n",
            "Evaluated 477/1000 samples\n",
            "Evaluated 478/1000 samples\n",
            "Evaluated 479/1000 samples\n",
            "Evaluated 480/1000 samples\n",
            "Evaluated 481/1000 samples\n",
            "Evaluated 482/1000 samples\n",
            "Evaluated 483/1000 samples\n",
            "Evaluated 484/1000 samples\n",
            "Evaluated 485/1000 samples\n",
            "Evaluated 486/1000 samples\n",
            "Evaluated 487/1000 samples\n",
            "Evaluated 488/1000 samples\n",
            "Evaluated 489/1000 samples\n",
            "Evaluated 490/1000 samples\n",
            "Evaluated 491/1000 samples\n",
            "Evaluated 492/1000 samples\n",
            "Evaluated 493/1000 samples\n",
            "Evaluated 494/1000 samples\n",
            "Evaluated 495/1000 samples\n",
            "Evaluated 496/1000 samples\n",
            "Evaluated 497/1000 samples\n",
            "Evaluated 498/1000 samples\n",
            "Evaluated 499/1000 samples\n",
            "Evaluated 500/1000 samples\n",
            "Evaluated 501/1000 samples\n",
            "Evaluated 502/1000 samples\n",
            "Evaluated 503/1000 samples\n",
            "Evaluated 504/1000 samples\n",
            "Evaluated 505/1000 samples\n",
            "Evaluated 506/1000 samples\n",
            "Evaluated 507/1000 samples\n",
            "Evaluated 508/1000 samples\n",
            "Evaluated 509/1000 samples\n",
            "Evaluated 510/1000 samples\n",
            "Evaluated 511/1000 samples\n",
            "Evaluated 512/1000 samples\n",
            "Evaluated 513/1000 samples\n",
            "Evaluated 514/1000 samples\n",
            "Evaluated 515/1000 samples\n",
            "Evaluated 516/1000 samples\n",
            "Evaluated 517/1000 samples\n",
            "Evaluated 518/1000 samples\n",
            "Evaluated 519/1000 samples\n",
            "Evaluated 520/1000 samples\n",
            "Evaluated 521/1000 samples\n",
            "Evaluated 522/1000 samples\n",
            "Evaluated 523/1000 samples\n",
            "Evaluated 524/1000 samples\n",
            "Evaluated 525/1000 samples\n",
            "Evaluated 526/1000 samples\n",
            "Evaluated 527/1000 samples\n",
            "Evaluated 528/1000 samples\n",
            "Evaluated 529/1000 samples\n",
            "Evaluated 530/1000 samples\n",
            "Evaluated 531/1000 samples\n",
            "Evaluated 532/1000 samples\n",
            "Evaluated 533/1000 samples\n",
            "Evaluated 534/1000 samples\n",
            "Evaluated 535/1000 samples\n",
            "Evaluated 536/1000 samples\n",
            "Evaluated 537/1000 samples\n",
            "Evaluated 538/1000 samples\n",
            "Evaluated 539/1000 samples\n",
            "Evaluated 540/1000 samples\n",
            "Evaluated 541/1000 samples\n",
            "Evaluated 542/1000 samples\n",
            "Evaluated 543/1000 samples\n",
            "Evaluated 544/1000 samples\n",
            "Evaluated 545/1000 samples\n",
            "Evaluated 546/1000 samples\n",
            "Evaluated 547/1000 samples\n",
            "Evaluated 548/1000 samples\n",
            "Evaluated 549/1000 samples\n",
            "Evaluated 550/1000 samples\n",
            "Evaluated 551/1000 samples\n",
            "Evaluated 552/1000 samples\n",
            "Evaluated 553/1000 samples\n",
            "Evaluated 554/1000 samples\n",
            "Evaluated 555/1000 samples\n",
            "Evaluated 556/1000 samples\n",
            "Evaluated 557/1000 samples\n",
            "Evaluated 558/1000 samples\n",
            "Evaluated 559/1000 samples\n",
            "Evaluated 560/1000 samples\n",
            "Evaluated 561/1000 samples\n",
            "Evaluated 562/1000 samples\n",
            "Evaluated 563/1000 samples\n",
            "Evaluated 564/1000 samples\n",
            "Evaluated 565/1000 samples\n",
            "Evaluated 566/1000 samples\n",
            "Evaluated 567/1000 samples\n",
            "Evaluated 568/1000 samples\n",
            "Evaluated 569/1000 samples\n",
            "Evaluated 570/1000 samples\n",
            "Evaluated 571/1000 samples\n",
            "Evaluated 572/1000 samples\n",
            "Evaluated 573/1000 samples\n",
            "Evaluated 574/1000 samples\n",
            "Evaluated 575/1000 samples\n",
            "Evaluated 576/1000 samples\n",
            "Evaluated 577/1000 samples\n",
            "Evaluated 578/1000 samples\n",
            "Evaluated 579/1000 samples\n",
            "Evaluated 580/1000 samples\n",
            "Evaluated 581/1000 samples\n",
            "Evaluated 582/1000 samples\n",
            "Evaluated 583/1000 samples\n",
            "Evaluated 584/1000 samples\n",
            "Evaluated 585/1000 samples\n",
            "Evaluated 586/1000 samples\n",
            "Evaluated 587/1000 samples\n",
            "Evaluated 588/1000 samples\n",
            "Evaluated 589/1000 samples\n",
            "Evaluated 590/1000 samples\n",
            "Evaluated 591/1000 samples\n",
            "Evaluated 592/1000 samples\n",
            "Evaluated 593/1000 samples\n",
            "Evaluated 594/1000 samples\n",
            "Evaluated 595/1000 samples\n",
            "Evaluated 596/1000 samples\n",
            "Evaluated 597/1000 samples\n",
            "Evaluated 598/1000 samples\n",
            "Evaluated 599/1000 samples\n",
            "Evaluated 600/1000 samples\n",
            "Evaluated 601/1000 samples\n",
            "Evaluated 602/1000 samples\n",
            "Evaluated 603/1000 samples\n",
            "Evaluated 604/1000 samples\n",
            "Evaluated 605/1000 samples\n",
            "Evaluated 606/1000 samples\n",
            "Evaluated 607/1000 samples\n",
            "Evaluated 608/1000 samples\n",
            "Evaluated 609/1000 samples\n",
            "Evaluated 610/1000 samples\n",
            "Evaluated 611/1000 samples\n",
            "Evaluated 612/1000 samples\n",
            "Evaluated 613/1000 samples\n",
            "Evaluated 614/1000 samples\n",
            "Evaluated 615/1000 samples\n",
            "Evaluated 616/1000 samples\n",
            "Evaluated 617/1000 samples\n",
            "Evaluated 618/1000 samples\n",
            "Evaluated 619/1000 samples\n",
            "Evaluated 620/1000 samples\n",
            "Evaluated 621/1000 samples\n",
            "Evaluated 622/1000 samples\n",
            "Evaluated 623/1000 samples\n",
            "Evaluated 624/1000 samples\n",
            "Evaluated 625/1000 samples\n",
            "Evaluated 626/1000 samples\n",
            "Evaluated 627/1000 samples\n",
            "Evaluated 628/1000 samples\n",
            "Evaluated 629/1000 samples\n",
            "Evaluated 630/1000 samples\n",
            "Evaluated 631/1000 samples\n",
            "Evaluated 632/1000 samples\n",
            "Evaluated 633/1000 samples\n",
            "Evaluated 634/1000 samples\n",
            "Evaluated 635/1000 samples\n",
            "Evaluated 636/1000 samples\n",
            "Evaluated 637/1000 samples\n",
            "Evaluated 638/1000 samples\n",
            "Evaluated 639/1000 samples\n",
            "Evaluated 640/1000 samples\n",
            "Evaluated 641/1000 samples\n",
            "Evaluated 642/1000 samples\n",
            "Evaluated 643/1000 samples\n",
            "Evaluated 644/1000 samples\n",
            "Evaluated 645/1000 samples\n",
            "Evaluated 646/1000 samples\n",
            "Evaluated 647/1000 samples\n",
            "Evaluated 648/1000 samples\n",
            "Evaluated 649/1000 samples\n",
            "Evaluated 650/1000 samples\n",
            "Evaluated 651/1000 samples\n",
            "Evaluated 652/1000 samples\n",
            "Evaluated 653/1000 samples\n",
            "Evaluated 654/1000 samples\n",
            "Evaluated 655/1000 samples\n",
            "Evaluated 656/1000 samples\n",
            "Evaluated 657/1000 samples\n",
            "Evaluated 658/1000 samples\n",
            "Evaluated 659/1000 samples\n",
            "Evaluated 660/1000 samples\n",
            "Evaluated 661/1000 samples\n",
            "Evaluated 662/1000 samples\n",
            "Evaluated 663/1000 samples\n",
            "Evaluated 664/1000 samples\n",
            "Evaluated 665/1000 samples\n",
            "Evaluated 666/1000 samples\n",
            "Evaluated 667/1000 samples\n",
            "Evaluated 668/1000 samples\n",
            "Evaluated 669/1000 samples\n",
            "Evaluated 670/1000 samples\n",
            "Evaluated 671/1000 samples\n",
            "Evaluated 672/1000 samples\n",
            "Evaluated 673/1000 samples\n",
            "Evaluated 674/1000 samples\n",
            "Evaluated 675/1000 samples\n",
            "Evaluated 676/1000 samples\n",
            "Evaluated 677/1000 samples\n",
            "Evaluated 678/1000 samples\n",
            "Evaluated 679/1000 samples\n",
            "Evaluated 680/1000 samples\n",
            "Evaluated 681/1000 samples\n",
            "Evaluated 682/1000 samples\n",
            "Evaluated 683/1000 samples\n",
            "Evaluated 684/1000 samples\n",
            "Evaluated 685/1000 samples\n",
            "Evaluated 686/1000 samples\n",
            "Evaluated 687/1000 samples\n",
            "Evaluated 688/1000 samples\n",
            "Evaluated 689/1000 samples\n",
            "Evaluated 690/1000 samples\n",
            "Evaluated 691/1000 samples\n",
            "Evaluated 692/1000 samples\n",
            "Evaluated 693/1000 samples\n",
            "Evaluated 694/1000 samples\n",
            "Evaluated 695/1000 samples\n",
            "Evaluated 696/1000 samples\n",
            "Evaluated 697/1000 samples\n",
            "Evaluated 698/1000 samples\n",
            "Evaluated 699/1000 samples\n",
            "Evaluated 700/1000 samples\n",
            "Evaluated 701/1000 samples\n",
            "Evaluated 702/1000 samples\n",
            "Evaluated 703/1000 samples\n",
            "Evaluated 704/1000 samples\n",
            "Evaluated 705/1000 samples\n",
            "Evaluated 706/1000 samples\n",
            "Evaluated 707/1000 samples\n",
            "Evaluated 708/1000 samples\n",
            "Evaluated 709/1000 samples\n",
            "Evaluated 710/1000 samples\n",
            "Evaluated 711/1000 samples\n",
            "Evaluated 712/1000 samples\n",
            "Evaluated 713/1000 samples\n",
            "Evaluated 714/1000 samples\n",
            "Evaluated 715/1000 samples\n",
            "Evaluated 716/1000 samples\n",
            "Evaluated 717/1000 samples\n",
            "Evaluated 718/1000 samples\n",
            "Evaluated 719/1000 samples\n",
            "Evaluated 720/1000 samples\n",
            "Evaluated 721/1000 samples\n",
            "Evaluated 722/1000 samples\n",
            "Evaluated 723/1000 samples\n",
            "Evaluated 724/1000 samples\n",
            "Evaluated 725/1000 samples\n",
            "Evaluated 726/1000 samples\n",
            "Evaluated 727/1000 samples\n",
            "Evaluated 728/1000 samples\n",
            "Evaluated 729/1000 samples\n",
            "Evaluated 730/1000 samples\n",
            "Evaluated 731/1000 samples\n",
            "Evaluated 732/1000 samples\n",
            "Evaluated 733/1000 samples\n",
            "Evaluated 734/1000 samples\n",
            "Evaluated 735/1000 samples\n",
            "Evaluated 736/1000 samples\n",
            "Evaluated 737/1000 samples\n",
            "Evaluated 738/1000 samples\n",
            "Evaluated 739/1000 samples\n",
            "Evaluated 740/1000 samples\n",
            "Evaluated 741/1000 samples\n",
            "Evaluated 742/1000 samples\n",
            "Evaluated 743/1000 samples\n",
            "Evaluated 744/1000 samples\n",
            "Evaluated 745/1000 samples\n",
            "Evaluated 746/1000 samples\n",
            "Evaluated 747/1000 samples\n",
            "Evaluated 748/1000 samples\n",
            "Evaluated 749/1000 samples\n",
            "Evaluated 750/1000 samples\n",
            "Evaluated 751/1000 samples\n",
            "Evaluated 752/1000 samples\n",
            "Evaluated 753/1000 samples\n",
            "Evaluated 754/1000 samples\n",
            "Evaluated 755/1000 samples\n",
            "Evaluated 756/1000 samples\n",
            "Evaluated 757/1000 samples\n",
            "Evaluated 758/1000 samples\n",
            "Evaluated 759/1000 samples\n",
            "Evaluated 760/1000 samples\n",
            "Evaluated 761/1000 samples\n",
            "Evaluated 762/1000 samples\n",
            "Evaluated 763/1000 samples\n",
            "Evaluated 764/1000 samples\n",
            "Evaluated 765/1000 samples\n",
            "Evaluated 766/1000 samples\n",
            "Evaluated 767/1000 samples\n",
            "Evaluated 768/1000 samples\n",
            "Evaluated 769/1000 samples\n",
            "Evaluated 770/1000 samples\n",
            "Evaluated 771/1000 samples\n",
            "Evaluated 772/1000 samples\n",
            "Evaluated 773/1000 samples\n",
            "Evaluated 774/1000 samples\n",
            "Evaluated 775/1000 samples\n",
            "Evaluated 776/1000 samples\n",
            "Evaluated 777/1000 samples\n",
            "Evaluated 778/1000 samples\n",
            "Evaluated 779/1000 samples\n",
            "Evaluated 780/1000 samples\n",
            "Evaluated 781/1000 samples\n",
            "Evaluated 782/1000 samples\n",
            "Evaluated 783/1000 samples\n",
            "Evaluated 784/1000 samples\n",
            "Evaluated 785/1000 samples\n",
            "Evaluated 786/1000 samples\n",
            "Evaluated 787/1000 samples\n",
            "Evaluated 788/1000 samples\n",
            "Evaluated 789/1000 samples\n",
            "Evaluated 790/1000 samples\n",
            "Evaluated 791/1000 samples\n",
            "Evaluated 792/1000 samples\n",
            "Evaluated 793/1000 samples\n",
            "Evaluated 794/1000 samples\n",
            "Evaluated 795/1000 samples\n",
            "Evaluated 796/1000 samples\n",
            "Evaluated 797/1000 samples\n",
            "Evaluated 798/1000 samples\n",
            "Evaluated 799/1000 samples\n",
            "Evaluated 800/1000 samples\n",
            "Evaluated 801/1000 samples\n",
            "Evaluated 802/1000 samples\n",
            "Evaluated 803/1000 samples\n",
            "Evaluated 804/1000 samples\n",
            "Evaluated 805/1000 samples\n",
            "Evaluated 806/1000 samples\n",
            "Evaluated 807/1000 samples\n",
            "Evaluated 808/1000 samples\n",
            "Evaluated 809/1000 samples\n",
            "Evaluated 810/1000 samples\n",
            "Evaluated 811/1000 samples\n",
            "Evaluated 812/1000 samples\n",
            "Evaluated 813/1000 samples\n",
            "Evaluated 814/1000 samples\n",
            "Evaluated 815/1000 samples\n",
            "Evaluated 816/1000 samples\n",
            "Evaluated 817/1000 samples\n",
            "Evaluated 818/1000 samples\n",
            "Evaluated 819/1000 samples\n",
            "Evaluated 820/1000 samples\n",
            "Evaluated 821/1000 samples\n",
            "Evaluated 822/1000 samples\n",
            "Evaluated 823/1000 samples\n",
            "Evaluated 824/1000 samples\n",
            "Evaluated 825/1000 samples\n",
            "Evaluated 826/1000 samples\n",
            "Evaluated 827/1000 samples\n",
            "Evaluated 828/1000 samples\n",
            "Evaluated 829/1000 samples\n",
            "Evaluated 830/1000 samples\n",
            "Evaluated 831/1000 samples\n",
            "Evaluated 832/1000 samples\n",
            "Evaluated 833/1000 samples\n",
            "Evaluated 834/1000 samples\n",
            "Evaluated 835/1000 samples\n",
            "Evaluated 836/1000 samples\n",
            "Evaluated 837/1000 samples\n",
            "Evaluated 838/1000 samples\n",
            "Evaluated 839/1000 samples\n",
            "Evaluated 840/1000 samples\n",
            "Evaluated 841/1000 samples\n",
            "Evaluated 842/1000 samples\n",
            "Evaluated 843/1000 samples\n",
            "Evaluated 844/1000 samples\n",
            "Evaluated 845/1000 samples\n",
            "Evaluated 846/1000 samples\n",
            "Evaluated 847/1000 samples\n",
            "Evaluated 848/1000 samples\n",
            "Evaluated 849/1000 samples\n",
            "Evaluated 850/1000 samples\n",
            "Evaluated 851/1000 samples\n",
            "Evaluated 852/1000 samples\n",
            "Evaluated 853/1000 samples\n",
            "Evaluated 854/1000 samples\n",
            "Evaluated 855/1000 samples\n",
            "Evaluated 856/1000 samples\n",
            "Evaluated 857/1000 samples\n",
            "Evaluated 858/1000 samples\n",
            "Evaluated 859/1000 samples\n",
            "Evaluated 860/1000 samples\n",
            "Evaluated 861/1000 samples\n",
            "Evaluated 862/1000 samples\n",
            "Evaluated 863/1000 samples\n",
            "Evaluated 864/1000 samples\n",
            "Evaluated 865/1000 samples\n",
            "Evaluated 866/1000 samples\n",
            "Evaluated 867/1000 samples\n",
            "Evaluated 868/1000 samples\n",
            "Evaluated 869/1000 samples\n",
            "Evaluated 870/1000 samples\n",
            "Evaluated 871/1000 samples\n",
            "Evaluated 872/1000 samples\n",
            "Evaluated 873/1000 samples\n",
            "Evaluated 874/1000 samples\n",
            "Evaluated 875/1000 samples\n",
            "Evaluated 876/1000 samples\n",
            "Evaluated 877/1000 samples\n",
            "Evaluated 878/1000 samples\n",
            "Evaluated 879/1000 samples\n",
            "Evaluated 880/1000 samples\n",
            "Evaluated 881/1000 samples\n",
            "Evaluated 882/1000 samples\n",
            "Evaluated 883/1000 samples\n",
            "Evaluated 884/1000 samples\n",
            "Evaluated 885/1000 samples\n",
            "Evaluated 886/1000 samples\n",
            "Evaluated 887/1000 samples\n",
            "Evaluated 888/1000 samples\n",
            "Evaluated 889/1000 samples\n",
            "Evaluated 890/1000 samples\n",
            "Evaluated 891/1000 samples\n",
            "Evaluated 892/1000 samples\n",
            "Evaluated 893/1000 samples\n",
            "Evaluated 894/1000 samples\n",
            "Evaluated 895/1000 samples\n",
            "Evaluated 896/1000 samples\n",
            "Evaluated 897/1000 samples\n",
            "Evaluated 898/1000 samples\n",
            "Evaluated 899/1000 samples\n",
            "Evaluated 900/1000 samples\n",
            "Evaluated 901/1000 samples\n",
            "Evaluated 902/1000 samples\n",
            "Evaluated 903/1000 samples\n",
            "Evaluated 904/1000 samples\n",
            "Evaluated 905/1000 samples\n",
            "Evaluated 906/1000 samples\n",
            "Evaluated 907/1000 samples\n",
            "Evaluated 908/1000 samples\n",
            "Evaluated 909/1000 samples\n",
            "Evaluated 910/1000 samples\n",
            "Evaluated 911/1000 samples\n",
            "Evaluated 912/1000 samples\n",
            "Evaluated 913/1000 samples\n",
            "Evaluated 914/1000 samples\n",
            "Evaluated 915/1000 samples\n",
            "Evaluated 916/1000 samples\n",
            "Evaluated 917/1000 samples\n",
            "Evaluated 918/1000 samples\n",
            "Evaluated 919/1000 samples\n",
            "Evaluated 920/1000 samples\n",
            "Evaluated 921/1000 samples\n",
            "Evaluated 922/1000 samples\n",
            "Evaluated 923/1000 samples\n",
            "Evaluated 924/1000 samples\n",
            "Evaluated 925/1000 samples\n",
            "Evaluated 926/1000 samples\n",
            "Evaluated 927/1000 samples\n",
            "Evaluated 928/1000 samples\n",
            "Evaluated 929/1000 samples\n",
            "Evaluated 930/1000 samples\n",
            "Evaluated 931/1000 samples\n",
            "Evaluated 932/1000 samples\n",
            "Evaluated 933/1000 samples\n",
            "Evaluated 934/1000 samples\n",
            "Evaluated 935/1000 samples\n",
            "Evaluated 936/1000 samples\n",
            "Evaluated 937/1000 samples\n",
            "Evaluated 938/1000 samples\n",
            "Evaluated 939/1000 samples\n",
            "Evaluated 940/1000 samples\n",
            "Evaluated 941/1000 samples\n",
            "Evaluated 942/1000 samples\n",
            "Evaluated 943/1000 samples\n",
            "Evaluated 944/1000 samples\n",
            "Evaluated 945/1000 samples\n",
            "Evaluated 946/1000 samples\n",
            "Evaluated 947/1000 samples\n",
            "Evaluated 948/1000 samples\n",
            "Evaluated 949/1000 samples\n",
            "Evaluated 950/1000 samples\n",
            "Evaluated 951/1000 samples\n",
            "Evaluated 952/1000 samples\n",
            "Evaluated 953/1000 samples\n",
            "Evaluated 954/1000 samples\n",
            "Evaluated 955/1000 samples\n",
            "Evaluated 956/1000 samples\n",
            "Evaluated 957/1000 samples\n",
            "Evaluated 958/1000 samples\n",
            "Evaluated 959/1000 samples\n",
            "Evaluated 960/1000 samples\n",
            "Evaluated 961/1000 samples\n",
            "Evaluated 962/1000 samples\n",
            "Evaluated 963/1000 samples\n",
            "Evaluated 964/1000 samples\n",
            "Evaluated 965/1000 samples\n",
            "Evaluated 966/1000 samples\n",
            "Evaluated 967/1000 samples\n",
            "Evaluated 968/1000 samples\n",
            "Evaluated 969/1000 samples\n",
            "Evaluated 970/1000 samples\n",
            "Evaluated 971/1000 samples\n",
            "Evaluated 972/1000 samples\n",
            "Evaluated 973/1000 samples\n",
            "Evaluated 974/1000 samples\n",
            "Evaluated 975/1000 samples\n",
            "Evaluated 976/1000 samples\n",
            "Evaluated 977/1000 samples\n",
            "Evaluated 978/1000 samples\n",
            "Evaluated 979/1000 samples\n",
            "Evaluated 980/1000 samples\n",
            "Evaluated 981/1000 samples\n",
            "Evaluated 982/1000 samples\n",
            "Evaluated 983/1000 samples\n",
            "Evaluated 984/1000 samples\n",
            "Evaluated 985/1000 samples\n",
            "Evaluated 986/1000 samples\n",
            "Evaluated 987/1000 samples\n",
            "Evaluated 988/1000 samples\n",
            "Evaluated 989/1000 samples\n",
            "Evaluated 990/1000 samples\n",
            "Evaluated 991/1000 samples\n",
            "Evaluated 992/1000 samples\n",
            "Evaluated 993/1000 samples\n",
            "Evaluated 994/1000 samples\n",
            "Evaluated 995/1000 samples\n",
            "Evaluated 996/1000 samples\n",
            "Evaluated 997/1000 samples\n",
            "Evaluated 998/1000 samples\n",
            "Evaluated 999/1000 samples\n",
            "Evaluated 1000/1000 samples\n",
            "\n",
            "Evaluation results saved to arithmetic_llm/evaluation_results/instruction_id_eval\n",
            "  - Metrics: arithmetic_llm/evaluation_results/instruction_id_eval/evaluation_metrics_20260219_233208.json\n",
            "  - Samples: arithmetic_llm/evaluation_results/instruction_id_eval/sample_outputs_20260219_233208.json\n",
            "  - Summary: arithmetic_llm/evaluation_results/instruction_id_eval/evaluation_summary_20260219_233208.txt\n",
            "\n",
            "============================================================\n",
            "EVALUATION RESULTS\n",
            "============================================================\n",
            "\n",
            "Total Samples: 1000\n",
            "Correct Samples: 406\n",
            "Parseable Samples: 905\n",
            "\n",
            "Exact Match Accuracy: 40.60%\n",
            "Parse Success Rate: 90.50%\n",
            "Avg Generation Length: 195.22 tokens\n",
            "============================================================\n",
            "\n",
            "Interpretation:\n",
            "  ~ Moderate performance - consider more training\n",
            "============================================================\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "DMiZPvZMR6YO",
        "outputId": "9356fc60-239e-429a-b3a2-8fdd553159ff",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "DMiZPvZMR6YO",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p /content/drive/MyDrive/arithmetic_llm_backup\n",
        "\n",
        "!cp -r arithmetic_llm/data/tokenizer \\\n",
        "      arithmetic_llm/models \\\n",
        "      arithmetic_llm/evaluation_results \\\n",
        "      /content/drive/MyDrive/arithmetic_llm_backup/"
      ],
      "metadata": {
        "id": "Ml5ZaC_gSEA5",
        "outputId": "872c296d-47a6-4626-df90-fda1da0291ce",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "Ml5ZaC_gSEA5",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cp: cannot stat 'arithmetic_llm/data/tokenizer': No such file or directory\n",
            "cp: cannot stat 'arithmetic_llm/models': No such file or directory\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}