{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "021690bc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "021690bc",
        "outputId": "b3133eb2-79a5-4126-8f7c-7649af5343e7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'stat359'...\n",
            "remote: Enumerating objects: 426, done.\u001b[K\n",
            "remote: Counting objects: 100% (158/158), done.\u001b[K\n",
            "remote: Compressing objects: 100% (64/64), done.\u001b[K\n",
            "remote: Total 426 (delta 139), reused 94 (delta 94), pack-reused 268 (from 3)\u001b[K\n",
            "Receiving objects: 100% (426/426), 65.48 MiB | 19.61 MiB/s, done.\n",
            "Resolving deltas: 100% (225/225), done.\n",
            "Updating files: 100% (169/169), done.\n",
            "/content/stat359/student/Final_Project\n",
            "arithmetic_llm\tProposal.docx\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/Anushka-De/stat359.git\n",
        "%cd stat359/student/Final_Project\n",
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install numpy pandas gensim torch scikit-learn matplotlib ipywidgets nltk tqdm"
      ],
      "metadata": {
        "id": "mho6YpH99zcy",
        "outputId": "4a8ec65e-87dc-45f3-afed-6464d3840863",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "mho6YpH99zcy",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.9/27.9 MB\u001b[0m \u001b[31m70.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m56.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!mkdir -p arithmetic_llm/data\n",
        "#!mkdir -p arithmetic_llm/models\n",
        "!mkdir -p arithmetic_llm/evaluation_results\n",
        "!mkdir -p arithmetic_llm/analysis\n",
        "!mkdir -p arithmetic_llm/experiments\n"
      ],
      "metadata": {
        "id": "3n3QmUV9BTP5"
      },
      "id": "3n3QmUV9BTP5",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Generate training corpus (100,000 samples recommended)\n",
        "# Generate foundational training corpus with 100K samples (plain text)\n",
        "# This large corpus provides the base model with extensive arithmetic patterns\n",
        "!python -m arithmetic_llm.generate_foundational_plaintext \\\n",
        "  --num-samples 100000 \\\n",
        "  --max-depth 4 \\\n",
        "  --num-range 1 20 \\\n",
        "  --invalid-rate 0.05 \\\n",
        "  --output-txt arithmetic_llm/data/foundational_corpus.txt\n",
        "\n",
        "# Generate mixed instruction corpus (valid + invalid)\n",
        "# This creates a balanced dataset without writing intermediate files\n",
        "!python -m arithmetic_llm.generate_instruction_corpus_mixed \\\n",
        "  --num-samples 20000 \\\n",
        "  --max-depth 4 \\\n",
        "  --num-range 1 20 \\\n",
        "  --invalid-rate 0 \\\n",
        "  --output-mixed arithmetic_llm/data/instruction_corpus.txt\n",
        "\n",
        "# Generate separate test corpus for evaluation (10K samples, minimal errors)\n",
        "# This provides a clean test set with only 1% invalid expressions\n",
        "!python -m arithmetic_llm.generate_corpus \\\n",
        "  --instruction-only \\\n",
        "  --num-samples 1000 \\\n",
        "  --max-depth 4 \\\n",
        "  --output-instruction arithmetic_llm/data/instruction_corpus_test.txt \\\n",
        "  --num-range 1 20 \\\n",
        "  --invalid-rate 0\n",
        "\n",
        "#check line counts\n",
        "!python -c \"import sys; [print(f'{sum(1 for _ in open(f))} {f}') for f in ['arithmetic_llm/data/foundational_corpus.txt', 'arithmetic_llm/data/instruction_corpus.txt', 'arithmetic_llm/data/instruction_corpus_test.txt']]\"\n",
        "\n"
      ],
      "metadata": {
        "id": "jc07_tF0_P3d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ecd7c54-4ffa-4fc4-b1a5-9455fc90c8ab"
      },
      "id": "jc07_tF0_P3d",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating instruction corpus with 1000 samples...\n",
            "Instruction corpus saved to: arithmetic_llm/data/instruction_corpus_test.txt\n",
            "Corpus generation complete!\n",
            "200000 arithmetic_llm/data/foundational_corpus.txt\n",
            "40000 arithmetic_llm/data/instruction_corpus.txt\n",
            "1000 arithmetic_llm/data/instruction_corpus_test.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#2. Tokenizer Traning\n",
        "!python -m arithmetic_llm.train_tokenizer \\\n",
        "  --corpus-path arithmetic_llm/data/foundational_corpus.txt \\\n",
        "  --vocab-size 1000 \\\n",
        "  --output-dir arithmetic_llm/data/tokenizer\n",
        "\n",
        "# show tokenizer table\n",
        "!python -m arithmetic_llm.print_token_table \\\n",
        "  --tokenizer_path arithmetic_llm/data/tokenizer/tokenizer.pkl \\\n",
        "  > tokens.csv\n",
        "\n",
        "# Analyze your instruction corpus\n",
        "!python -m arithmetic_llm.check_sequence_lengths \\\n",
        "  --corpus-path arithmetic_llm/data/instruction_corpus.txt \\\n",
        "  --tokenizer-path arithmetic_llm/data/tokenizer\n"
      ],
      "metadata": {
        "id": "tcxuzdRYEIu2",
        "outputId": "f6bb2252-d7e5-41ff-d640-89322161c74d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "tcxuzdRYEIu2",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training BPE tokenizer with vocabulary size 1000...\n",
            "Corpus: arithmetic_llm/data/foundational_corpus.txt\n",
            "Building corpus: 200000it [00:09, 22065.48it/s]\n",
            "BPE merges:  19% 188/1000 [00:00<00:00, 3564.25it/s]\n",
            "Saving tokenizer to: arithmetic_llm/data/tokenizer\n",
            "\n",
            "Tokenizer Statistics:\n",
            "  Vocabulary size: 276\n",
            "  BPE merge operations: 188\n",
            "  Special tokens: <pad>, <unk>, <bos>, <eos>, <think>, </think>\n",
            "\n",
            "Test encoding:\n",
            "  Input: 5 + 10 - 3\n",
            "  Encoded (with BOS/EOS): [176, 122, 8, 28, 11, 98, 177]\n",
            "  Decoded: <bos> 5 + 10 - 3 <eos>\n",
            "  Encoded (without BOS/EOS): [122, 8, 28, 11, 98]\n",
            "\n",
            "Test encoding:\n",
            "  Input: 12 - (4 + 2)\n",
            "  Encoded (with BOS/EOS): [176, 49, 11, 4, 110, 8, 86, 6, 177]\n",
            "  Decoded: <bos> 12 - ( 4 + 2 ) <eos>\n",
            "  Encoded (without BOS/EOS): [49, 11, 4, 110, 8, 86, 6]\n",
            "\n",
            "Test encoding:\n",
            "  Input: ((7+3)-(3+5))\n",
            "  Encoded (with BOS/EOS): [176, 4, 4, 146, 8, 98, 6, 11, 4, 98, 8, 122, 6, 6, 177]\n",
            "  Decoded: <bos> ( ( 7 + 3 ) - ( 3 + 5 ) ) <eos>\n",
            "  Encoded (without BOS/EOS): [4, 4, 146, 8, 98, 6, 11, 4, 98, 8, 122, 6, 6]\n",
            "\n",
            "Tokenizer training complete!\n",
            "Auto-detected corpus type: instruction\n",
            "Loading tokenizer from: arithmetic_llm/data/tokenizer\n",
            "\n",
            "Analyzing corpus: arithmetic_llm/data/instruction_corpus.txt\n",
            "Corpus type: instruction\n",
            "Analyzing: all lines\n",
            "\n",
            "============================================================\n",
            "SEQUENCE LENGTH ANALYSIS\n",
            "============================================================\n",
            "\n",
            "Total sequences analyzed: 40000\n",
            "\n",
            "Basic Statistics:\n",
            "  Min length:         12 tokens\n",
            "  Max length:        687 tokens\n",
            "  Mean length:     163.6 tokens\n",
            "  Median length:   125.0 tokens\n",
            "  Std deviation:   159.1 tokens\n",
            "\n",
            "Percentiles:\n",
            "   50.0th percentile:    125 tokens\n",
            "   75.0th percentile:    249 tokens\n",
            "   90.0th percentile:    402 tokens\n",
            "   95.0th percentile:    469 tokens\n",
            "   99.0th percentile:    594 tokens\n",
            "   99.5th percentile:    607 tokens\n",
            "  100.0th percentile:    687 tokens\n",
            "\n",
            "Coverage by max_seq_length:\n",
            "  max_seq_length=  64:  15648/40000 ( 39.1%) | 24352 truncated\n",
            "  max_seq_length= 128:  20908/40000 ( 52.3%) | 19092 truncated\n",
            "  max_seq_length= 192:  24180/40000 ( 60.5%) | 15820 truncated\n",
            "  max_seq_length= 256:  30619/40000 ( 76.5%) |  9381 truncated\n",
            "  max_seq_length= 384:  34539/40000 ( 86.3%) |  5461 truncated\n",
            "  max_seq_length= 512:  38376/40000 ( 95.9%) |  1624 truncated\n",
            "  max_seq_length= 768:  40000/40000 (100.0%) |     0 truncated\n",
            "  max_seq_length=1024:  40000/40000 (100.0%) |     0 truncated\n",
            "\n",
            "Recommendations:\n",
            "  • For 95% coverage: max_seq_length >= 469\n",
            "  • For 99% coverage: max_seq_length >= 594\n",
            "  • For 100% coverage: max_seq_length >= 687\n",
            "\n",
            "  Practical suggestions:\n",
            "  • Balanced (95% coverage): --max-seq-length 512\n",
            "  • Conservative (99% coverage): --max-seq-length 768\n",
            "============================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls arithmetic_llm/data\n"
      ],
      "metadata": {
        "id": "7QSTAlP4EdLq",
        "outputId": "2fa3e962-e9b6-49f9-dbba-9bce9e9ae735",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "7QSTAlP4EdLq",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "foundational_corpus.txt      instruction_corpus.txt\n",
            "instruction_corpus_test.txt  tokenizer\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Train foundational model\n",
        "!python -m arithmetic_llm.run_foundational_training \\\n",
        "  --corpus-path arithmetic_llm/data/foundational_corpus.txt \\\n",
        "  --output-dir arithmetic_llm/models \\\n",
        "  --tokenizer-path arithmetic_llm/data/tokenizer \\\n",
        "  --num-epochs 10 \\\n",
        "  --max-seq-length 512 \\\n",
        "  --batch-size 16 \\\n",
        "  --device auto\n"
      ],
      "metadata": {
        "id": "PnfIUWSAHJ8g",
        "outputId": "a9a3b4ad-79ef-4563-958d-61924291069c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "PnfIUWSAHJ8g",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "FOUNDATIONAL MODEL TRAINING\n",
            "============================================================\n",
            "\n",
            "Corpus: arithmetic_llm/data/foundational_corpus.txt\n",
            "Tokenizer: arithmetic_llm/data/tokenizer\n",
            "Output directory: arithmetic_llm/models\n",
            "\n",
            "Training Configuration:\n",
            "  Learning rate: 0.0001\n",
            "  Batch size: 16\n",
            "  Epochs: 10\n",
            "  Warmup steps: 1000\n",
            "  Gradient clip: 1.0\n",
            "  Save every: 1000 steps\n",
            "  Device: cuda\n",
            "\n",
            "Model Configuration:\n",
            "  d_model: 256\n",
            "  nhead: 8\n",
            "  num_layers: 6\n",
            "  dim_feedforward: 1024\n",
            "  dropout: 0.1\n",
            "  max_seq_length: 512\n",
            "============================================================\n",
            "\n",
            "Training output directory: arithmetic_llm/models/foundational_20260220_223759_565501\n",
            "Configuration: {'learning_rate': 0.0001, 'batch_size': 16, 'num_epochs': 10, 'warmup_steps': 1000, 'gradient_clip': 1.0, 'save_every': 1000, 'eval_every': 500, 'device': 'cuda', 'lora_config': None}\n",
            "Loading tokenizer...\n",
            "Tokenizer vocabulary size: 276\n",
            "Initializing model configuration...\n",
            "Creating dataloaders...\n",
            "Training batches: 11250\n",
            "Validation batches: 1250\n",
            "Initializing model...\n",
            "Model parameters: 4,940,800\n",
            "\n",
            "Starting training...\n",
            "\n",
            "============================================================\n",
            "Epoch 1/10\n",
            "============================================================\n",
            "Epoch 1:   9% 998/11250 [01:39<22:09,  7.71it/s, loss=1.74, avg_loss=2.59, lr=9.91e-5]\n",
            "Checkpoint saved at step 1000: arithmetic_llm/models/foundational_20260220_223759_565501/checkpoint_step_1000.pt\n",
            "Epoch 1:  18% 1999/11250 [03:27<15:23, 10.02it/s, loss=1.35, avg_loss=2.03, lr=9.91e-5]\n",
            "Checkpoint saved at step 2000: arithmetic_llm/models/foundational_20260220_223759_565501/checkpoint_step_2000.pt\n",
            "Epoch 1:  27% 2998/11250 [05:12<14:35,  9.42it/s, loss=1.33, avg_loss=1.79, lr=9.82e-5]\n",
            "Checkpoint saved at step 3000: arithmetic_llm/models/foundational_20260220_223759_565501/checkpoint_step_3000.pt\n",
            "Epoch 1:  36% 3998/11250 [07:01<14:09,  8.54it/s, loss=1.25, avg_loss=1.65, lr=9.73e-5]\n",
            "Checkpoint saved at step 4000: arithmetic_llm/models/foundational_20260220_223759_565501/checkpoint_step_4000.pt\n",
            "Epoch 1:  44% 4998/11250 [08:48<09:20, 11.15it/s, loss=1.01, avg_loss=1.54, lr=9.64e-5]\n",
            "Checkpoint saved at step 5000: arithmetic_llm/models/foundational_20260220_223759_565501/checkpoint_step_5000.pt\n",
            "Epoch 1:  53% 5998/11250 [10:36<12:01,  7.28it/s, loss=0.762, avg_loss=1.44, lr=9.55e-5]\n",
            "Checkpoint saved at step 6000: arithmetic_llm/models/foundational_20260220_223759_565501/checkpoint_step_6000.pt\n",
            "Epoch 1:  62% 6999/11250 [12:23<07:30,  9.44it/s, loss=0.896, avg_loss=1.35, lr=9.46e-5]\n",
            "Checkpoint saved at step 7000: arithmetic_llm/models/foundational_20260220_223759_565501/checkpoint_step_7000.pt\n",
            "Epoch 1:  71% 7998/11250 [14:10<05:58,  9.08it/s, loss=0.604, avg_loss=1.27, lr=9.37e-5]\n",
            "Checkpoint saved at step 8000: arithmetic_llm/models/foundational_20260220_223759_565501/checkpoint_step_8000.pt\n",
            "Epoch 1:  80% 8998/11250 [16:00<04:50,  7.75it/s, loss=0.664, avg_loss=1.2, lr=9.28e-5]\n",
            "Checkpoint saved at step 9000: arithmetic_llm/models/foundational_20260220_223759_565501/checkpoint_step_9000.pt\n",
            "Epoch 1:  89% 9999/11250 [17:48<02:34,  8.11it/s, loss=0.494, avg_loss=1.15, lr=9.19e-5]\n",
            "Checkpoint saved at step 10000: arithmetic_llm/models/foundational_20260220_223759_565501/checkpoint_step_10000.pt\n",
            "Epoch 1:  98% 10998/11250 [19:34<00:28,  8.97it/s, loss=0.531, avg_loss=1.1, lr=9.1e-5]\n",
            "Checkpoint saved at step 11000: arithmetic_llm/models/foundational_20260220_223759_565501/checkpoint_step_11000.pt\n",
            "Epoch 1: 100% 11250/11250 [20:00<00:00,  9.37it/s, loss=0.584, avg_loss=1.09, lr=9.08e-5]\n",
            "\n",
            "Evaluating on validation set...\n",
            "\n",
            "Epoch 1 Summary:\n",
            "  Training Loss: 1.0882\n",
            "  Validation Loss: 0.5487\n",
            "  New best model saved: arithmetic_llm/models/foundational_20260220_223759_565501/best_model.pt\n",
            "\n",
            "============================================================\n",
            "Epoch 2/10\n",
            "============================================================\n",
            "Epoch 2:   7% 749/11250 [01:20<16:14, 10.77it/s, loss=0.397, avg_loss=0.609, lr=9.01e-5]\n",
            "Checkpoint saved at step 12000: arithmetic_llm/models/foundational_20260220_223759_565501/checkpoint_step_12000.pt\n",
            "Epoch 2:  16% 1749/11250 [03:05<18:33,  8.54it/s, loss=0.47, avg_loss=0.604, lr=8.92e-5]\n",
            "Checkpoint saved at step 13000: arithmetic_llm/models/foundational_20260220_223759_565501/checkpoint_step_13000.pt\n",
            "Epoch 2:  24% 2749/11250 [04:56<15:21,  9.23it/s, loss=0.591, avg_loss=0.595, lr=8.83e-5]\n",
            "Checkpoint saved at step 14000: arithmetic_llm/models/foundational_20260220_223759_565501/checkpoint_step_14000.pt\n",
            "Epoch 2:  33% 3748/11250 [06:43<10:55, 11.45it/s, loss=0.752, avg_loss=0.59, lr=8.75e-5]\n",
            "Checkpoint saved at step 15000: arithmetic_llm/models/foundational_20260220_223759_565501/checkpoint_step_15000.pt\n",
            "Epoch 2:  42% 4749/11250 [08:28<13:55,  7.78it/s, loss=0.555, avg_loss=0.587, lr=8.66e-5]\n",
            "Checkpoint saved at step 16000: arithmetic_llm/models/foundational_20260220_223759_565501/checkpoint_step_16000.pt\n",
            "Epoch 2:  51% 5749/11250 [10:17<11:55,  7.69it/s, loss=0.803, avg_loss=0.582, lr=8.57e-5]\n",
            "Checkpoint saved at step 17000: arithmetic_llm/models/foundational_20260220_223759_565501/checkpoint_step_17000.pt\n",
            "Epoch 2:  60% 6748/11250 [12:03<08:13,  9.11it/s, loss=0.805, avg_loss=0.578, lr=8.48e-5]\n",
            "Checkpoint saved at step 18000: arithmetic_llm/models/foundational_20260220_223759_565501/checkpoint_step_18000.pt\n",
            "Epoch 2:  69% 7748/11250 [13:52<06:31,  8.95it/s, loss=0.408, avg_loss=0.573, lr=8.39e-5]\n",
            "Checkpoint saved at step 19000: arithmetic_llm/models/foundational_20260220_223759_565501/checkpoint_step_19000.pt\n",
            "Epoch 2:  78% 8749/11250 [15:39<04:26,  9.39it/s, loss=0.726, avg_loss=0.571, lr=8.3e-5]\n",
            "Checkpoint saved at step 20000: arithmetic_llm/models/foundational_20260220_223759_565501/checkpoint_step_20000.pt\n",
            "Epoch 2:  87% 9749/11250 [17:28<02:49,  8.86it/s, loss=0.356, avg_loss=0.567, lr=8.21e-5]\n",
            "Checkpoint saved at step 21000: arithmetic_llm/models/foundational_20260220_223759_565501/checkpoint_step_21000.pt\n",
            "Epoch 2:  96% 10749/11250 [19:13<00:58,  8.63it/s, loss=0.466, avg_loss=0.564, lr=8.12e-5]\n",
            "Checkpoint saved at step 22000: arithmetic_llm/models/foundational_20260220_223759_565501/checkpoint_step_22000.pt\n",
            "Epoch 2: 100% 11250/11250 [20:06<00:00,  9.32it/s, loss=0.665, avg_loss=0.563, lr=8.07e-5]\n",
            "\n",
            "Evaluating on validation set...\n",
            "\n",
            "Epoch 2 Summary:\n",
            "  Training Loss: 0.5631\n",
            "  Validation Loss: 0.5028\n",
            "  New best model saved: arithmetic_llm/models/foundational_20260220_223759_565501/best_model.pt\n",
            "\n",
            "============================================================\n",
            "Epoch 3/10\n",
            "============================================================\n",
            "Epoch 3:   4% 499/11250 [00:54<15:56, 11.24it/s, loss=0.56, avg_loss=0.524, lr=8.03e-5]\n",
            "Checkpoint saved at step 23000: arithmetic_llm/models/foundational_20260220_223759_565501/checkpoint_step_23000.pt\n",
            "Epoch 3:  13% 1498/11250 [02:38<22:10,  7.33it/s, loss=0.371, avg_loss=0.536, lr=7.94e-5]\n",
            "Checkpoint saved at step 24000: arithmetic_llm/models/foundational_20260220_223759_565501/checkpoint_step_24000.pt\n",
            "Epoch 3:  22% 2499/11250 [04:23<14:28, 10.08it/s, loss=0.643, avg_loss=0.533, lr=7.85e-5]\n",
            "Checkpoint saved at step 25000: arithmetic_llm/models/foundational_20260220_223759_565501/checkpoint_step_25000.pt\n",
            "Epoch 3:  31% 3499/11250 [06:10<15:09,  8.52it/s, loss=0.577, avg_loss=0.53, lr=7.76e-5]\n",
            "Checkpoint saved at step 26000: arithmetic_llm/models/foundational_20260220_223759_565501/checkpoint_step_26000.pt\n",
            "Epoch 3:  40% 4498/11250 [07:57<11:14, 10.01it/s, loss=0.612, avg_loss=0.527, lr=7.67e-5]\n",
            "Checkpoint saved at step 27000: arithmetic_llm/models/foundational_20260220_223759_565501/checkpoint_step_27000.pt\n",
            "Epoch 3:  49% 5497/11250 [09:48<13:31,  7.09it/s, loss=0.347, avg_loss=0.523, lr=7.58e-5]\n",
            "Checkpoint saved at step 28000: arithmetic_llm/models/foundational_20260220_223759_565501/checkpoint_step_28000.pt\n",
            "Epoch 3:  58% 6499/11250 [11:34<05:50, 13.54it/s, loss=0.702, avg_loss=0.523, lr=7.49e-5]\n",
            "Checkpoint saved at step 29000: arithmetic_llm/models/foundational_20260220_223759_565501/checkpoint_step_29000.pt\n",
            "Epoch 3:  67% 7499/11250 [13:23<06:26,  9.72it/s, loss=0.399, avg_loss=0.522, lr=7.4e-5]\n",
            "Checkpoint saved at step 30000: arithmetic_llm/models/foundational_20260220_223759_565501/checkpoint_step_30000.pt\n",
            "Epoch 3:  76% 8499/11250 [15:10<03:58, 11.51it/s, loss=0.52, avg_loss=0.52, lr=7.31e-5]\n",
            "Checkpoint saved at step 31000: arithmetic_llm/models/foundational_20260220_223759_565501/checkpoint_step_31000.pt\n",
            "Epoch 3:  84% 9499/11250 [17:00<03:20,  8.74it/s, loss=0.526, avg_loss=0.519, lr=7.22e-5]\n",
            "Checkpoint saved at step 32000: arithmetic_llm/models/foundational_20260220_223759_565501/checkpoint_step_32000.pt\n",
            "Epoch 3:  93% 10499/11250 [18:46<01:10, 10.60it/s, loss=0.773, avg_loss=0.518, lr=7.13e-5]\n",
            "Checkpoint saved at step 33000: arithmetic_llm/models/foundational_20260220_223759_565501/checkpoint_step_33000.pt\n",
            "Epoch 3: 100% 11250/11250 [20:08<00:00,  9.31it/s, loss=0.658, avg_loss=0.517, lr=7.06e-5]\n",
            "\n",
            "Evaluating on validation set...\n",
            "\n",
            "Epoch 3 Summary:\n",
            "  Training Loss: 0.5168\n",
            "  Validation Loss: 0.4852\n",
            "  New best model saved: arithmetic_llm/models/foundational_20260220_223759_565501/best_model.pt\n",
            "\n",
            "============================================================\n",
            "Epoch 4/10\n",
            "============================================================\n",
            "Epoch 4:   2% 249/11250 [00:26<17:24, 10.54it/s, loss=0.51, avg_loss=0.502, lr=7.04e-5]\n",
            "Checkpoint saved at step 34000: arithmetic_llm/models/foundational_20260220_223759_565501/checkpoint_step_34000.pt\n",
            "Epoch 4:  11% 1249/11250 [02:12<20:05,  8.29it/s, loss=0.474, avg_loss=0.509, lr=6.95e-5]\n",
            "Checkpoint saved at step 35000: arithmetic_llm/models/foundational_20260220_223759_565501/checkpoint_step_35000.pt\n",
            "Epoch 4:  20% 2248/11250 [04:00<18:20,  8.18it/s, loss=0.479, avg_loss=0.506, lr=6.86e-5]\n",
            "Checkpoint saved at step 36000: arithmetic_llm/models/foundational_20260220_223759_565501/checkpoint_step_36000.pt\n",
            "Epoch 4:  29% 3249/11250 [05:47<13:53,  9.60it/s, loss=0.76, avg_loss=0.502, lr=6.77e-5]\n",
            "Checkpoint saved at step 37000: arithmetic_llm/models/foundational_20260220_223759_565501/checkpoint_step_37000.pt\n",
            "Epoch 4:  38% 4248/11250 [07:38<17:11,  6.79it/s, loss=0.374, avg_loss=0.501, lr=6.68e-5]\n",
            "Checkpoint saved at step 38000: arithmetic_llm/models/foundational_20260220_223759_565501/checkpoint_step_38000.pt\n",
            "Epoch 4:  47% 5248/11250 [09:24<10:37,  9.41it/s, loss=0.753, avg_loss=0.5, lr=6.59e-5]\n",
            "Checkpoint saved at step 39000: arithmetic_llm/models/foundational_20260220_223759_565501/checkpoint_step_39000.pt\n",
            "Epoch 4:  56% 6248/11250 [11:14<07:48, 10.69it/s, loss=0.322, avg_loss=0.498, lr=6.5e-5]\n",
            "Checkpoint saved at step 40000: arithmetic_llm/models/foundational_20260220_223759_565501/checkpoint_step_40000.pt\n",
            "Epoch 4:  64% 7249/11250 [13:03<08:13,  8.10it/s, loss=0.386, avg_loss=0.497, lr=6.41e-5]\n",
            "Checkpoint saved at step 41000: arithmetic_llm/models/foundational_20260220_223759_565501/checkpoint_step_41000.pt\n",
            "Epoch 4:  73% 8249/11250 [14:49<05:27,  9.16it/s, loss=0.613, avg_loss=0.497, lr=6.32e-5]\n",
            "Checkpoint saved at step 42000: arithmetic_llm/models/foundational_20260220_223759_565501/checkpoint_step_42000.pt\n",
            "Epoch 4:  82% 9249/11250 [16:35<02:45, 12.07it/s, loss=0.608, avg_loss=0.497, lr=6.23e-5]\n",
            "Checkpoint saved at step 43000: arithmetic_llm/models/foundational_20260220_223759_565501/checkpoint_step_43000.pt\n",
            "Epoch 4:  91% 10249/11250 [18:20<01:15, 13.18it/s, loss=1.13, avg_loss=0.497, lr=6.14e-5]\n",
            "Checkpoint saved at step 44000: arithmetic_llm/models/foundational_20260220_223759_565501/checkpoint_step_44000.pt\n",
            "Epoch 4: 100% 11248/11250 [20:06<00:00, 11.11it/s, loss=0.402, avg_loss=0.497, lr=6.05e-5]\n",
            "Checkpoint saved at step 45000: arithmetic_llm/models/foundational_20260220_223759_565501/checkpoint_step_45000.pt\n",
            "Epoch 4: 100% 11250/11250 [20:07<00:00,  9.32it/s, loss=0.402, avg_loss=0.497, lr=6.05e-5]\n",
            "\n",
            "Evaluating on validation set...\n",
            "\n",
            "Epoch 4 Summary:\n",
            "  Training Loss: 0.4970\n",
            "  Validation Loss: 0.4757\n",
            "  New best model saved: arithmetic_llm/models/foundational_20260220_223759_565501/best_model.pt\n",
            "\n",
            "============================================================\n",
            "Epoch 5/10\n",
            "============================================================\n",
            "Epoch 5:   9% 999/11250 [01:47<18:11,  9.39it/s, loss=0.531, avg_loss=0.486, lr=5.96e-5]\n",
            "Checkpoint saved at step 46000: arithmetic_llm/models/foundational_20260220_223759_565501/checkpoint_step_46000.pt\n",
            "Epoch 5:  18% 1998/11250 [03:37<19:02,  8.10it/s, loss=0.415, avg_loss=0.486, lr=5.88e-5]\n",
            "Checkpoint saved at step 47000: arithmetic_llm/models/foundational_20260220_223759_565501/checkpoint_step_47000.pt\n",
            "Epoch 5:  27% 2999/11250 [05:25<17:16,  7.96it/s, loss=0.521, avg_loss=0.488, lr=5.79e-5]\n",
            "Checkpoint saved at step 48000: arithmetic_llm/models/foundational_20260220_223759_565501/checkpoint_step_48000.pt\n",
            "Epoch 5:  36% 3998/11250 [07:12<11:54, 10.15it/s, loss=0.7, avg_loss=0.488, lr=5.7e-5]\n",
            "Checkpoint saved at step 49000: arithmetic_llm/models/foundational_20260220_223759_565501/checkpoint_step_49000.pt\n",
            "Epoch 5:  44% 4999/11250 [08:58<13:14,  7.87it/s, loss=0.42, avg_loss=0.489, lr=5.61e-5]\n",
            "Checkpoint saved at step 50000: arithmetic_llm/models/foundational_20260220_223759_565501/checkpoint_step_50000.pt\n",
            "Epoch 5:  53% 5998/11250 [10:45<07:21, 11.89it/s, loss=0.42, avg_loss=0.488, lr=5.52e-5]\n",
            "Checkpoint saved at step 51000: arithmetic_llm/models/foundational_20260220_223759_565501/checkpoint_step_51000.pt\n",
            "Epoch 5:  62% 6998/11250 [12:30<07:05,  9.99it/s, loss=0.319, avg_loss=0.489, lr=5.43e-5]\n",
            "Checkpoint saved at step 52000: arithmetic_llm/models/foundational_20260220_223759_565501/checkpoint_step_52000.pt\n",
            "Epoch 5:  71% 7998/11250 [14:17<06:16,  8.65it/s, loss=1.02, avg_loss=0.489, lr=5.34e-5]\n",
            "Checkpoint saved at step 53000: arithmetic_llm/models/foundational_20260220_223759_565501/checkpoint_step_53000.pt\n",
            "Epoch 5:  80% 8998/11250 [16:05<04:53,  7.68it/s, loss=0.428, avg_loss=0.488, lr=5.25e-5]\n",
            "Checkpoint saved at step 54000: arithmetic_llm/models/foundational_20260220_223759_565501/checkpoint_step_54000.pt\n",
            "Epoch 5:  89% 9998/11250 [17:50<01:52, 11.17it/s, loss=0.315, avg_loss=0.488, lr=5.16e-5]\n",
            "Checkpoint saved at step 55000: arithmetic_llm/models/foundational_20260220_223759_565501/checkpoint_step_55000.pt\n",
            "Epoch 5:  98% 10998/11250 [19:39<00:28,  8.81it/s, loss=0.667, avg_loss=0.487, lr=5.07e-5]\n",
            "Checkpoint saved at step 56000: arithmetic_llm/models/foundational_20260220_223759_565501/checkpoint_step_56000.pt\n",
            "Epoch 5: 100% 11250/11250 [20:06<00:00,  9.32it/s, loss=0.496, avg_loss=0.487, lr=5.05e-5]\n",
            "\n",
            "Evaluating on validation set...\n",
            "\n",
            "Epoch 5 Summary:\n",
            "  Training Loss: 0.4871\n",
            "  Validation Loss: 0.4706\n",
            "  New best model saved: arithmetic_llm/models/foundational_20260220_223759_565501/best_model.pt\n",
            "\n",
            "============================================================\n",
            "Epoch 6/10\n",
            "============================================================\n",
            "Epoch 6:   7% 749/11250 [01:20<20:21,  8.59it/s, loss=0.519, avg_loss=0.486, lr=4.98e-5]\n",
            "Checkpoint saved at step 57000: arithmetic_llm/models/foundational_20260220_223759_565501/checkpoint_step_57000.pt\n",
            "Epoch 6:  16% 1749/11250 [03:05<12:08, 13.04it/s, loss=0.469, avg_loss=0.484, lr=4.89e-5]\n",
            "Checkpoint saved at step 58000: arithmetic_llm/models/foundational_20260220_223759_565501/checkpoint_step_58000.pt\n",
            "Epoch 6:  24% 2748/11250 [04:50<14:10, 10.00it/s, loss=0.445, avg_loss=0.486, lr=4.8e-5]\n",
            "Checkpoint saved at step 59000: arithmetic_llm/models/foundational_20260220_223759_565501/checkpoint_step_59000.pt\n",
            "Epoch 6:  33% 3748/11250 [06:38<10:47, 11.59it/s, loss=0.506, avg_loss=0.485, lr=4.71e-5]\n",
            "Checkpoint saved at step 60000: arithmetic_llm/models/foundational_20260220_223759_565501/checkpoint_step_60000.pt\n",
            "Epoch 6:  42% 4749/11250 [08:28<14:18,  7.57it/s, loss=0.414, avg_loss=0.484, lr=4.62e-5]\n",
            "Checkpoint saved at step 61000: arithmetic_llm/models/foundational_20260220_223759_565501/checkpoint_step_61000.pt\n",
            "Epoch 6:  51% 5748/11250 [10:15<09:19,  9.84it/s, loss=0.296, avg_loss=0.484, lr=4.53e-5]\n",
            "Checkpoint saved at step 62000: arithmetic_llm/models/foundational_20260220_223759_565501/checkpoint_step_62000.pt\n",
            "Epoch 6:  60% 6748/11250 [12:02<06:41, 11.22it/s, loss=0.397, avg_loss=0.483, lr=4.44e-5]\n",
            "Checkpoint saved at step 63000: arithmetic_llm/models/foundational_20260220_223759_565501/checkpoint_step_63000.pt\n",
            "Epoch 6:  69% 7749/11250 [13:50<04:56, 11.81it/s, loss=0.469, avg_loss=0.482, lr=4.35e-5]\n",
            "Checkpoint saved at step 64000: arithmetic_llm/models/foundational_20260220_223759_565501/checkpoint_step_64000.pt\n",
            "Epoch 6:  78% 8748/11250 [15:38<04:18,  9.67it/s, loss=0.33, avg_loss=0.481, lr=4.26e-5]\n",
            "Checkpoint saved at step 65000: arithmetic_llm/models/foundational_20260220_223759_565501/checkpoint_step_65000.pt\n",
            "Epoch 6:  87% 9749/11250 [17:25<02:11, 11.41it/s, loss=0.324, avg_loss=0.481, lr=4.17e-5]\n",
            "Checkpoint saved at step 66000: arithmetic_llm/models/foundational_20260220_223759_565501/checkpoint_step_66000.pt\n",
            "Epoch 6:  96% 10749/11250 [19:12<01:01,  8.16it/s, loss=0.304, avg_loss=0.48, lr=4.08e-5]\n",
            "Checkpoint saved at step 67000: arithmetic_llm/models/foundational_20260220_223759_565501/checkpoint_step_67000.pt\n",
            "Epoch 6: 100% 11250/11250 [20:06<00:00,  9.32it/s, loss=0.602, avg_loss=0.479, lr=4.04e-5]\n",
            "\n",
            "Evaluating on validation set...\n",
            "\n",
            "Epoch 6 Summary:\n",
            "  Training Loss: 0.4790\n",
            "  Validation Loss: 0.4669\n",
            "  New best model saved: arithmetic_llm/models/foundational_20260220_223759_565501/best_model.pt\n",
            "\n",
            "============================================================\n",
            "Epoch 7/10\n",
            "============================================================\n",
            "Epoch 7:   4% 498/11250 [00:52<15:52, 11.29it/s, loss=0.743, avg_loss=0.48, lr=3.99e-5]\n",
            "Checkpoint saved at step 68000: arithmetic_llm/models/foundational_20260220_223759_565501/checkpoint_step_68000.pt\n",
            "Epoch 7:  13% 1498/11250 [02:37<18:46,  8.66it/s, loss=0.413, avg_loss=0.481, lr=3.9e-5]\n",
            "Checkpoint saved at step 69000: arithmetic_llm/models/foundational_20260220_223759_565501/checkpoint_step_69000.pt\n",
            "Epoch 7:  22% 2499/11250 [04:23<17:08,  8.51it/s, loss=0.614, avg_loss=0.483, lr=3.81e-5]\n",
            "Checkpoint saved at step 70000: arithmetic_llm/models/foundational_20260220_223759_565501/checkpoint_step_70000.pt\n",
            "Epoch 7:  31% 3499/11250 [06:11<17:32,  7.36it/s, loss=0.231, avg_loss=0.48, lr=3.72e-5]\n",
            "Checkpoint saved at step 71000: arithmetic_llm/models/foundational_20260220_223759_565501/checkpoint_step_71000.pt\n",
            "Epoch 7:  40% 4499/11250 [08:03<14:51,  7.57it/s, loss=0.393, avg_loss=0.477, lr=3.63e-5]\n",
            "Checkpoint saved at step 72000: arithmetic_llm/models/foundational_20260220_223759_565501/checkpoint_step_72000.pt\n",
            "Epoch 7:  49% 5499/11250 [09:45<11:14,  8.53it/s, loss=0.373, avg_loss=0.479, lr=3.54e-5]\n",
            "Checkpoint saved at step 73000: arithmetic_llm/models/foundational_20260220_223759_565501/checkpoint_step_73000.pt\n",
            "Epoch 7:  58% 6499/11250 [11:33<10:00,  7.91it/s, loss=0.632, avg_loss=0.479, lr=3.45e-5]\n",
            "Checkpoint saved at step 74000: arithmetic_llm/models/foundational_20260220_223759_565501/checkpoint_step_74000.pt\n",
            "Epoch 7:  67% 7498/11250 [13:19<05:01, 12.45it/s, loss=0.592, avg_loss=0.479, lr=3.36e-5]\n",
            "Checkpoint saved at step 75000: arithmetic_llm/models/foundational_20260220_223759_565501/checkpoint_step_75000.pt\n",
            "Epoch 7:  76% 8499/11250 [15:08<05:36,  8.18it/s, loss=0.371, avg_loss=0.478, lr=3.27e-5]\n",
            "Checkpoint saved at step 76000: arithmetic_llm/models/foundational_20260220_223759_565501/checkpoint_step_76000.pt\n",
            "Epoch 7:  84% 9499/11250 [16:57<04:22,  6.67it/s, loss=0.365, avg_loss=0.477, lr=3.18e-5]\n",
            "Checkpoint saved at step 77000: arithmetic_llm/models/foundational_20260220_223759_565501/checkpoint_step_77000.pt\n",
            "Epoch 7:  93% 10499/11250 [18:44<01:27,  8.60it/s, loss=0.488, avg_loss=0.477, lr=3.09e-5]\n",
            "Checkpoint saved at step 78000: arithmetic_llm/models/foundational_20260220_223759_565501/checkpoint_step_78000.pt\n",
            "Epoch 7: 100% 11250/11250 [20:06<00:00,  9.33it/s, loss=0.928, avg_loss=0.476, lr=3.03e-5]\n",
            "\n",
            "Evaluating on validation set...\n",
            "\n",
            "Epoch 7 Summary:\n",
            "  Training Loss: 0.4757\n",
            "  Validation Loss: 0.4650\n",
            "  New best model saved: arithmetic_llm/models/foundational_20260220_223759_565501/best_model.pt\n",
            "\n",
            "============================================================\n",
            "Epoch 8/10\n",
            "============================================================\n",
            "Epoch 8:   2% 248/11250 [00:27<13:36, 13.47it/s, loss=0.413, avg_loss=0.455, lr=3.01e-5]\n",
            "Checkpoint saved at step 79000: arithmetic_llm/models/foundational_20260220_223759_565501/checkpoint_step_79000.pt\n",
            "Epoch 8:  11% 1249/11250 [02:15<22:58,  7.25it/s, loss=0.355, avg_loss=0.47, lr=2.92e-5]\n",
            "Checkpoint saved at step 80000: arithmetic_llm/models/foundational_20260220_223759_565501/checkpoint_step_80000.pt\n",
            "Epoch 8:  20% 2248/11250 [04:03<15:23,  9.75it/s, loss=0.489, avg_loss=0.473, lr=2.83e-5]\n",
            "Checkpoint saved at step 81000: arithmetic_llm/models/foundational_20260220_223759_565501/checkpoint_step_81000.pt\n",
            "Epoch 8:  29% 3248/11250 [05:51<14:41,  9.08it/s, loss=0.26, avg_loss=0.474, lr=2.74e-5]\n",
            "Checkpoint saved at step 82000: arithmetic_llm/models/foundational_20260220_223759_565501/checkpoint_step_82000.pt\n",
            "Epoch 8:  38% 4248/11250 [07:38<11:55,  9.79it/s, loss=0.359, avg_loss=0.474, lr=2.65e-5]\n",
            "Checkpoint saved at step 83000: arithmetic_llm/models/foundational_20260220_223759_565501/checkpoint_step_83000.pt\n",
            "Epoch 8:  47% 5248/11250 [09:27<11:52,  8.43it/s, loss=0.499, avg_loss=0.472, lr=2.56e-5]\n",
            "Checkpoint saved at step 84000: arithmetic_llm/models/foundational_20260220_223759_565501/checkpoint_step_84000.pt\n",
            "Epoch 8:  56% 6248/11250 [11:14<07:08, 11.68it/s, loss=0.516, avg_loss=0.473, lr=2.47e-5]\n",
            "Checkpoint saved at step 85000: arithmetic_llm/models/foundational_20260220_223759_565501/checkpoint_step_85000.pt\n",
            "Epoch 8:  64% 7249/11250 [13:04<09:43,  6.85it/s, loss=0.458, avg_loss=0.472, lr=2.38e-5]\n",
            "Checkpoint saved at step 86000: arithmetic_llm/models/foundational_20260220_223759_565501/checkpoint_step_86000.pt\n",
            "Epoch 8:  73% 8249/11250 [14:53<05:50,  8.57it/s, loss=0.408, avg_loss=0.471, lr=2.29e-5]\n",
            "Checkpoint saved at step 87000: arithmetic_llm/models/foundational_20260220_223759_565501/checkpoint_step_87000.pt\n",
            "Epoch 8:  82% 9248/11250 [16:39<03:56,  8.46it/s, loss=0.446, avg_loss=0.471, lr=2.2e-5]\n",
            "Checkpoint saved at step 88000: arithmetic_llm/models/foundational_20260220_223759_565501/checkpoint_step_88000.pt\n",
            "Epoch 8:  91% 10248/11250 [18:25<01:45,  9.50it/s, loss=0.36, avg_loss=0.472, lr=2.11e-5]\n",
            "Checkpoint saved at step 89000: arithmetic_llm/models/foundational_20260220_223759_565501/checkpoint_step_89000.pt\n",
            "Epoch 8: 100% 11248/11250 [20:12<00:00,  9.82it/s, loss=0.303, avg_loss=0.471, lr=2.02e-5]\n",
            "Checkpoint saved at step 90000: arithmetic_llm/models/foundational_20260220_223759_565501/checkpoint_step_90000.pt\n",
            "Epoch 8: 100% 11250/11250 [20:13<00:00,  9.27it/s, loss=0.303, avg_loss=0.471, lr=2.02e-5]\n",
            "\n",
            "Evaluating on validation set...\n",
            "\n",
            "Epoch 8 Summary:\n",
            "  Training Loss: 0.4713\n",
            "  Validation Loss: 0.4637\n",
            "  New best model saved: arithmetic_llm/models/foundational_20260220_223759_565501/best_model.pt\n",
            "\n",
            "============================================================\n",
            "Epoch 9/10\n",
            "============================================================\n",
            "Epoch 9:   9% 998/11250 [01:45<14:46, 11.57it/s, loss=0.348, avg_loss=0.463, lr=1.93e-5]\n",
            "Checkpoint saved at step 91000: arithmetic_llm/models/foundational_20260220_223759_565501/checkpoint_step_91000.pt\n",
            "Epoch 9:  18% 1999/11250 [03:32<17:04,  9.03it/s, loss=0.446, avg_loss=0.466, lr=1.84e-5]\n",
            "Checkpoint saved at step 92000: arithmetic_llm/models/foundational_20260220_223759_565501/checkpoint_step_92000.pt\n",
            "Epoch 9:  27% 2999/11250 [05:21<17:20,  7.93it/s, loss=0.658, avg_loss=0.467, lr=1.75e-5]\n",
            "Checkpoint saved at step 93000: arithmetic_llm/models/foundational_20260220_223759_565501/checkpoint_step_93000.pt\n",
            "Epoch 9:  36% 3999/11250 [07:03<09:09, 13.20it/s, loss=0.417, avg_loss=0.468, lr=1.66e-5]\n",
            "Checkpoint saved at step 94000: arithmetic_llm/models/foundational_20260220_223759_565501/checkpoint_step_94000.pt\n",
            "Epoch 9:  44% 4998/11250 [08:50<08:52, 11.75it/s, loss=0.434, avg_loss=0.47, lr=1.57e-5]\n",
            "Checkpoint saved at step 95000: arithmetic_llm/models/foundational_20260220_223759_565501/checkpoint_step_95000.pt\n",
            "Epoch 9:  53% 5999/11250 [10:37<09:14,  9.46it/s, loss=0.28, avg_loss=0.47, lr=1.48e-5]\n",
            "Checkpoint saved at step 96000: arithmetic_llm/models/foundational_20260220_223759_565501/checkpoint_step_96000.pt\n",
            "Epoch 9:  62% 6998/11250 [12:30<06:35, 10.74it/s, loss=0.811, avg_loss=0.469, lr=1.39e-5]\n",
            "Checkpoint saved at step 97000: arithmetic_llm/models/foundational_20260220_223759_565501/checkpoint_step_97000.pt\n",
            "Epoch 9:  71% 7999/11250 [14:17<06:37,  8.17it/s, loss=0.457, avg_loss=0.47, lr=1.3e-5]\n",
            "Checkpoint saved at step 98000: arithmetic_llm/models/foundational_20260220_223759_565501/checkpoint_step_98000.pt\n",
            "Epoch 9:  80% 8999/11250 [16:06<03:21, 11.19it/s, loss=0.409, avg_loss=0.47, lr=1.21e-5]\n",
            "Checkpoint saved at step 99000: arithmetic_llm/models/foundational_20260220_223759_565501/checkpoint_step_99000.pt\n",
            "Epoch 9:  89% 9999/11250 [17:52<01:56, 10.72it/s, loss=0.619, avg_loss=0.47, lr=1.12e-5]\n",
            "Checkpoint saved at step 100000: arithmetic_llm/models/foundational_20260220_223759_565501/checkpoint_step_100000.pt\n",
            "Epoch 9:  98% 10999/11250 [19:41<00:27,  9.12it/s, loss=0.383, avg_loss=0.469, lr=1.03e-5]\n",
            "Checkpoint saved at step 101000: arithmetic_llm/models/foundational_20260220_223759_565501/checkpoint_step_101000.pt\n",
            "Epoch 9: 100% 11250/11250 [20:07<00:00,  9.31it/s, loss=0.449, avg_loss=0.469, lr=1.01e-5]\n",
            "\n",
            "Evaluating on validation set...\n",
            "\n",
            "Epoch 9 Summary:\n",
            "  Training Loss: 0.4690\n",
            "  Validation Loss: 0.4625\n",
            "  New best model saved: arithmetic_llm/models/foundational_20260220_223759_565501/best_model.pt\n",
            "\n",
            "============================================================\n",
            "Epoch 10/10\n",
            "============================================================\n",
            "Epoch 10:   7% 749/11250 [01:22<21:23,  8.18it/s, loss=0.381, avg_loss=0.461, lr=9.43e-6]\n",
            "Checkpoint saved at step 102000: arithmetic_llm/models/foundational_20260220_223759_565501/checkpoint_step_102000.pt\n",
            "Epoch 10:  16% 1749/11250 [03:12<18:35,  8.52it/s, loss=0.831, avg_loss=0.462, lr=8.53e-6]\n",
            "Checkpoint saved at step 103000: arithmetic_llm/models/foundational_20260220_223759_565501/checkpoint_step_103000.pt\n",
            "Epoch 10:  24% 2748/11250 [04:57<18:50,  7.52it/s, loss=0.415, avg_loss=0.465, lr=7.63e-6]\n",
            "Checkpoint saved at step 104000: arithmetic_llm/models/foundational_20260220_223759_565501/checkpoint_step_104000.pt\n",
            "Epoch 10:  33% 3748/11250 [06:44<12:18, 10.15it/s, loss=0.463, avg_loss=0.464, lr=6.73e-6]\n",
            "Checkpoint saved at step 105000: arithmetic_llm/models/foundational_20260220_223759_565501/checkpoint_step_105000.pt\n",
            "Epoch 10:  42% 4748/11250 [08:32<09:01, 12.00it/s, loss=0.452, avg_loss=0.464, lr=5.84e-6]\n",
            "Checkpoint saved at step 106000: arithmetic_llm/models/foundational_20260220_223759_565501/checkpoint_step_106000.pt\n",
            "Epoch 10:  51% 5749/11250 [10:16<08:06, 11.31it/s, loss=0.482, avg_loss=0.466, lr=4.94e-6]\n",
            "Checkpoint saved at step 107000: arithmetic_llm/models/foundational_20260220_223759_565501/checkpoint_step_107000.pt\n",
            "Epoch 10:  60% 6749/11250 [12:05<07:46,  9.65it/s, loss=0.355, avg_loss=0.465, lr=4.04e-6]\n",
            "Checkpoint saved at step 108000: arithmetic_llm/models/foundational_20260220_223759_565501/checkpoint_step_108000.pt\n",
            "Epoch 10:  69% 7748/11250 [13:53<07:36,  7.67it/s, loss=0.205, avg_loss=0.465, lr=3.15e-6]\n",
            "Checkpoint saved at step 109000: arithmetic_llm/models/foundational_20260220_223759_565501/checkpoint_step_109000.pt\n",
            "Epoch 10:  78% 8748/11250 [15:36<04:01, 10.34it/s, loss=0.656, avg_loss=0.466, lr=2.25e-6]\n",
            "Checkpoint saved at step 110000: arithmetic_llm/models/foundational_20260220_223759_565501/checkpoint_step_110000.pt\n",
            "Epoch 10:  87% 9749/11250 [17:24<02:13, 11.22it/s, loss=0.389, avg_loss=0.466, lr=1.35e-6]\n",
            "Checkpoint saved at step 111000: arithmetic_llm/models/foundational_20260220_223759_565501/checkpoint_step_111000.pt\n",
            "Epoch 10:  96% 10749/11250 [19:10<01:06,  7.54it/s, loss=0.329, avg_loss=0.466, lr=4.57e-7]\n",
            "Checkpoint saved at step 112000: arithmetic_llm/models/foundational_20260220_223759_565501/checkpoint_step_112000.pt\n",
            "Epoch 10: 100% 11250/11250 [20:02<00:00,  9.36it/s, loss=0.334, avg_loss=0.467, lr=8.07e-9]\n",
            "\n",
            "Evaluating on validation set...\n",
            "\n",
            "Epoch 10 Summary:\n",
            "  Training Loss: 0.4673\n",
            "  Validation Loss: 0.4618\n",
            "  New best model saved: arithmetic_llm/models/foundational_20260220_223759_565501/best_model.pt\n",
            "\n",
            "Saving final model...\n",
            "Final model saved: arithmetic_llm/models/foundational_20260220_223759_565501/final_model.pt\n",
            "Training log saved: arithmetic_llm/models/foundational_20260220_223759_565501/training_log.json\n",
            "Training summary saved: arithmetic_llm/models/foundational_20260220_223759_565501/training_summary.json\n",
            "\n",
            "============================================================\n",
            "Training completed successfully!\n",
            "Output directory: arithmetic_llm/models/foundational_20260220_223759_565501\n",
            "============================================================\n",
            "\n",
            "============================================================\n",
            "TRAINING COMPLETED SUCCESSFULLY!\n",
            "============================================================\n",
            "Final checkpoint: arithmetic_llm/models/foundational_20260220_223759_565501/final_model.pt\n",
            "============================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls arithmetic_llm/models"
      ],
      "metadata": {
        "id": "7P_K1Aq9S214",
        "outputId": "d5b6bdf8-05f7-4468-b85b-8708bba34822",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "7P_K1Aq9S214",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "foundational_20260220_223759_565501\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Zip the folders\n",
        "!zip -r arithmetic_llm_data.zip arithmetic_llm/data\n",
        "# Download to your local machine\n",
        "from google.colab import files\n",
        "files.download(\"arithmetic_llm_data.zip\")\n",
        "files.download(\"arithmetic_llm/models/foundational_20260220_223759_565501/best_model.pt\")\n",
        "files.download(\"arithmetic_llm/models/foundational_20260220_223759_565501/final_model.pt\")\n",
        "files.download(\"arithmetic_llm/models/foundational_20260220_223759_565501/training_log.json\")\n",
        "files.download(\"arithmetic_llm/models/foundational_20260220_223759_565501/training_summary.json\")"
      ],
      "metadata": {
        "id": "yAL5y_kDAwB3",
        "outputId": "e5d232b9-89e8-4087-f02b-6ae32c54d80d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "id": "yAL5y_kDAwB3",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "updating: arithmetic_llm/data/ (stored 0%)\n",
            "updating: arithmetic_llm/data/tokenizer/ (stored 0%)\n",
            "updating: arithmetic_llm/data/tokenizer/tokenizer.pkl (deflated 52%)\n",
            "updating: arithmetic_llm/data/foundational_corpus.txt (deflated 82%)\n",
            "updating: arithmetic_llm/data/instruction_corpus.txt (deflated 86%)\n",
            "updating: arithmetic_llm/data/instruction_corpus_test.txt (deflated 86%)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_516d10db-f8c6-4411-b615-98fd2997361b\", \"arithmetic_llm_data.zip\", 7862831)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_0c49dd3f-de2b-4f86-a1b5-86efa7c4c53f\", \"best_model.pt\", 59422774)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_b317b991-1dd6-487c-84ec-87531bcb4930\", \"final_model.pt\", 59418233)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_7917536d-1b5e-4cc9-9cf1-bafac4204124\", \"training_log.json\", 1604)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_d2804824-ca62-4528-b5b0-e37eff12fca8\", \"training_summary.json\", 628)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#3.1 Evaluate the foundational model, performance would be bad\n",
        "!python -m arithmetic_llm.run_evaluation \\\n",
        "  --model-path arithmetic_llm/models/foundational_20260220_223759_565501/best_model.pt \\\n",
        "  --tokenizer-path arithmetic_llm/data/tokenizer \\\n",
        "  --max-gen-length 512 \\\n",
        "  --batch-size 1 \\\n",
        "  --num-samples 100 \\\n",
        "  --output-dir arithmetic_llm/evaluation_results/foundational_eval"
      ],
      "metadata": {
        "id": "iTN-g2K2xCGT",
        "outputId": "e15f4378-c55b-4e2d-a19f-29c80505b31c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "iTN-g2K2xCGT",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "MODEL EVALUATION\n",
            "============================================================\n",
            "\n",
            "Model: arithmetic_llm/models/foundational_20260220_223759_565501/best_model.pt\n",
            "Tokenizer: arithmetic_llm/data/tokenizer\n",
            "Device: cuda\n",
            "\n",
            "Evaluation Configuration:\n",
            "  Test samples: 100\n",
            "  Max depth: 5\n",
            "  Number range: 1 to 20\n",
            "  Batch size: 1\n",
            "  Max generation length: 512\n",
            "  Output directory: arithmetic_llm/evaluation_results/foundational_eval\n",
            "============================================================\n",
            "\n",
            "Loading model and tokenizer...\n",
            "Model loaded successfully!\n",
            "\n",
            "Starting evaluation...\n",
            "Generating 100 test expressions...\n",
            "Generated 100 valid test expressions\n",
            "Evaluating model with batch size 1...\n",
            "Evaluated 1/100 samples\n",
            "Evaluated 2/100 samples\n",
            "Evaluated 3/100 samples\n",
            "Evaluated 4/100 samples\n",
            "Evaluated 5/100 samples\n",
            "Evaluated 6/100 samples\n",
            "Evaluated 7/100 samples\n",
            "Evaluated 8/100 samples\n",
            "Evaluated 9/100 samples\n",
            "Evaluated 10/100 samples\n",
            "Evaluated 11/100 samples\n",
            "Evaluated 12/100 samples\n",
            "Evaluated 13/100 samples\n",
            "Evaluated 14/100 samples\n",
            "Evaluated 15/100 samples\n",
            "Evaluated 16/100 samples\n",
            "Evaluated 17/100 samples\n",
            "Evaluated 18/100 samples\n",
            "Evaluated 19/100 samples\n",
            "Evaluated 20/100 samples\n",
            "Evaluated 21/100 samples\n",
            "Evaluated 22/100 samples\n",
            "Evaluated 23/100 samples\n",
            "Evaluated 24/100 samples\n",
            "Evaluated 25/100 samples\n",
            "Evaluated 26/100 samples\n",
            "Evaluated 27/100 samples\n",
            "Evaluated 28/100 samples\n",
            "Evaluated 29/100 samples\n",
            "Evaluated 30/100 samples\n",
            "Evaluated 31/100 samples\n",
            "Evaluated 32/100 samples\n",
            "Evaluated 33/100 samples\n",
            "Evaluated 34/100 samples\n",
            "Evaluated 35/100 samples\n",
            "Evaluated 36/100 samples\n",
            "Evaluated 37/100 samples\n",
            "Evaluated 38/100 samples\n",
            "Evaluated 39/100 samples\n",
            "Evaluated 40/100 samples\n",
            "Evaluated 41/100 samples\n",
            "Evaluated 42/100 samples\n",
            "Evaluated 43/100 samples\n",
            "Evaluated 44/100 samples\n",
            "Evaluated 45/100 samples\n",
            "Evaluated 46/100 samples\n",
            "Evaluated 47/100 samples\n",
            "Evaluated 48/100 samples\n",
            "Evaluated 49/100 samples\n",
            "Evaluated 50/100 samples\n",
            "Evaluated 51/100 samples\n",
            "Evaluated 52/100 samples\n",
            "Evaluated 53/100 samples\n",
            "Evaluated 54/100 samples\n",
            "Evaluated 55/100 samples\n",
            "Evaluated 56/100 samples\n",
            "Evaluated 57/100 samples\n",
            "Evaluated 58/100 samples\n",
            "Evaluated 59/100 samples\n",
            "Evaluated 60/100 samples\n",
            "Evaluated 61/100 samples\n",
            "Evaluated 62/100 samples\n",
            "Evaluated 63/100 samples\n",
            "Evaluated 64/100 samples\n",
            "Evaluated 65/100 samples\n",
            "Evaluated 66/100 samples\n",
            "Evaluated 67/100 samples\n",
            "Evaluated 68/100 samples\n",
            "Evaluated 69/100 samples\n",
            "Evaluated 70/100 samples\n",
            "Evaluated 71/100 samples\n",
            "Evaluated 72/100 samples\n",
            "Evaluated 73/100 samples\n",
            "Evaluated 74/100 samples\n",
            "Evaluated 75/100 samples\n",
            "Evaluated 76/100 samples\n",
            "Evaluated 77/100 samples\n",
            "Evaluated 78/100 samples\n",
            "Evaluated 79/100 samples\n",
            "Evaluated 80/100 samples\n",
            "Evaluated 81/100 samples\n",
            "Evaluated 82/100 samples\n",
            "Evaluated 83/100 samples\n",
            "Evaluated 84/100 samples\n",
            "Evaluated 85/100 samples\n",
            "Evaluated 86/100 samples\n",
            "Evaluated 87/100 samples\n",
            "Evaluated 88/100 samples\n",
            "Evaluated 89/100 samples\n",
            "Evaluated 90/100 samples\n",
            "Evaluated 91/100 samples\n",
            "Evaluated 92/100 samples\n",
            "Evaluated 93/100 samples\n",
            "Evaluated 94/100 samples\n",
            "Evaluated 95/100 samples\n",
            "Evaluated 96/100 samples\n",
            "Evaluated 97/100 samples\n",
            "Evaluated 98/100 samples\n",
            "Evaluated 99/100 samples\n",
            "Evaluated 100/100 samples\n",
            "\n",
            "Evaluation results saved to arithmetic_llm/evaluation_results/foundational_eval\n",
            "  - Metrics: arithmetic_llm/evaluation_results/foundational_eval/evaluation_metrics_20260221_022914.json\n",
            "  - Samples: arithmetic_llm/evaluation_results/foundational_eval/sample_outputs_20260221_022914.json\n",
            "  - Summary: arithmetic_llm/evaluation_results/foundational_eval/evaluation_summary_20260221_022914.txt\n",
            "\n",
            "============================================================\n",
            "EVALUATION RESULTS\n",
            "============================================================\n",
            "\n",
            "Total Samples: 100\n",
            "Correct Samples: 0\n",
            "Parseable Samples: 0\n",
            "\n",
            "Exact Match Accuracy: 0.00%\n",
            "Parse Success Rate: 0.00%\n",
            "Avg Generation Length: 42.70 tokens\n",
            "============================================================\n",
            "\n",
            "Interpretation:\n",
            "  ✗ Poor performance - model needs more training or debugging\n",
            "  ! Low parse success rate - model may need better instruction tuning\n",
            "============================================================\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls arithmetic_llm/models"
      ],
      "metadata": {
        "id": "maIX6eOrB-m7",
        "outputId": "d94c718d-d942-40d6-f68f-edd227bc4666",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "maIX6eOrB-m7",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "foundational_20260220_223759_565501\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Fine-tune instruction model\n",
        "!python -m arithmetic_llm.run_instruction_training \\\n",
        "  --instruction-corpus-path arithmetic_llm/data/instruction_corpus.txt \\\n",
        "  --output-dir arithmetic_llm/models \\\n",
        "  --tokenizer-path arithmetic_llm/data/tokenizer \\\n",
        "  --foundational-checkpoint arithmetic_llm/models/foundational_20260220_223759_565501/best_model.pt \\\n",
        "  --num-epochs 10 \\\n",
        "  --batch-size 16 \\\n",
        "  --device auto"
      ],
      "metadata": {
        "id": "CoqjxRPe7ThA",
        "outputId": "03395501-9b5d-474c-cac5-28540e82e8c4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "CoqjxRPe7ThA",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "INSTRUCTION FINE-TUNING\n",
            "============================================================\n",
            "\n",
            "Instruction corpus: arithmetic_llm/data/instruction_corpus.txt\n",
            "Tokenizer: arithmetic_llm/data/tokenizer\n",
            "Foundational checkpoint: arithmetic_llm/models/foundational_20260220_223759_565501/best_model.pt\n",
            "Output directory: arithmetic_llm/models\n",
            "\n",
            "Training Configuration:\n",
            "  Learning rate: 5e-05\n",
            "  Batch size: 16\n",
            "  Epochs: 10\n",
            "  Warmup steps: 500\n",
            "  Gradient clip: 1.0\n",
            "  Save every: 500 steps\n",
            "  Device: cuda\n",
            "============================================================\n",
            "\n",
            "Fine-tuning output directory: arithmetic_llm/models/instruction_20260221_044544_481715\n",
            "Configuration: {'learning_rate': 5e-05, 'batch_size': 16, 'num_epochs': 10, 'warmup_steps': 500, 'gradient_clip': 1.0, 'save_every': 500, 'eval_every': 500, 'device': 'cuda', 'lora_config': None}\n",
            "Loading tokenizer...\n",
            "Tokenizer vocabulary size: 276\n",
            "Initializing model architecture...\n",
            "Creating dataloaders...\n",
            "Training batches: 2250\n",
            "Validation batches: 250\n",
            "Loading foundational model from: arithmetic_llm/models/foundational_20260220_223759_565501/best_model.pt\n",
            "Loaded checkpoint from epoch 10, step 112500\n",
            "Model parameters: 4,940,800\n",
            "\n",
            "Starting instruction fine-tuning...\n",
            "\n",
            "============================================================\n",
            "Epoch 1/10\n",
            "============================================================\n",
            "Epoch 1:  22% 499/2250 [01:22<04:38,  6.30it/s, loss=0.0193, avg_loss=0.0752, lr=4.91e-5]\n",
            "Checkpoint saved at step 500: arithmetic_llm/models/instruction_20260221_044544_481715/checkpoint_step_500.pt\n",
            "Epoch 1:  44% 999/2250 [02:46<03:12,  6.50it/s, loss=0.0207, avg_loss=0.0459, lr=4.89e-5]\n",
            "Checkpoint saved at step 1000: arithmetic_llm/models/instruction_20260221_044544_481715/checkpoint_step_1000.pt\n",
            "Epoch 1:  67% 1499/2250 [04:10<02:02,  6.15it/s, loss=0.0167, avg_loss=0.0359, lr=4.77e-5]\n",
            "Checkpoint saved at step 1500: arithmetic_llm/models/instruction_20260221_044544_481715/checkpoint_step_1500.pt\n",
            "Epoch 1:  89% 1999/2250 [05:36<00:43,  5.72it/s, loss=0.011, avg_loss=0.0308, lr=4.66e-5]\n",
            "Checkpoint saved at step 2000: arithmetic_llm/models/instruction_20260221_044544_481715/checkpoint_step_2000.pt\n",
            "Epoch 1: 100% 2250/2250 [06:19<00:00,  5.93it/s, loss=0.0229, avg_loss=0.029, lr=4.6e-5]\n",
            "\n",
            "Evaluating on validation set...\n",
            "\n",
            "Epoch 1 Summary:\n",
            "  Training Loss: 0.0290\n",
            "  Validation Loss: 0.0069\n",
            "  New best model saved: arithmetic_llm/models/instruction_20260221_044544_481715/best_model.pt\n",
            "\n",
            "============================================================\n",
            "Epoch 2/10\n",
            "============================================================\n",
            "Epoch 2:  11% 249/2250 [00:41<05:44,  5.80it/s, loss=0.019, avg_loss=0.0148, lr=4.55e-5]\n",
            "Checkpoint saved at step 2500: arithmetic_llm/models/instruction_20260221_044544_481715/checkpoint_step_2500.pt\n",
            "Epoch 2:  33% 749/2250 [02:06<03:23,  7.38it/s, loss=0.0126, avg_loss=0.0149, lr=4.43e-5]\n",
            "Checkpoint saved at step 3000: arithmetic_llm/models/instruction_20260221_044544_481715/checkpoint_step_3000.pt\n",
            "Epoch 2:  56% 1249/2250 [03:31<02:33,  6.51it/s, loss=0.0194, avg_loss=0.0148, lr=4.32e-5]\n",
            "Checkpoint saved at step 3500: arithmetic_llm/models/instruction_20260221_044544_481715/checkpoint_step_3500.pt\n",
            "Epoch 2:  78% 1749/2250 [04:56<01:30,  5.54it/s, loss=0.0144, avg_loss=0.0147, lr=4.21e-5]\n",
            "Checkpoint saved at step 4000: arithmetic_llm/models/instruction_20260221_044544_481715/checkpoint_step_4000.pt\n",
            "Epoch 2: 100% 2249/2250 [06:22<00:00,  6.68it/s, loss=0.0214, avg_loss=0.0144, lr=4.09e-5]\n",
            "Checkpoint saved at step 4500: arithmetic_llm/models/instruction_20260221_044544_481715/checkpoint_step_4500.pt\n",
            "Epoch 2: 100% 2250/2250 [06:22<00:00,  5.88it/s, loss=0.0214, avg_loss=0.0144, lr=4.09e-5]\n",
            "\n",
            "Evaluating on validation set...\n",
            "\n",
            "Epoch 2 Summary:\n",
            "  Training Loss: 0.0144\n",
            "  Validation Loss: 0.0065\n",
            "  New best model saved: arithmetic_llm/models/instruction_20260221_044544_481715/best_model.pt\n",
            "\n",
            "============================================================\n",
            "Epoch 3/10\n",
            "============================================================\n",
            "Epoch 3:  22% 499/2250 [01:24<05:06,  5.71it/s, loss=0.0103, avg_loss=0.0136, lr=3.98e-5]\n",
            "Checkpoint saved at step 5000: arithmetic_llm/models/instruction_20260221_044544_481715/checkpoint_step_5000.pt\n",
            "Epoch 3:  44% 999/2250 [02:49<03:21,  6.20it/s, loss=0.0134, avg_loss=0.0132, lr=3.87e-5]\n",
            "Checkpoint saved at step 5500: arithmetic_llm/models/instruction_20260221_044544_481715/checkpoint_step_5500.pt\n",
            "Epoch 3:  67% 1499/2250 [04:14<02:18,  5.41it/s, loss=0.0138, avg_loss=0.0132, lr=3.75e-5]\n",
            "Checkpoint saved at step 6000: arithmetic_llm/models/instruction_20260221_044544_481715/checkpoint_step_6000.pt\n",
            "Epoch 3:  89% 1999/2250 [05:40<00:45,  5.46it/s, loss=0.0135, avg_loss=0.0131, lr=3.64e-5]\n",
            "Checkpoint saved at step 6500: arithmetic_llm/models/instruction_20260221_044544_481715/checkpoint_step_6500.pt\n",
            "Epoch 3: 100% 2250/2250 [06:22<00:00,  5.88it/s, loss=0.0126, avg_loss=0.013, lr=3.58e-5]\n",
            "\n",
            "Evaluating on validation set...\n",
            "\n",
            "Epoch 3 Summary:\n",
            "  Training Loss: 0.0130\n",
            "  Validation Loss: 0.0058\n",
            "  New best model saved: arithmetic_llm/models/instruction_20260221_044544_481715/best_model.pt\n",
            "\n",
            "============================================================\n",
            "Epoch 4/10\n",
            "============================================================\n",
            "Epoch 4:  11% 248/2250 [00:42<06:00,  5.55it/s, loss=0.0137, avg_loss=0.012, lr=3.52e-5]\n",
            "Checkpoint saved at step 7000: arithmetic_llm/models/instruction_20260221_044544_481715/checkpoint_step_7000.pt\n",
            "Epoch 4:  33% 749/2250 [02:06<04:17,  5.84it/s, loss=0.0151, avg_loss=0.0119, lr=3.41e-5]\n",
            "Checkpoint saved at step 7500: arithmetic_llm/models/instruction_20260221_044544_481715/checkpoint_step_7500.pt\n",
            "Epoch 4:  56% 1249/2250 [03:31<03:03,  5.46it/s, loss=0.0116, avg_loss=0.0121, lr=3.3e-5]\n",
            "Checkpoint saved at step 8000: arithmetic_llm/models/instruction_20260221_044544_481715/checkpoint_step_8000.pt\n",
            "Epoch 4:  78% 1749/2250 [04:56<01:29,  5.63it/s, loss=0.0129, avg_loss=0.0121, lr=3.18e-5]\n",
            "Checkpoint saved at step 8500: arithmetic_llm/models/instruction_20260221_044544_481715/checkpoint_step_8500.pt\n",
            "Epoch 4: 100% 2249/2250 [06:22<00:00,  6.90it/s, loss=0.0141, avg_loss=0.0119, lr=3.07e-5]\n",
            "Checkpoint saved at step 9000: arithmetic_llm/models/instruction_20260221_044544_481715/checkpoint_step_9000.pt\n",
            "Epoch 4: 100% 2250/2250 [06:22<00:00,  5.88it/s, loss=0.0141, avg_loss=0.0119, lr=3.07e-5]\n",
            "\n",
            "Evaluating on validation set...\n",
            "\n",
            "Epoch 4 Summary:\n",
            "  Training Loss: 0.0119\n",
            "  Validation Loss: 0.0058\n",
            "  New best model saved: arithmetic_llm/models/instruction_20260221_044544_481715/best_model.pt\n",
            "\n",
            "============================================================\n",
            "Epoch 5/10\n",
            "============================================================\n",
            "Epoch 5:  22% 499/2250 [01:24<05:05,  5.74it/s, loss=0.0107, avg_loss=0.0115, lr=2.96e-5]\n",
            "Checkpoint saved at step 9500: arithmetic_llm/models/instruction_20260221_044544_481715/checkpoint_step_9500.pt\n",
            "Epoch 5:  44% 999/2250 [02:49<03:23,  6.15it/s, loss=0.0109, avg_loss=0.0111, lr=2.84e-5]\n",
            "Checkpoint saved at step 10000: arithmetic_llm/models/instruction_20260221_044544_481715/checkpoint_step_10000.pt\n",
            "Epoch 5:  67% 1499/2250 [04:14<02:11,  5.73it/s, loss=0.0096, avg_loss=0.0111, lr=2.73e-5]\n",
            "Checkpoint saved at step 10500: arithmetic_llm/models/instruction_20260221_044544_481715/checkpoint_step_10500.pt\n",
            "Epoch 5:  89% 1999/2250 [05:39<00:45,  5.46it/s, loss=0.00637, avg_loss=0.011, lr=2.62e-5]\n",
            "Checkpoint saved at step 11000: arithmetic_llm/models/instruction_20260221_044544_481715/checkpoint_step_11000.pt\n",
            "Epoch 5: 100% 2250/2250 [06:22<00:00,  5.89it/s, loss=0.00923, avg_loss=0.011, lr=2.56e-5]\n",
            "\n",
            "Evaluating on validation set...\n",
            "\n",
            "Epoch 5 Summary:\n",
            "  Training Loss: 0.0110\n",
            "  Validation Loss: 0.0052\n",
            "  New best model saved: arithmetic_llm/models/instruction_20260221_044544_481715/best_model.pt\n",
            "\n",
            "============================================================\n",
            "Epoch 6/10\n",
            "============================================================\n",
            "Epoch 6:  11% 249/2250 [00:41<06:03,  5.51it/s, loss=0.00957, avg_loss=0.0108, lr=2.5e-5]\n",
            "Checkpoint saved at step 11500: arithmetic_llm/models/instruction_20260221_044544_481715/checkpoint_step_11500.pt\n",
            "Epoch 6:  33% 749/2250 [02:06<03:46,  6.62it/s, loss=0.0116, avg_loss=0.0103, lr=2.39e-5]\n",
            "Checkpoint saved at step 12000: arithmetic_llm/models/instruction_20260221_044544_481715/checkpoint_step_12000.pt\n",
            "Epoch 6:  56% 1249/2250 [03:30<03:00,  5.56it/s, loss=0.00439, avg_loss=0.0102, lr=2.27e-5]\n",
            "Checkpoint saved at step 12500: arithmetic_llm/models/instruction_20260221_044544_481715/checkpoint_step_12500.pt\n",
            "Epoch 6:  78% 1748/2250 [04:55<01:19,  6.35it/s, loss=0.00588, avg_loss=0.0101, lr=2.16e-5]\n",
            "Checkpoint saved at step 13000: arithmetic_llm/models/instruction_20260221_044544_481715/checkpoint_step_13000.pt\n",
            "Epoch 6: 100% 2249/2250 [06:22<00:00,  5.78it/s, loss=0.00576, avg_loss=0.0101, lr=2.05e-5]\n",
            "Checkpoint saved at step 13500: arithmetic_llm/models/instruction_20260221_044544_481715/checkpoint_step_13500.pt\n",
            "Epoch 6: 100% 2250/2250 [06:22<00:00,  5.88it/s, loss=0.00576, avg_loss=0.0101, lr=2.05e-5]\n",
            "\n",
            "Evaluating on validation set...\n",
            "\n",
            "Epoch 6 Summary:\n",
            "  Training Loss: 0.0101\n",
            "  Validation Loss: 0.0049\n",
            "  New best model saved: arithmetic_llm/models/instruction_20260221_044544_481715/best_model.pt\n",
            "\n",
            "============================================================\n",
            "Epoch 7/10\n",
            "============================================================\n",
            "Epoch 7:  22% 499/2250 [01:24<04:45,  6.13it/s, loss=0.0082, avg_loss=0.00985, lr=1.93e-5]\n",
            "Checkpoint saved at step 14000: arithmetic_llm/models/instruction_20260221_044544_481715/checkpoint_step_14000.pt\n",
            "Epoch 7:  44% 999/2250 [02:49<03:22,  6.18it/s, loss=0.0088, avg_loss=0.0097, lr=1.82e-5]\n",
            "Checkpoint saved at step 14500: arithmetic_llm/models/instruction_20260221_044544_481715/checkpoint_step_14500.pt\n",
            "Epoch 7:  67% 1499/2250 [04:14<02:16,  5.50it/s, loss=0.0135, avg_loss=0.00953, lr=1.71e-5]\n",
            "Checkpoint saved at step 15000: arithmetic_llm/models/instruction_20260221_044544_481715/checkpoint_step_15000.pt\n",
            "Epoch 7:  89% 1999/2250 [05:38<00:42,  5.89it/s, loss=0.00908, avg_loss=0.00944, lr=1.59e-5]\n",
            "Checkpoint saved at step 15500: arithmetic_llm/models/instruction_20260221_044544_481715/checkpoint_step_15500.pt\n",
            "Epoch 7: 100% 2250/2250 [06:21<00:00,  5.90it/s, loss=0.00664, avg_loss=0.00938, lr=1.54e-5]\n",
            "\n",
            "Evaluating on validation set...\n",
            "\n",
            "Epoch 7 Summary:\n",
            "  Training Loss: 0.0094\n",
            "  Validation Loss: 0.0047\n",
            "  New best model saved: arithmetic_llm/models/instruction_20260221_044544_481715/best_model.pt\n",
            "\n",
            "============================================================\n",
            "Epoch 8/10\n",
            "============================================================\n",
            "Epoch 8:  11% 249/2250 [00:42<06:04,  5.49it/s, loss=0.0133, avg_loss=0.00904, lr=1.48e-5]\n",
            "Checkpoint saved at step 16000: arithmetic_llm/models/instruction_20260221_044544_481715/checkpoint_step_16000.pt\n",
            "Epoch 8:  33% 749/2250 [02:08<04:13,  5.92it/s, loss=0.0102, avg_loss=0.0089, lr=1.37e-5]\n",
            "Checkpoint saved at step 16500: arithmetic_llm/models/instruction_20260221_044544_481715/checkpoint_step_16500.pt\n",
            "Epoch 8:  56% 1249/2250 [03:32<02:35,  6.44it/s, loss=0.00793, avg_loss=0.00882, lr=1.25e-5]\n",
            "Checkpoint saved at step 17000: arithmetic_llm/models/instruction_20260221_044544_481715/checkpoint_step_17000.pt\n",
            "Epoch 8:  78% 1749/2250 [04:58<01:33,  5.35it/s, loss=0.00987, avg_loss=0.00875, lr=1.14e-5]\n",
            "Checkpoint saved at step 17500: arithmetic_llm/models/instruction_20260221_044544_481715/checkpoint_step_17500.pt\n",
            "Epoch 8: 100% 2249/2250 [06:22<00:00,  6.09it/s, loss=0.00211, avg_loss=0.0087, lr=1.02e-5]\n",
            "Checkpoint saved at step 18000: arithmetic_llm/models/instruction_20260221_044544_481715/checkpoint_step_18000.pt\n",
            "Epoch 8: 100% 2250/2250 [06:23<00:00,  5.87it/s, loss=0.00211, avg_loss=0.0087, lr=1.02e-5]\n",
            "\n",
            "Evaluating on validation set...\n",
            "\n",
            "Epoch 8 Summary:\n",
            "  Training Loss: 0.0087\n",
            "  Validation Loss: 0.0045\n",
            "  New best model saved: arithmetic_llm/models/instruction_20260221_044544_481715/best_model.pt\n",
            "\n",
            "============================================================\n",
            "Epoch 9/10\n",
            "============================================================\n",
            "Epoch 9:  22% 499/2250 [01:23<04:59,  5.84it/s, loss=0.00634, avg_loss=0.00821, lr=9.11e-6]\n",
            "Checkpoint saved at step 18500: arithmetic_llm/models/instruction_20260221_044544_481715/checkpoint_step_18500.pt\n",
            "Epoch 9:  44% 999/2250 [02:50<03:21,  6.21it/s, loss=0.00926, avg_loss=0.00828, lr=7.98e-6]\n",
            "Checkpoint saved at step 19000: arithmetic_llm/models/instruction_20260221_044544_481715/checkpoint_step_19000.pt\n",
            "Epoch 9:  67% 1499/2250 [04:14<02:07,  5.88it/s, loss=0.0107, avg_loss=0.00819, lr=6.84e-6]\n",
            "Checkpoint saved at step 19500: arithmetic_llm/models/instruction_20260221_044544_481715/checkpoint_step_19500.pt\n",
            "Epoch 9:  89% 1999/2250 [05:39<00:43,  5.72it/s, loss=0.00443, avg_loss=0.00818, lr=5.7e-6]\n",
            "Checkpoint saved at step 20000: arithmetic_llm/models/instruction_20260221_044544_481715/checkpoint_step_20000.pt\n",
            "Epoch 9: 100% 2250/2250 [06:21<00:00,  5.89it/s, loss=0.00911, avg_loss=0.00812, lr=5.13e-6]\n",
            "\n",
            "Evaluating on validation set...\n",
            "\n",
            "Epoch 9 Summary:\n",
            "  Training Loss: 0.0081\n",
            "  Validation Loss: 0.0044\n",
            "  New best model saved: arithmetic_llm/models/instruction_20260221_044544_481715/best_model.pt\n",
            "\n",
            "============================================================\n",
            "Epoch 10/10\n",
            "============================================================\n",
            "Epoch 10:  11% 249/2250 [00:42<05:58,  5.58it/s, loss=0.0056, avg_loss=0.00787, lr=4.57e-6]\n",
            "Checkpoint saved at step 20500: arithmetic_llm/models/instruction_20260221_044544_481715/checkpoint_step_20500.pt\n",
            "Epoch 10:  33% 749/2250 [02:07<04:28,  5.58it/s, loss=0.0048, avg_loss=0.00786, lr=3.43e-6]\n",
            "Checkpoint saved at step 21000: arithmetic_llm/models/instruction_20260221_044544_481715/checkpoint_step_21000.pt\n",
            "Epoch 10:  56% 1249/2250 [03:34<02:53,  5.77it/s, loss=0.0034, avg_loss=0.00787, lr=2.29e-6]\n",
            "Checkpoint saved at step 21500: arithmetic_llm/models/instruction_20260221_044544_481715/checkpoint_step_21500.pt\n",
            "Epoch 10:  78% 1749/2250 [04:58<01:28,  5.69it/s, loss=0.00624, avg_loss=0.00788, lr=1.16e-6]\n",
            "Checkpoint saved at step 22000: arithmetic_llm/models/instruction_20260221_044544_481715/checkpoint_step_22000.pt\n",
            "Epoch 10: 100% 2249/2250 [06:22<00:00,  5.92it/s, loss=0.00734, avg_loss=0.00785, lr=2.05e-8]\n",
            "Checkpoint saved at step 22500: arithmetic_llm/models/instruction_20260221_044544_481715/checkpoint_step_22500.pt\n",
            "Epoch 10: 100% 2250/2250 [06:23<00:00,  5.87it/s, loss=0.00734, avg_loss=0.00785, lr=2.05e-8]\n",
            "\n",
            "Evaluating on validation set...\n",
            "\n",
            "Epoch 10 Summary:\n",
            "  Training Loss: 0.0078\n",
            "  Validation Loss: 0.0043\n",
            "  New best model saved: arithmetic_llm/models/instruction_20260221_044544_481715/best_model.pt\n",
            "\n",
            "Saving final fine-tuned model...\n",
            "Final model saved: arithmetic_llm/models/instruction_20260221_044544_481715/final_model.pt\n",
            "Training log saved: arithmetic_llm/models/instruction_20260221_044544_481715/training_log.json\n",
            "Training summary saved: arithmetic_llm/models/instruction_20260221_044544_481715/training_summary.json\n",
            "\n",
            "============================================================\n",
            "Instruction fine-tuning completed successfully!\n",
            "Output directory: arithmetic_llm/models/instruction_20260221_044544_481715\n",
            "============================================================\n",
            "\n",
            "============================================================\n",
            "FINE-TUNING COMPLETED SUCCESSFULLY!\n",
            "============================================================\n",
            "Final checkpoint: arithmetic_llm/models/instruction_20260221_044544_481715/final_model.pt\n",
            "============================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls arithmetic_llm/models"
      ],
      "metadata": {
        "id": "1NNj8FQQLVy9",
        "outputId": "af41fb76-97cd-4b57-c531-13a82f8fe266",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "1NNj8FQQLVy9",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "foundational_20260220_223759_565501  instruction_20260221_044544_481715\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download(\"arithmetic_llm/models/instruction_20260221_044544_481715/best_model.pt\")\n",
        "files.download(\"arithmetic_llm/models/instruction_20260221_044544_481715/final_model.pt\")\n",
        "files.download(\"arithmetic_llm/models/instruction_20260221_044544_481715/training_log.json\")\n",
        "files.download(\"arithmetic_llm/models/instruction_20260221_044544_481715/training_summary.json\")"
      ],
      "metadata": {
        "id": "DkIKqJFXhYVo",
        "outputId": "dde3ba28-094e-4b70-d17e-503a0ad7ccdd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        }
      },
      "id": "DkIKqJFXhYVo",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_4dd92411-6a83-42c6-8ef6-0c0115ee07d7\", \"best_model.pt\", 59422303)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_07c0d7a3-152e-4a95-8aa6-8e842e28145e\", \"final_model.pt\", 59418169)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_e900e372-3af0-4067-935a-8873c55236ef\", \"training_log.json\", 1638)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_0638b852-9c4b-4914-91f0-97b692d067c9\", \"training_summary.json\", 733)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 4.1 Evaluate the model\n",
        "!python -m arithmetic_llm.run_evaluation \\\n",
        "  --model-path arithmetic_llm/models/instruction_20260221_044544_481715/best_model.pt \\\n",
        "  --tokenizer-path arithmetic_llm/data/tokenizer \\\n",
        "  --max-gen-length 512 \\\n",
        "  --batch-size 1 \\\n",
        "  --num-samples 1000 \\\n",
        "  --output-dir arithmetic_llm/evaluation_results/instruction_id_eval"
      ],
      "metadata": {
        "collapsed": true,
        "id": "w4LFiyQC7sXx",
        "outputId": "147fd3d7-9e9d-4230-b7d9-8b205577f112",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "w4LFiyQC7sXx",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "MODEL EVALUATION\n",
            "============================================================\n",
            "\n",
            "Model: arithmetic_llm/models/instruction_20260221_044544_481715/best_model.pt\n",
            "Tokenizer: arithmetic_llm/data/tokenizer\n",
            "Device: cuda\n",
            "\n",
            "Evaluation Configuration:\n",
            "  Test samples: 1000\n",
            "  Max depth: 5\n",
            "  Number range: 1 to 20\n",
            "  Batch size: 1\n",
            "  Max generation length: 512\n",
            "  Output directory: arithmetic_llm/evaluation_results/instruction_id_eval\n",
            "============================================================\n",
            "\n",
            "Loading model and tokenizer...\n",
            "Model loaded successfully!\n",
            "\n",
            "Starting evaluation...\n",
            "Generating 1000 test expressions...\n",
            "Generated 1000 valid test expressions\n",
            "Evaluating model with batch size 1...\n",
            "Evaluated 1/1000 samples\n",
            "Evaluated 2/1000 samples\n",
            "Evaluated 3/1000 samples\n",
            "Evaluated 4/1000 samples\n",
            "Evaluated 5/1000 samples\n",
            "Evaluated 6/1000 samples\n",
            "Evaluated 7/1000 samples\n",
            "Evaluated 8/1000 samples\n",
            "Evaluated 9/1000 samples\n",
            "Evaluated 10/1000 samples\n",
            "Evaluated 11/1000 samples\n",
            "Evaluated 12/1000 samples\n",
            "Evaluated 13/1000 samples\n",
            "Evaluated 14/1000 samples\n",
            "Evaluated 15/1000 samples\n",
            "Evaluated 16/1000 samples\n",
            "Evaluated 17/1000 samples\n",
            "Evaluated 18/1000 samples\n",
            "Evaluated 19/1000 samples\n",
            "Evaluated 20/1000 samples\n",
            "Evaluated 21/1000 samples\n",
            "Evaluated 22/1000 samples\n",
            "Evaluated 23/1000 samples\n",
            "Evaluated 24/1000 samples\n",
            "Evaluated 25/1000 samples\n",
            "Evaluated 26/1000 samples\n",
            "Evaluated 27/1000 samples\n",
            "Evaluated 28/1000 samples\n",
            "Evaluated 29/1000 samples\n",
            "Evaluated 30/1000 samples\n",
            "Evaluated 31/1000 samples\n",
            "Evaluated 32/1000 samples\n",
            "Evaluated 33/1000 samples\n",
            "Evaluated 34/1000 samples\n",
            "Evaluated 35/1000 samples\n",
            "Evaluated 36/1000 samples\n",
            "Evaluated 37/1000 samples\n",
            "Evaluated 38/1000 samples\n",
            "Evaluated 39/1000 samples\n",
            "Evaluated 40/1000 samples\n",
            "Evaluated 41/1000 samples\n",
            "Evaluated 42/1000 samples\n",
            "Evaluated 43/1000 samples\n",
            "Evaluated 44/1000 samples\n",
            "Evaluated 45/1000 samples\n",
            "Evaluated 46/1000 samples\n",
            "Evaluated 47/1000 samples\n",
            "Evaluated 48/1000 samples\n",
            "Evaluated 49/1000 samples\n",
            "Evaluated 50/1000 samples\n",
            "Evaluated 51/1000 samples\n",
            "Evaluated 52/1000 samples\n",
            "Evaluated 53/1000 samples\n",
            "Evaluated 54/1000 samples\n",
            "Evaluated 55/1000 samples\n",
            "Evaluated 56/1000 samples\n",
            "Evaluated 57/1000 samples\n",
            "Evaluated 58/1000 samples\n",
            "Evaluated 59/1000 samples\n",
            "Evaluated 60/1000 samples\n",
            "Evaluated 61/1000 samples\n",
            "Evaluated 62/1000 samples\n",
            "Evaluated 63/1000 samples\n",
            "Evaluated 64/1000 samples\n",
            "Evaluated 65/1000 samples\n",
            "Evaluated 66/1000 samples\n",
            "Evaluated 67/1000 samples\n",
            "Evaluated 68/1000 samples\n",
            "Evaluated 69/1000 samples\n",
            "Evaluated 70/1000 samples\n",
            "Evaluated 71/1000 samples\n",
            "Evaluated 72/1000 samples\n",
            "Evaluated 73/1000 samples\n",
            "Evaluated 74/1000 samples\n",
            "Evaluated 75/1000 samples\n",
            "Evaluated 76/1000 samples\n",
            "Evaluated 77/1000 samples\n",
            "Evaluated 78/1000 samples\n",
            "Evaluated 79/1000 samples\n",
            "Evaluated 80/1000 samples\n",
            "Evaluated 81/1000 samples\n",
            "Evaluated 82/1000 samples\n",
            "Evaluated 83/1000 samples\n",
            "Evaluated 84/1000 samples\n",
            "Evaluated 85/1000 samples\n",
            "Evaluated 86/1000 samples\n",
            "Evaluated 87/1000 samples\n",
            "Evaluated 88/1000 samples\n",
            "Evaluated 89/1000 samples\n",
            "Evaluated 90/1000 samples\n",
            "Evaluated 91/1000 samples\n",
            "Evaluated 92/1000 samples\n",
            "Evaluated 93/1000 samples\n",
            "Evaluated 94/1000 samples\n",
            "Evaluated 95/1000 samples\n",
            "Evaluated 96/1000 samples\n",
            "Evaluated 97/1000 samples\n",
            "Evaluated 98/1000 samples\n",
            "Evaluated 99/1000 samples\n",
            "Evaluated 100/1000 samples\n",
            "Evaluated 101/1000 samples\n",
            "Evaluated 102/1000 samples\n",
            "Evaluated 103/1000 samples\n",
            "Evaluated 104/1000 samples\n",
            "Evaluated 105/1000 samples\n",
            "Evaluated 106/1000 samples\n",
            "Evaluated 107/1000 samples\n",
            "Evaluated 108/1000 samples\n",
            "Evaluated 109/1000 samples\n",
            "Evaluated 110/1000 samples\n",
            "Evaluated 111/1000 samples\n",
            "Evaluated 112/1000 samples\n",
            "Evaluated 113/1000 samples\n",
            "Evaluated 114/1000 samples\n",
            "Evaluated 115/1000 samples\n",
            "Evaluated 116/1000 samples\n",
            "Evaluated 117/1000 samples\n",
            "Evaluated 118/1000 samples\n",
            "Evaluated 119/1000 samples\n",
            "Evaluated 120/1000 samples\n",
            "Evaluated 121/1000 samples\n",
            "Evaluated 122/1000 samples\n",
            "Evaluated 123/1000 samples\n",
            "Evaluated 124/1000 samples\n",
            "Evaluated 125/1000 samples\n",
            "Evaluated 126/1000 samples\n",
            "Evaluated 127/1000 samples\n",
            "Evaluated 128/1000 samples\n",
            "Evaluated 129/1000 samples\n",
            "Evaluated 130/1000 samples\n",
            "Evaluated 131/1000 samples\n",
            "Evaluated 132/1000 samples\n",
            "Evaluated 133/1000 samples\n",
            "Evaluated 134/1000 samples\n",
            "Evaluated 135/1000 samples\n",
            "Evaluated 136/1000 samples\n",
            "Evaluated 137/1000 samples\n",
            "Evaluated 138/1000 samples\n",
            "Evaluated 139/1000 samples\n",
            "Evaluated 140/1000 samples\n",
            "Evaluated 141/1000 samples\n",
            "Evaluated 142/1000 samples\n",
            "Evaluated 143/1000 samples\n",
            "Evaluated 144/1000 samples\n",
            "Evaluated 145/1000 samples\n",
            "Evaluated 146/1000 samples\n",
            "Evaluated 147/1000 samples\n",
            "Evaluated 148/1000 samples\n",
            "Evaluated 149/1000 samples\n",
            "Evaluated 150/1000 samples\n",
            "Evaluated 151/1000 samples\n",
            "Evaluated 152/1000 samples\n",
            "Evaluated 153/1000 samples\n",
            "Evaluated 154/1000 samples\n",
            "Evaluated 155/1000 samples\n",
            "Evaluated 156/1000 samples\n",
            "Evaluated 157/1000 samples\n",
            "Evaluated 158/1000 samples\n",
            "Evaluated 159/1000 samples\n",
            "Evaluated 160/1000 samples\n",
            "Evaluated 161/1000 samples\n",
            "Evaluated 162/1000 samples\n",
            "Evaluated 163/1000 samples\n",
            "Evaluated 164/1000 samples\n",
            "Evaluated 165/1000 samples\n",
            "Evaluated 166/1000 samples\n",
            "Evaluated 167/1000 samples\n",
            "Evaluated 168/1000 samples\n",
            "Evaluated 169/1000 samples\n",
            "Evaluated 170/1000 samples\n",
            "Evaluated 171/1000 samples\n",
            "Evaluated 172/1000 samples\n",
            "Evaluated 173/1000 samples\n",
            "Evaluated 174/1000 samples\n",
            "Evaluated 175/1000 samples\n",
            "Evaluated 176/1000 samples\n",
            "Evaluated 177/1000 samples\n",
            "Evaluated 178/1000 samples\n",
            "Evaluated 179/1000 samples\n",
            "Evaluated 180/1000 samples\n",
            "Evaluated 181/1000 samples\n",
            "Evaluated 182/1000 samples\n",
            "Evaluated 183/1000 samples\n",
            "Evaluated 184/1000 samples\n",
            "Evaluated 185/1000 samples\n",
            "Evaluated 186/1000 samples\n",
            "Evaluated 187/1000 samples\n",
            "Evaluated 188/1000 samples\n",
            "Evaluated 189/1000 samples\n",
            "Evaluated 190/1000 samples\n",
            "Evaluated 191/1000 samples\n",
            "Evaluated 192/1000 samples\n",
            "Evaluated 193/1000 samples\n",
            "Evaluated 194/1000 samples\n",
            "Evaluated 195/1000 samples\n",
            "Evaluated 196/1000 samples\n",
            "Evaluated 197/1000 samples\n",
            "Evaluated 198/1000 samples\n",
            "Evaluated 199/1000 samples\n",
            "Evaluated 200/1000 samples\n",
            "Evaluated 201/1000 samples\n",
            "Evaluated 202/1000 samples\n",
            "Evaluated 203/1000 samples\n",
            "Evaluated 204/1000 samples\n",
            "Evaluated 205/1000 samples\n",
            "Evaluated 206/1000 samples\n",
            "Evaluated 207/1000 samples\n",
            "Evaluated 208/1000 samples\n",
            "Evaluated 209/1000 samples\n",
            "Evaluated 210/1000 samples\n",
            "Evaluated 211/1000 samples\n",
            "Evaluated 212/1000 samples\n",
            "Evaluated 213/1000 samples\n",
            "Evaluated 214/1000 samples\n",
            "Evaluated 215/1000 samples\n",
            "Evaluated 216/1000 samples\n",
            "Evaluated 217/1000 samples\n",
            "Evaluated 218/1000 samples\n",
            "Evaluated 219/1000 samples\n",
            "Evaluated 220/1000 samples\n",
            "Evaluated 221/1000 samples\n",
            "Evaluated 222/1000 samples\n",
            "Evaluated 223/1000 samples\n",
            "Evaluated 224/1000 samples\n",
            "Evaluated 225/1000 samples\n",
            "Evaluated 226/1000 samples\n",
            "Evaluated 227/1000 samples\n",
            "Evaluated 228/1000 samples\n",
            "Evaluated 229/1000 samples\n",
            "Evaluated 230/1000 samples\n",
            "Evaluated 231/1000 samples\n",
            "Evaluated 232/1000 samples\n",
            "Evaluated 233/1000 samples\n",
            "Evaluated 234/1000 samples\n",
            "Evaluated 235/1000 samples\n",
            "Evaluated 236/1000 samples\n",
            "Evaluated 237/1000 samples\n",
            "Evaluated 238/1000 samples\n",
            "Evaluated 239/1000 samples\n",
            "Evaluated 240/1000 samples\n",
            "Evaluated 241/1000 samples\n",
            "Evaluated 242/1000 samples\n",
            "Evaluated 243/1000 samples\n",
            "Evaluated 244/1000 samples\n",
            "Evaluated 245/1000 samples\n",
            "Evaluated 246/1000 samples\n",
            "Evaluated 247/1000 samples\n",
            "Evaluated 248/1000 samples\n",
            "Evaluated 249/1000 samples\n",
            "Evaluated 250/1000 samples\n",
            "Evaluated 251/1000 samples\n",
            "Evaluated 252/1000 samples\n",
            "Evaluated 253/1000 samples\n",
            "Evaluated 254/1000 samples\n",
            "Evaluated 255/1000 samples\n",
            "Evaluated 256/1000 samples\n",
            "Evaluated 257/1000 samples\n",
            "Evaluated 258/1000 samples\n",
            "Evaluated 259/1000 samples\n",
            "Evaluated 260/1000 samples\n",
            "Evaluated 261/1000 samples\n",
            "Evaluated 262/1000 samples\n",
            "Evaluated 263/1000 samples\n",
            "Evaluated 264/1000 samples\n",
            "Evaluated 265/1000 samples\n",
            "Evaluated 266/1000 samples\n",
            "Evaluated 267/1000 samples\n",
            "Evaluated 268/1000 samples\n",
            "Evaluated 269/1000 samples\n",
            "Evaluated 270/1000 samples\n",
            "Evaluated 271/1000 samples\n",
            "Evaluated 272/1000 samples\n",
            "Evaluated 273/1000 samples\n",
            "Evaluated 274/1000 samples\n",
            "Evaluated 275/1000 samples\n",
            "Evaluated 276/1000 samples\n",
            "Evaluated 277/1000 samples\n",
            "Evaluated 278/1000 samples\n",
            "Evaluated 279/1000 samples\n",
            "Evaluated 280/1000 samples\n",
            "Evaluated 281/1000 samples\n",
            "Evaluated 282/1000 samples\n",
            "Evaluated 283/1000 samples\n",
            "Evaluated 284/1000 samples\n",
            "Evaluated 285/1000 samples\n",
            "Evaluated 286/1000 samples\n",
            "Evaluated 287/1000 samples\n",
            "Evaluated 288/1000 samples\n",
            "Evaluated 289/1000 samples\n",
            "Evaluated 290/1000 samples\n",
            "Evaluated 291/1000 samples\n",
            "Evaluated 292/1000 samples\n",
            "Evaluated 293/1000 samples\n",
            "Evaluated 294/1000 samples\n",
            "Evaluated 295/1000 samples\n",
            "Evaluated 296/1000 samples\n",
            "Evaluated 297/1000 samples\n",
            "Evaluated 298/1000 samples\n",
            "Evaluated 299/1000 samples\n",
            "Evaluated 300/1000 samples\n",
            "Evaluated 301/1000 samples\n",
            "Evaluated 302/1000 samples\n",
            "Evaluated 303/1000 samples\n",
            "Evaluated 304/1000 samples\n",
            "Evaluated 305/1000 samples\n",
            "Evaluated 306/1000 samples\n",
            "Evaluated 307/1000 samples\n",
            "Evaluated 308/1000 samples\n",
            "Evaluated 309/1000 samples\n",
            "Evaluated 310/1000 samples\n",
            "Evaluated 311/1000 samples\n",
            "Evaluated 312/1000 samples\n",
            "Evaluated 313/1000 samples\n",
            "Evaluated 314/1000 samples\n",
            "Evaluated 315/1000 samples\n",
            "Evaluated 316/1000 samples\n",
            "Evaluated 317/1000 samples\n",
            "Evaluated 318/1000 samples\n",
            "Evaluated 319/1000 samples\n",
            "Evaluated 320/1000 samples\n",
            "Evaluated 321/1000 samples\n",
            "Evaluated 322/1000 samples\n",
            "Evaluated 323/1000 samples\n",
            "Evaluated 324/1000 samples\n",
            "Evaluated 325/1000 samples\n",
            "Evaluated 326/1000 samples\n",
            "Evaluated 327/1000 samples\n",
            "Evaluated 328/1000 samples\n",
            "Evaluated 329/1000 samples\n",
            "Evaluated 330/1000 samples\n",
            "Evaluated 331/1000 samples\n",
            "Evaluated 332/1000 samples\n",
            "Evaluated 333/1000 samples\n",
            "Evaluated 334/1000 samples\n",
            "Evaluated 335/1000 samples\n",
            "Evaluated 336/1000 samples\n",
            "Evaluated 337/1000 samples\n",
            "Evaluated 338/1000 samples\n",
            "Evaluated 339/1000 samples\n",
            "Evaluated 340/1000 samples\n",
            "Evaluated 341/1000 samples\n",
            "Evaluated 342/1000 samples\n",
            "Evaluated 343/1000 samples\n",
            "Evaluated 344/1000 samples\n",
            "Evaluated 345/1000 samples\n",
            "Evaluated 346/1000 samples\n",
            "Evaluated 347/1000 samples\n",
            "Evaluated 348/1000 samples\n",
            "Evaluated 349/1000 samples\n",
            "Evaluated 350/1000 samples\n",
            "Evaluated 351/1000 samples\n",
            "Evaluated 352/1000 samples\n",
            "Evaluated 353/1000 samples\n",
            "Evaluated 354/1000 samples\n",
            "Evaluated 355/1000 samples\n",
            "Evaluated 356/1000 samples\n",
            "Evaluated 357/1000 samples\n",
            "Evaluated 358/1000 samples\n",
            "Evaluated 359/1000 samples\n",
            "Evaluated 360/1000 samples\n",
            "Evaluated 361/1000 samples\n",
            "Evaluated 362/1000 samples\n",
            "Evaluated 363/1000 samples\n",
            "Evaluated 364/1000 samples\n",
            "Evaluated 365/1000 samples\n",
            "Evaluated 366/1000 samples\n",
            "Evaluated 367/1000 samples\n",
            "Evaluated 368/1000 samples\n",
            "Evaluated 369/1000 samples\n",
            "Evaluated 370/1000 samples\n",
            "Evaluated 371/1000 samples\n",
            "Evaluated 372/1000 samples\n",
            "Evaluated 373/1000 samples\n",
            "Evaluated 374/1000 samples\n",
            "Evaluated 375/1000 samples\n",
            "Evaluated 376/1000 samples\n",
            "Evaluated 377/1000 samples\n",
            "Evaluated 378/1000 samples\n",
            "Evaluated 379/1000 samples\n",
            "Evaluated 380/1000 samples\n",
            "Evaluated 381/1000 samples\n",
            "Evaluated 382/1000 samples\n",
            "Evaluated 383/1000 samples\n",
            "Evaluated 384/1000 samples\n",
            "Evaluated 385/1000 samples\n",
            "Evaluated 386/1000 samples\n",
            "Evaluated 387/1000 samples\n",
            "Evaluated 388/1000 samples\n",
            "Evaluated 389/1000 samples\n",
            "Evaluated 390/1000 samples\n",
            "Evaluated 391/1000 samples\n",
            "Evaluated 392/1000 samples\n",
            "Evaluated 393/1000 samples\n",
            "Evaluated 394/1000 samples\n",
            "Evaluated 395/1000 samples\n",
            "Evaluated 396/1000 samples\n",
            "Evaluated 397/1000 samples\n",
            "Evaluated 398/1000 samples\n",
            "Evaluated 399/1000 samples\n",
            "Evaluated 400/1000 samples\n",
            "Evaluated 401/1000 samples\n",
            "Evaluated 402/1000 samples\n",
            "Evaluated 403/1000 samples\n",
            "Evaluated 404/1000 samples\n",
            "Evaluated 405/1000 samples\n",
            "Evaluated 406/1000 samples\n",
            "Evaluated 407/1000 samples\n",
            "Evaluated 408/1000 samples\n",
            "Evaluated 409/1000 samples\n",
            "Evaluated 410/1000 samples\n",
            "Evaluated 411/1000 samples\n",
            "Evaluated 412/1000 samples\n",
            "Evaluated 413/1000 samples\n",
            "Evaluated 414/1000 samples\n",
            "Evaluated 415/1000 samples\n",
            "Evaluated 416/1000 samples\n",
            "Evaluated 417/1000 samples\n",
            "Evaluated 418/1000 samples\n",
            "Evaluated 419/1000 samples\n",
            "Evaluated 420/1000 samples\n",
            "Evaluated 421/1000 samples\n",
            "Evaluated 422/1000 samples\n",
            "Evaluated 423/1000 samples\n",
            "Evaluated 424/1000 samples\n",
            "Evaluated 425/1000 samples\n",
            "Evaluated 426/1000 samples\n",
            "Evaluated 427/1000 samples\n",
            "Evaluated 428/1000 samples\n",
            "Evaluated 429/1000 samples\n",
            "Evaluated 430/1000 samples\n",
            "Evaluated 431/1000 samples\n",
            "Evaluated 432/1000 samples\n",
            "Evaluated 433/1000 samples\n",
            "Evaluated 434/1000 samples\n",
            "Evaluated 435/1000 samples\n",
            "Evaluated 436/1000 samples\n",
            "Evaluated 437/1000 samples\n",
            "Evaluated 438/1000 samples\n",
            "Evaluated 439/1000 samples\n",
            "Evaluated 440/1000 samples\n",
            "Evaluated 441/1000 samples\n",
            "Evaluated 442/1000 samples\n",
            "Evaluated 443/1000 samples\n",
            "Evaluated 444/1000 samples\n",
            "Evaluated 445/1000 samples\n",
            "Evaluated 446/1000 samples\n",
            "Evaluated 447/1000 samples\n",
            "Evaluated 448/1000 samples\n",
            "Evaluated 449/1000 samples\n",
            "Evaluated 450/1000 samples\n",
            "Evaluated 451/1000 samples\n",
            "Evaluated 452/1000 samples\n",
            "Evaluated 453/1000 samples\n",
            "Evaluated 454/1000 samples\n",
            "Evaluated 455/1000 samples\n",
            "Evaluated 456/1000 samples\n",
            "Evaluated 457/1000 samples\n",
            "Evaluated 458/1000 samples\n",
            "Evaluated 459/1000 samples\n",
            "Evaluated 460/1000 samples\n",
            "Evaluated 461/1000 samples\n",
            "Evaluated 462/1000 samples\n",
            "Evaluated 463/1000 samples\n",
            "Evaluated 464/1000 samples\n",
            "Evaluated 465/1000 samples\n",
            "Evaluated 466/1000 samples\n",
            "Evaluated 467/1000 samples\n",
            "Evaluated 468/1000 samples\n",
            "Evaluated 469/1000 samples\n",
            "Evaluated 470/1000 samples\n",
            "Evaluated 471/1000 samples\n",
            "Evaluated 472/1000 samples\n",
            "Evaluated 473/1000 samples\n",
            "Evaluated 474/1000 samples\n",
            "Evaluated 475/1000 samples\n",
            "Evaluated 476/1000 samples\n",
            "Evaluated 477/1000 samples\n",
            "Evaluated 478/1000 samples\n",
            "Evaluated 479/1000 samples\n",
            "Evaluated 480/1000 samples\n",
            "Evaluated 481/1000 samples\n",
            "Evaluated 482/1000 samples\n",
            "Evaluated 483/1000 samples\n",
            "Evaluated 484/1000 samples\n",
            "Evaluated 485/1000 samples\n",
            "Evaluated 486/1000 samples\n",
            "Evaluated 487/1000 samples\n",
            "Evaluated 488/1000 samples\n",
            "Evaluated 489/1000 samples\n",
            "Evaluated 490/1000 samples\n",
            "Evaluated 491/1000 samples\n",
            "Evaluated 492/1000 samples\n",
            "Evaluated 493/1000 samples\n",
            "Evaluated 494/1000 samples\n",
            "Evaluated 495/1000 samples\n",
            "Evaluated 496/1000 samples\n",
            "Evaluated 497/1000 samples\n",
            "Evaluated 498/1000 samples\n",
            "Evaluated 499/1000 samples\n",
            "Evaluated 500/1000 samples\n",
            "Evaluated 501/1000 samples\n",
            "Evaluated 502/1000 samples\n",
            "Evaluated 503/1000 samples\n",
            "Evaluated 504/1000 samples\n",
            "Evaluated 505/1000 samples\n",
            "Evaluated 506/1000 samples\n",
            "Evaluated 507/1000 samples\n",
            "Evaluated 508/1000 samples\n",
            "Evaluated 509/1000 samples\n",
            "Evaluated 510/1000 samples\n",
            "Evaluated 511/1000 samples\n",
            "Evaluated 512/1000 samples\n",
            "Evaluated 513/1000 samples\n",
            "Evaluated 514/1000 samples\n",
            "Evaluated 515/1000 samples\n",
            "Evaluated 516/1000 samples\n",
            "Evaluated 517/1000 samples\n",
            "Evaluated 518/1000 samples\n",
            "Evaluated 519/1000 samples\n",
            "Evaluated 520/1000 samples\n",
            "Evaluated 521/1000 samples\n",
            "Evaluated 522/1000 samples\n",
            "Evaluated 523/1000 samples\n",
            "Evaluated 524/1000 samples\n",
            "Evaluated 525/1000 samples\n",
            "Evaluated 526/1000 samples\n",
            "Evaluated 527/1000 samples\n",
            "Evaluated 528/1000 samples\n",
            "Evaluated 529/1000 samples\n",
            "Evaluated 530/1000 samples\n",
            "Evaluated 531/1000 samples\n",
            "Evaluated 532/1000 samples\n",
            "Evaluated 533/1000 samples\n",
            "Evaluated 534/1000 samples\n",
            "Evaluated 535/1000 samples\n",
            "Evaluated 536/1000 samples\n",
            "Evaluated 537/1000 samples\n",
            "Evaluated 538/1000 samples\n",
            "Evaluated 539/1000 samples\n",
            "Evaluated 540/1000 samples\n",
            "Evaluated 541/1000 samples\n",
            "Evaluated 542/1000 samples\n",
            "Evaluated 543/1000 samples\n",
            "Evaluated 544/1000 samples\n",
            "Evaluated 545/1000 samples\n",
            "Evaluated 546/1000 samples\n",
            "Evaluated 547/1000 samples\n",
            "Evaluated 548/1000 samples\n",
            "Evaluated 549/1000 samples\n",
            "Evaluated 550/1000 samples\n",
            "Evaluated 551/1000 samples\n",
            "Evaluated 552/1000 samples\n",
            "Evaluated 553/1000 samples\n",
            "Evaluated 554/1000 samples\n",
            "Evaluated 555/1000 samples\n",
            "Evaluated 556/1000 samples\n",
            "Evaluated 557/1000 samples\n",
            "Evaluated 558/1000 samples\n",
            "Evaluated 559/1000 samples\n",
            "Evaluated 560/1000 samples\n",
            "Evaluated 561/1000 samples\n",
            "Evaluated 562/1000 samples\n",
            "Evaluated 563/1000 samples\n",
            "Evaluated 564/1000 samples\n",
            "Evaluated 565/1000 samples\n",
            "Evaluated 566/1000 samples\n",
            "Evaluated 567/1000 samples\n",
            "Evaluated 568/1000 samples\n",
            "Evaluated 569/1000 samples\n",
            "Evaluated 570/1000 samples\n",
            "Evaluated 571/1000 samples\n",
            "Evaluated 572/1000 samples\n",
            "Evaluated 573/1000 samples\n",
            "Evaluated 574/1000 samples\n",
            "Evaluated 575/1000 samples\n",
            "Evaluated 576/1000 samples\n",
            "Evaluated 577/1000 samples\n",
            "Evaluated 578/1000 samples\n",
            "Evaluated 579/1000 samples\n",
            "Evaluated 580/1000 samples\n",
            "Evaluated 581/1000 samples\n",
            "Evaluated 582/1000 samples\n",
            "Evaluated 583/1000 samples\n",
            "Evaluated 584/1000 samples\n",
            "Evaluated 585/1000 samples\n",
            "Evaluated 586/1000 samples\n",
            "Evaluated 587/1000 samples\n",
            "Evaluated 588/1000 samples\n",
            "Evaluated 589/1000 samples\n",
            "Evaluated 590/1000 samples\n",
            "Evaluated 591/1000 samples\n",
            "Evaluated 592/1000 samples\n",
            "Evaluated 593/1000 samples\n",
            "Evaluated 594/1000 samples\n",
            "Evaluated 595/1000 samples\n",
            "Evaluated 596/1000 samples\n",
            "Evaluated 597/1000 samples\n",
            "Evaluated 598/1000 samples\n",
            "Evaluated 599/1000 samples\n",
            "Evaluated 600/1000 samples\n",
            "Evaluated 601/1000 samples\n",
            "Evaluated 602/1000 samples\n",
            "Evaluated 603/1000 samples\n",
            "Evaluated 604/1000 samples\n",
            "Evaluated 605/1000 samples\n",
            "Evaluated 606/1000 samples\n",
            "Evaluated 607/1000 samples\n",
            "Evaluated 608/1000 samples\n",
            "Evaluated 609/1000 samples\n",
            "Evaluated 610/1000 samples\n",
            "Evaluated 611/1000 samples\n",
            "Evaluated 612/1000 samples\n",
            "Evaluated 613/1000 samples\n",
            "Evaluated 614/1000 samples\n",
            "Evaluated 615/1000 samples\n",
            "Evaluated 616/1000 samples\n",
            "Evaluated 617/1000 samples\n",
            "Evaluated 618/1000 samples\n",
            "Evaluated 619/1000 samples\n",
            "Evaluated 620/1000 samples\n",
            "Evaluated 621/1000 samples\n",
            "Evaluated 622/1000 samples\n",
            "Evaluated 623/1000 samples\n",
            "Evaluated 624/1000 samples\n",
            "Evaluated 625/1000 samples\n",
            "Evaluated 626/1000 samples\n",
            "Evaluated 627/1000 samples\n",
            "Evaluated 628/1000 samples\n",
            "Evaluated 629/1000 samples\n",
            "Evaluated 630/1000 samples\n",
            "Evaluated 631/1000 samples\n",
            "Evaluated 632/1000 samples\n",
            "Evaluated 633/1000 samples\n",
            "Evaluated 634/1000 samples\n",
            "Evaluated 635/1000 samples\n",
            "Evaluated 636/1000 samples\n",
            "Evaluated 637/1000 samples\n",
            "Evaluated 638/1000 samples\n",
            "Evaluated 639/1000 samples\n",
            "Evaluated 640/1000 samples\n",
            "Evaluated 641/1000 samples\n",
            "Evaluated 642/1000 samples\n",
            "Evaluated 643/1000 samples\n",
            "Evaluated 644/1000 samples\n",
            "Evaluated 645/1000 samples\n",
            "Evaluated 646/1000 samples\n",
            "Evaluated 647/1000 samples\n",
            "Evaluated 648/1000 samples\n",
            "Evaluated 649/1000 samples\n",
            "Evaluated 650/1000 samples\n",
            "Evaluated 651/1000 samples\n",
            "Evaluated 652/1000 samples\n",
            "Evaluated 653/1000 samples\n",
            "Evaluated 654/1000 samples\n",
            "Evaluated 655/1000 samples\n",
            "Evaluated 656/1000 samples\n",
            "Evaluated 657/1000 samples\n",
            "Evaluated 658/1000 samples\n",
            "Evaluated 659/1000 samples\n",
            "Evaluated 660/1000 samples\n",
            "Evaluated 661/1000 samples\n",
            "Evaluated 662/1000 samples\n",
            "Evaluated 663/1000 samples\n",
            "Evaluated 664/1000 samples\n",
            "Evaluated 665/1000 samples\n",
            "Evaluated 666/1000 samples\n",
            "Evaluated 667/1000 samples\n",
            "Evaluated 668/1000 samples\n",
            "Evaluated 669/1000 samples\n",
            "Evaluated 670/1000 samples\n",
            "Evaluated 671/1000 samples\n",
            "Evaluated 672/1000 samples\n",
            "Evaluated 673/1000 samples\n",
            "Evaluated 674/1000 samples\n",
            "Evaluated 675/1000 samples\n",
            "Evaluated 676/1000 samples\n",
            "Evaluated 677/1000 samples\n",
            "Evaluated 678/1000 samples\n",
            "Evaluated 679/1000 samples\n",
            "Evaluated 680/1000 samples\n",
            "Evaluated 681/1000 samples\n",
            "Evaluated 682/1000 samples\n",
            "Evaluated 683/1000 samples\n",
            "Evaluated 684/1000 samples\n",
            "Evaluated 685/1000 samples\n",
            "Evaluated 686/1000 samples\n",
            "Evaluated 687/1000 samples\n",
            "Evaluated 688/1000 samples\n",
            "Evaluated 689/1000 samples\n",
            "Evaluated 690/1000 samples\n",
            "Evaluated 691/1000 samples\n",
            "Evaluated 692/1000 samples\n",
            "Evaluated 693/1000 samples\n",
            "Evaluated 694/1000 samples\n",
            "Evaluated 695/1000 samples\n",
            "Evaluated 696/1000 samples\n",
            "Evaluated 697/1000 samples\n",
            "Evaluated 698/1000 samples\n",
            "Evaluated 699/1000 samples\n",
            "Evaluated 700/1000 samples\n",
            "Evaluated 701/1000 samples\n",
            "Evaluated 702/1000 samples\n",
            "Evaluated 703/1000 samples\n",
            "Evaluated 704/1000 samples\n",
            "Evaluated 705/1000 samples\n",
            "Evaluated 706/1000 samples\n",
            "Evaluated 707/1000 samples\n",
            "Evaluated 708/1000 samples\n",
            "Evaluated 709/1000 samples\n",
            "Evaluated 710/1000 samples\n",
            "Evaluated 711/1000 samples\n",
            "Evaluated 712/1000 samples\n",
            "Evaluated 713/1000 samples\n",
            "Evaluated 714/1000 samples\n",
            "Evaluated 715/1000 samples\n",
            "Evaluated 716/1000 samples\n",
            "Evaluated 717/1000 samples\n",
            "Evaluated 718/1000 samples\n",
            "Evaluated 719/1000 samples\n",
            "Evaluated 720/1000 samples\n",
            "Evaluated 721/1000 samples\n",
            "Evaluated 722/1000 samples\n",
            "Evaluated 723/1000 samples\n",
            "Evaluated 724/1000 samples\n",
            "Evaluated 725/1000 samples\n",
            "Evaluated 726/1000 samples\n",
            "Evaluated 727/1000 samples\n",
            "Evaluated 728/1000 samples\n",
            "Evaluated 729/1000 samples\n",
            "Evaluated 730/1000 samples\n",
            "Evaluated 731/1000 samples\n",
            "Evaluated 732/1000 samples\n",
            "Evaluated 733/1000 samples\n",
            "Evaluated 734/1000 samples\n",
            "Evaluated 735/1000 samples\n",
            "Evaluated 736/1000 samples\n",
            "Evaluated 737/1000 samples\n",
            "Evaluated 738/1000 samples\n",
            "Evaluated 739/1000 samples\n",
            "Evaluated 740/1000 samples\n",
            "Evaluated 741/1000 samples\n",
            "Evaluated 742/1000 samples\n",
            "Evaluated 743/1000 samples\n",
            "Evaluated 744/1000 samples\n",
            "Evaluated 745/1000 samples\n",
            "Evaluated 746/1000 samples\n",
            "Evaluated 747/1000 samples\n",
            "Evaluated 748/1000 samples\n",
            "Evaluated 749/1000 samples\n",
            "Evaluated 750/1000 samples\n",
            "Evaluated 751/1000 samples\n",
            "Evaluated 752/1000 samples\n",
            "Evaluated 753/1000 samples\n",
            "Evaluated 754/1000 samples\n",
            "Evaluated 755/1000 samples\n",
            "Evaluated 756/1000 samples\n",
            "Evaluated 757/1000 samples\n",
            "Evaluated 758/1000 samples\n",
            "Evaluated 759/1000 samples\n",
            "Evaluated 760/1000 samples\n",
            "Evaluated 761/1000 samples\n",
            "Evaluated 762/1000 samples\n",
            "Evaluated 763/1000 samples\n",
            "Evaluated 764/1000 samples\n",
            "Evaluated 765/1000 samples\n",
            "Evaluated 766/1000 samples\n",
            "Evaluated 767/1000 samples\n",
            "Evaluated 768/1000 samples\n",
            "Evaluated 769/1000 samples\n",
            "Evaluated 770/1000 samples\n",
            "Evaluated 771/1000 samples\n",
            "Evaluated 772/1000 samples\n",
            "Evaluated 773/1000 samples\n",
            "Evaluated 774/1000 samples\n",
            "Evaluated 775/1000 samples\n",
            "Evaluated 776/1000 samples\n",
            "Evaluated 777/1000 samples\n",
            "Evaluated 778/1000 samples\n",
            "Evaluated 779/1000 samples\n",
            "Evaluated 780/1000 samples\n",
            "Evaluated 781/1000 samples\n",
            "Evaluated 782/1000 samples\n",
            "Evaluated 783/1000 samples\n",
            "Evaluated 784/1000 samples\n",
            "Evaluated 785/1000 samples\n",
            "Evaluated 786/1000 samples\n",
            "Evaluated 787/1000 samples\n",
            "Evaluated 788/1000 samples\n",
            "Evaluated 789/1000 samples\n",
            "Evaluated 790/1000 samples\n",
            "Evaluated 791/1000 samples\n",
            "Evaluated 792/1000 samples\n",
            "Evaluated 793/1000 samples\n",
            "Evaluated 794/1000 samples\n",
            "Evaluated 795/1000 samples\n",
            "Evaluated 796/1000 samples\n",
            "Evaluated 797/1000 samples\n",
            "Evaluated 798/1000 samples\n",
            "Evaluated 799/1000 samples\n",
            "Evaluated 800/1000 samples\n",
            "Evaluated 801/1000 samples\n",
            "Evaluated 802/1000 samples\n",
            "Evaluated 803/1000 samples\n",
            "Evaluated 804/1000 samples\n",
            "Evaluated 805/1000 samples\n",
            "Evaluated 806/1000 samples\n",
            "Evaluated 807/1000 samples\n",
            "Evaluated 808/1000 samples\n",
            "Evaluated 809/1000 samples\n",
            "Evaluated 810/1000 samples\n",
            "Evaluated 811/1000 samples\n",
            "Evaluated 812/1000 samples\n",
            "Evaluated 813/1000 samples\n",
            "Evaluated 814/1000 samples\n",
            "Evaluated 815/1000 samples\n",
            "Evaluated 816/1000 samples\n",
            "Evaluated 817/1000 samples\n",
            "Evaluated 818/1000 samples\n",
            "Evaluated 819/1000 samples\n",
            "Evaluated 820/1000 samples\n",
            "Evaluated 821/1000 samples\n",
            "Evaluated 822/1000 samples\n",
            "Evaluated 823/1000 samples\n",
            "Evaluated 824/1000 samples\n",
            "Evaluated 825/1000 samples\n",
            "Evaluated 826/1000 samples\n",
            "Evaluated 827/1000 samples\n",
            "Evaluated 828/1000 samples\n",
            "Evaluated 829/1000 samples\n",
            "Evaluated 830/1000 samples\n",
            "Evaluated 831/1000 samples\n",
            "Evaluated 832/1000 samples\n",
            "Evaluated 833/1000 samples\n",
            "Evaluated 834/1000 samples\n",
            "Evaluated 835/1000 samples\n",
            "Evaluated 836/1000 samples\n",
            "Evaluated 837/1000 samples\n",
            "Evaluated 838/1000 samples\n",
            "Evaluated 839/1000 samples\n",
            "Evaluated 840/1000 samples\n",
            "Evaluated 841/1000 samples\n",
            "Evaluated 842/1000 samples\n",
            "Evaluated 843/1000 samples\n",
            "Evaluated 844/1000 samples\n",
            "Evaluated 845/1000 samples\n",
            "Evaluated 846/1000 samples\n",
            "Evaluated 847/1000 samples\n",
            "Evaluated 848/1000 samples\n",
            "Evaluated 849/1000 samples\n",
            "Evaluated 850/1000 samples\n",
            "Evaluated 851/1000 samples\n",
            "Evaluated 852/1000 samples\n",
            "Evaluated 853/1000 samples\n",
            "Evaluated 854/1000 samples\n",
            "Evaluated 855/1000 samples\n",
            "Evaluated 856/1000 samples\n",
            "Evaluated 857/1000 samples\n",
            "Evaluated 858/1000 samples\n",
            "Evaluated 859/1000 samples\n",
            "Evaluated 860/1000 samples\n",
            "Evaluated 861/1000 samples\n",
            "Evaluated 862/1000 samples\n",
            "Evaluated 863/1000 samples\n",
            "Evaluated 864/1000 samples\n",
            "Evaluated 865/1000 samples\n",
            "Evaluated 866/1000 samples\n",
            "Evaluated 867/1000 samples\n",
            "Evaluated 868/1000 samples\n",
            "Evaluated 869/1000 samples\n",
            "Evaluated 870/1000 samples\n",
            "Evaluated 871/1000 samples\n",
            "Evaluated 872/1000 samples\n",
            "Evaluated 873/1000 samples\n",
            "Evaluated 874/1000 samples\n",
            "Evaluated 875/1000 samples\n",
            "Evaluated 876/1000 samples\n",
            "Evaluated 877/1000 samples\n",
            "Evaluated 878/1000 samples\n",
            "Evaluated 879/1000 samples\n",
            "Evaluated 880/1000 samples\n",
            "Evaluated 881/1000 samples\n",
            "Evaluated 882/1000 samples\n",
            "Evaluated 883/1000 samples\n",
            "Evaluated 884/1000 samples\n",
            "Evaluated 885/1000 samples\n",
            "Evaluated 886/1000 samples\n",
            "Evaluated 887/1000 samples\n",
            "Evaluated 888/1000 samples\n",
            "Evaluated 889/1000 samples\n",
            "Evaluated 890/1000 samples\n",
            "Evaluated 891/1000 samples\n",
            "Evaluated 892/1000 samples\n",
            "Evaluated 893/1000 samples\n",
            "Evaluated 894/1000 samples\n",
            "Evaluated 895/1000 samples\n",
            "Evaluated 896/1000 samples\n",
            "Evaluated 897/1000 samples\n",
            "Evaluated 898/1000 samples\n",
            "Evaluated 899/1000 samples\n",
            "Evaluated 900/1000 samples\n",
            "Evaluated 901/1000 samples\n",
            "Evaluated 902/1000 samples\n",
            "Evaluated 903/1000 samples\n",
            "Evaluated 904/1000 samples\n",
            "Evaluated 905/1000 samples\n",
            "Evaluated 906/1000 samples\n",
            "Evaluated 907/1000 samples\n",
            "Evaluated 908/1000 samples\n",
            "Evaluated 909/1000 samples\n",
            "Evaluated 910/1000 samples\n",
            "Evaluated 911/1000 samples\n",
            "Evaluated 912/1000 samples\n",
            "Evaluated 913/1000 samples\n",
            "Evaluated 914/1000 samples\n",
            "Evaluated 915/1000 samples\n",
            "Evaluated 916/1000 samples\n",
            "Evaluated 917/1000 samples\n",
            "Evaluated 918/1000 samples\n",
            "Evaluated 919/1000 samples\n",
            "Evaluated 920/1000 samples\n",
            "Evaluated 921/1000 samples\n",
            "Evaluated 922/1000 samples\n",
            "Evaluated 923/1000 samples\n",
            "Evaluated 924/1000 samples\n",
            "Evaluated 925/1000 samples\n",
            "Evaluated 926/1000 samples\n",
            "Evaluated 927/1000 samples\n",
            "Evaluated 928/1000 samples\n",
            "Evaluated 929/1000 samples\n",
            "Evaluated 930/1000 samples\n",
            "Evaluated 931/1000 samples\n",
            "Evaluated 932/1000 samples\n",
            "Evaluated 933/1000 samples\n",
            "Evaluated 934/1000 samples\n",
            "Evaluated 935/1000 samples\n",
            "Evaluated 936/1000 samples\n",
            "Evaluated 937/1000 samples\n",
            "Evaluated 938/1000 samples\n",
            "Evaluated 939/1000 samples\n",
            "Evaluated 940/1000 samples\n",
            "Evaluated 941/1000 samples\n",
            "Evaluated 942/1000 samples\n",
            "Evaluated 943/1000 samples\n",
            "Evaluated 944/1000 samples\n",
            "Evaluated 945/1000 samples\n",
            "Evaluated 946/1000 samples\n",
            "Evaluated 947/1000 samples\n",
            "Evaluated 948/1000 samples\n",
            "Evaluated 949/1000 samples\n",
            "Evaluated 950/1000 samples\n",
            "Evaluated 951/1000 samples\n",
            "Evaluated 952/1000 samples\n",
            "Evaluated 953/1000 samples\n",
            "Evaluated 954/1000 samples\n",
            "Evaluated 955/1000 samples\n",
            "Evaluated 956/1000 samples\n",
            "Evaluated 957/1000 samples\n",
            "Evaluated 958/1000 samples\n",
            "Evaluated 959/1000 samples\n",
            "Evaluated 960/1000 samples\n",
            "Evaluated 961/1000 samples\n",
            "Evaluated 962/1000 samples\n",
            "Evaluated 963/1000 samples\n",
            "Evaluated 964/1000 samples\n",
            "Evaluated 965/1000 samples\n",
            "Evaluated 966/1000 samples\n",
            "Evaluated 967/1000 samples\n",
            "Evaluated 968/1000 samples\n",
            "Evaluated 969/1000 samples\n",
            "Evaluated 970/1000 samples\n",
            "Evaluated 971/1000 samples\n",
            "Evaluated 972/1000 samples\n",
            "Evaluated 973/1000 samples\n",
            "Evaluated 974/1000 samples\n",
            "Evaluated 975/1000 samples\n",
            "Evaluated 976/1000 samples\n",
            "Evaluated 977/1000 samples\n",
            "Evaluated 978/1000 samples\n",
            "Evaluated 979/1000 samples\n",
            "Evaluated 980/1000 samples\n",
            "Evaluated 981/1000 samples\n",
            "Evaluated 982/1000 samples\n",
            "Evaluated 983/1000 samples\n",
            "Evaluated 984/1000 samples\n",
            "Evaluated 985/1000 samples\n",
            "Evaluated 986/1000 samples\n",
            "Evaluated 987/1000 samples\n",
            "Evaluated 988/1000 samples\n",
            "Evaluated 989/1000 samples\n",
            "Evaluated 990/1000 samples\n",
            "Evaluated 991/1000 samples\n",
            "Evaluated 992/1000 samples\n",
            "Evaluated 993/1000 samples\n",
            "Evaluated 994/1000 samples\n",
            "Evaluated 995/1000 samples\n",
            "Evaluated 996/1000 samples\n",
            "Evaluated 997/1000 samples\n",
            "Evaluated 998/1000 samples\n",
            "Evaluated 999/1000 samples\n",
            "Evaluated 1000/1000 samples\n",
            "\n",
            "Evaluation results saved to arithmetic_llm/evaluation_results/instruction_id_eval\n",
            "  - Metrics: arithmetic_llm/evaluation_results/instruction_id_eval/evaluation_metrics_20260221_062256.json\n",
            "  - Samples: arithmetic_llm/evaluation_results/instruction_id_eval/sample_outputs_20260221_062256.json\n",
            "  - Summary: arithmetic_llm/evaluation_results/instruction_id_eval/evaluation_summary_20260221_062256.txt\n",
            "\n",
            "============================================================\n",
            "EVALUATION RESULTS\n",
            "============================================================\n",
            "\n",
            "Total Samples: 1000\n",
            "Correct Samples: 440\n",
            "Parseable Samples: 900\n",
            "\n",
            "Exact Match Accuracy: 44.00%\n",
            "Parse Success Rate: 90.00%\n",
            "Avg Generation Length: 191.14 tokens\n",
            "============================================================\n",
            "\n",
            "Interpretation:\n",
            "  ~ Moderate performance - consider more training\n",
            "============================================================\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r arithmetic_llm_evaluation_results.zip arithmetic_llm/evaluation_results\n",
        "files.download(\"arithmetic_llm_evaluation_results.zip\")"
      ],
      "metadata": {
        "id": "hoLxKy-u3yYZ",
        "outputId": "8ec013d3-ad2f-4eb6-9143-57891d4729ae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "id": "hoLxKy-u3yYZ",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: arithmetic_llm/evaluation_results/ (stored 0%)\n",
            "  adding: arithmetic_llm/evaluation_results/instruction_id_eval/ (stored 0%)\n",
            "  adding: arithmetic_llm/evaluation_results/instruction_id_eval/evaluation_summary_20260221_062256.txt (deflated 78%)\n",
            "  adding: arithmetic_llm/evaluation_results/instruction_id_eval/sample_outputs_20260221_062256.json (deflated 81%)\n",
            "  adding: arithmetic_llm/evaluation_results/instruction_id_eval/evaluation_metrics_20260221_062256.json (deflated 31%)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_6d70a05b-2894-4e5a-8950-573d2693a54c\", \"arithmetic_llm_evaluation_results.zip\", 4134)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}